{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb1a241c2fd642c99548fc162efb630e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f1d5d8a924a4994ac6f50df9303f073",
              "IPY_MODEL_260ce6a214bc43b9aac82489ee9b67ba",
              "IPY_MODEL_77d47ac88d6b4686871bfbd58a39785e"
            ],
            "layout": "IPY_MODEL_569b29e1f3ff4f60aa053f0e3ac8f8b3"
          }
        },
        "0f1d5d8a924a4994ac6f50df9303f073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c1f126eca274ca78940fed6a87193af",
            "placeholder": "​",
            "style": "IPY_MODEL_86e06defb5d64731949e6451b7da308a",
            "value": "config.json: 100%"
          }
        },
        "260ce6a214bc43b9aac82489ee9b67ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c3bdbf2bbc470f9a37fa89f20d3d8f",
            "max": 434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b145f25ce76d4e06bb2a3fd87503f5fb",
            "value": 434
          }
        },
        "77d47ac88d6b4686871bfbd58a39785e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9078798398d54f9aaa69f30c782d260b",
            "placeholder": "​",
            "style": "IPY_MODEL_8799819ee4c640baa6d634242730c2bc",
            "value": " 434/434 [00:00&lt;00:00, 43.9kB/s]"
          }
        },
        "569b29e1f3ff4f60aa053f0e3ac8f8b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1f126eca274ca78940fed6a87193af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e06defb5d64731949e6451b7da308a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9c3bdbf2bbc470f9a37fa89f20d3d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b145f25ce76d4e06bb2a3fd87503f5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9078798398d54f9aaa69f30c782d260b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8799819ee4c640baa6d634242730c2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a55515a675d44d0d86fedf53a3a96a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22e11a3c8ce84ef4aad43ef9fb8af295",
              "IPY_MODEL_230747481d834702932ea9f994c89e93",
              "IPY_MODEL_dd98bc59946743b99efd812545c0991c"
            ],
            "layout": "IPY_MODEL_b668c1649f5246408389d04781d73bb6"
          }
        },
        "22e11a3c8ce84ef4aad43ef9fb8af295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69aaded374174838a9e60616a49bb130",
            "placeholder": "​",
            "style": "IPY_MODEL_26461d5aa1d048938738b4d4cfac47e9",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "230747481d834702932ea9f994c89e93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82f6d89e539647bf963e0c6f5367c933",
            "max": 654186735,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e5dc562894643118042ea21ebca3ec1",
            "value": 654186735
          }
        },
        "dd98bc59946743b99efd812545c0991c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bea5412b3bd4c74a1f0066b78cee043",
            "placeholder": "​",
            "style": "IPY_MODEL_4304b6cf253847cf831a69381ce1b21b",
            "value": " 654M/654M [00:02&lt;00:00, 281MB/s]"
          }
        },
        "b668c1649f5246408389d04781d73bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69aaded374174838a9e60616a49bb130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26461d5aa1d048938738b4d4cfac47e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82f6d89e539647bf963e0c6f5367c933": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e5dc562894643118042ea21ebca3ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bea5412b3bd4c74a1f0066b78cee043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4304b6cf253847cf831a69381ce1b21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da5399621114460db73e0a1fc88a1451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ebdd2e201974f14bc5c7faef7b5834d",
              "IPY_MODEL_f56dde12ad374a099dfbe46c06f462bd",
              "IPY_MODEL_e12e4f9a7ddf473ea8a547d538f5d030"
            ],
            "layout": "IPY_MODEL_331f4788c1f94407a58a022b00090827"
          }
        },
        "1ebdd2e201974f14bc5c7faef7b5834d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae28a34ea3446539de0e6c9068ebef9",
            "placeholder": "​",
            "style": "IPY_MODEL_a7b260ef81904a38aea48d6de401ab03",
            "value": "model.safetensors: 100%"
          }
        },
        "f56dde12ad374a099dfbe46c06f462bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ffa5bf8a9114303b9ecc291afd84d46",
            "max": 654164136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_252a32e43c0441278b9e8bad7002c351",
            "value": 654164136
          }
        },
        "e12e4f9a7ddf473ea8a547d538f5d030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_667188570b4b42cda7a90df01ab4278d",
            "placeholder": "​",
            "style": "IPY_MODEL_acc3395d37e2475c91a9a0d6bce490e1",
            "value": " 654M/654M [00:07&lt;00:00, 149MB/s]"
          }
        },
        "331f4788c1f94407a58a022b00090827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae28a34ea3446539de0e6c9068ebef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b260ef81904a38aea48d6de401ab03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ffa5bf8a9114303b9ecc291afd84d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252a32e43c0441278b9e8bad7002c351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "667188570b4b42cda7a90df01ab4278d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc3395d37e2475c91a9a0d6bce490e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49bec66036ea454c9ab0e45befe71078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af8162fe8419400b9945f3efbb731f7a",
              "IPY_MODEL_a116bf6710fd4e00b762f04626612fb0",
              "IPY_MODEL_161349300cee46139d9d4f3f2b074c82"
            ],
            "layout": "IPY_MODEL_77a94efc3b3140e3920c2f5f85f5bb27"
          }
        },
        "af8162fe8419400b9945f3efbb731f7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a05ede3191a744d49f51b37d248ae0d4",
            "placeholder": "​",
            "style": "IPY_MODEL_5b3d5cd288604a8b9dfa1512ca792fc9",
            "value": "vocab.txt: "
          }
        },
        "a116bf6710fd4e00b762f04626612fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565bfbd5b1f840d7b4ea11884c418e67",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84756faf93aa4756a7d2284bb7b18875",
            "value": 1
          }
        },
        "161349300cee46139d9d4f3f2b074c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c510aa9815db4a49a2895b35ea2bc689",
            "placeholder": "​",
            "style": "IPY_MODEL_be726245c303415ebc87b8b2915a7afb",
            "value": " 1.22M/? [00:00&lt;00:00, 13.8MB/s]"
          }
        },
        "77a94efc3b3140e3920c2f5f85f5bb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a05ede3191a744d49f51b37d248ae0d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3d5cd288604a8b9dfa1512ca792fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "565bfbd5b1f840d7b4ea11884c418e67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "84756faf93aa4756a7d2284bb7b18875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c510aa9815db4a49a2895b35ea2bc689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be726245c303415ebc87b8b2915a7afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Installing and setting up ollama in colab**"
      ],
      "metadata": {
        "id": "Lwr4lpT1ZjQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QmvnIPowa83i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We want to make sure if the GPU works or not**"
      ],
      "metadata": {
        "id": "KXVblnE_XAZj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHKsbERYWvax",
        "outputId": "18b91210-b6bb-40ab-e92d-6a0a9f6b0ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,853 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,768 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,269 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,142 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,160 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,471 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,574 kB]\n",
            "Fetched 23.6 MB in 3s (9,401 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3 pci.ids usb.ids\n",
            "The following NEW packages will be installed:\n",
            "  libpci3 lshw pci.ids pciutils usb.ids\n",
            "0 upgraded, 5 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 883 kB of archives.\n",
            "After this operation, 3,256 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 pci.ids all 0.0~2022.01.22-1ubuntu0.1 [251 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpci3 amd64 1:3.7.0-6 [28.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 lshw amd64 02.19.git.2021.06.19.996aaad9c7-2build1 [321 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 pciutils amd64 1:3.7.0-6 [63.6 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 usb.ids all 2022.04.02-1 [219 kB]\n",
            "Fetched 883 kB in 1s (663 kB/s)\n",
            "Selecting previously unselected package pci.ids.\n",
            "(Reading database ... 126284 files and directories currently installed.)\n",
            "Preparing to unpack .../pci.ids_0.0~2022.01.22-1ubuntu0.1_all.deb ...\n",
            "Unpacking pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.7.0-6) ...\n",
            "Selecting previously unselected package lshw.\n",
            "Preparing to unpack .../lshw_02.19.git.2021.06.19.996aaad9c7-2build1_amd64.deb ...\n",
            "Unpacking lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.7.0-6_amd64.deb ...\n",
            "Unpacking pciutils (1:3.7.0-6) ...\n",
            "Selecting previously unselected package usb.ids.\n",
            "Preparing to unpack .../usb.ids_2022.04.02-1_all.deb ...\n",
            "Unpacking usb.ids (2022.04.02-1) ...\n",
            "Setting up pci.ids (0.0~2022.01.22-1ubuntu0.1) ...\n",
            "Setting up lshw (02.19.git.2021.06.19.996aaad9c7-2build1) ...\n",
            "Setting up usb.ids (2022.04.02-1) ...\n",
            "Setting up libpci3:amd64 (1:3.7.0-6) ...\n",
            "Setting up pciutils (1:3.7.0-6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "Fri Jul 25 18:13:00 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!apt update && apt install -y pciutils lshw\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install Ollama**"
      ],
      "metadata": {
        "id": "SYKl-706XnsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29w7w6MrXSdy",
        "outputId": "8376540c-a56a-456f-e5e1-c44c0cc6d2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we need to run Ollama as a service in the background**"
      ],
      "metadata": {
        "id": "RNrZaPlIYQVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zflfwaKhXvTe",
        "outputId": "f0889995-f88a-46e7-ab2d-de67a95d77d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now it's time to downloading our model(llama3.1)**"
      ],
      "metadata": {
        "id": "fchXtVpiYgwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run llama3.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b1LJn7YZVx",
        "outputId": "72e40b98-494e-479e-a346-af2e487a4d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K/bye\n",
            "... \n",
            "\u001b[?2004l"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1.2: Connect to the ollama server**"
      ],
      "metadata": {
        "id": "xIY-sUMcaRyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EZnQKKswa6-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install Libraries**"
      ],
      "metadata": {
        "id": "YPIxRP-JazAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-ollama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOxuzX7nYu6G",
        "outputId": "0eaf92f1-57bc-4b29-9a7c-f1a0da1c6359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting ollama<1.0.0,>=0.5.1 (from langchain-ollama)\n",
            "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.6 ollama-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a ChatOllama Instance**"
      ],
      "metadata": {
        "id": "clprEqhHbqCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Create a connection to the LLaMA3.1 model served in Ollama\n",
        "llm = ChatOllama(model=\"llama3.1\")\n",
        "\n",
        "# Try a English Prompt\n",
        "response = llm.invoke(\"Write about Future of artificial intelligence in iran.\")\n",
        "print(\"English Response:\\n\", response.content)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ukpy5Pj5ci0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f26cdac-5a24-4452-90ba-45fecf5d9cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Response:\n",
            " The future of artificial intelligence (AI) in Iran is a rapidly evolving and dynamic field, driven by the country's growing technological capabilities, increasing economic investment, and strategic partnerships with international companies.\n",
            "\n",
            "**Government Support and Investment**\n",
            "\n",
            "In recent years, the Iranian government has shown significant interest in developing AI capabilities within the country. The Ministry of Information and Communications Technology (ICT) has been actively working to promote AI research and development through various initiatives, including:\n",
            "\n",
            "1. **Iran's National AI Strategy**: Launched in 2020, this strategy aims to make Iran a regional leader in AI innovation by 2035.\n",
            "2. **Establishment of AI Research Centers**: Several research centers have been established across the country, focusing on areas like machine learning, natural language processing, and computer vision.\n",
            "3. **Funding for AI-related projects**: The government has allocated significant funds to support AI-related research and development projects.\n",
            "\n",
            "**Emerging Opportunities**\n",
            "\n",
            "Iran's growing AI ecosystem offers numerous opportunities in various sectors:\n",
            "\n",
            "1. **Healthcare**: Iran is leveraging AI to improve healthcare outcomes by developing intelligent systems for disease diagnosis, patient monitoring, and personalized medicine.\n",
            "2. **Smart Cities**: Many Iranian cities are implementing AI-powered smart city initiatives, which include IoT-based transportation management, energy efficiency, and public safety solutions.\n",
            "3. **E-commerce and Retail**: AI-driven e-commerce platforms are gaining traction in Iran, enabling consumers to interact with virtual assistants and receive personalized product recommendations.\n",
            "4. **Education**: The use of AI in education is increasing, with the development of intelligent tutoring systems and adaptive learning platforms.\n",
            "\n",
            "**Challenges and Concerns**\n",
            "\n",
            "While Iran's AI landscape holds great promise, several challenges and concerns need to be addressed:\n",
            "\n",
            "1. **Lack of Talent and Expertise**: Despite government efforts, a shortage of skilled professionals in AI remains a significant challenge.\n",
            "2. **Cybersecurity Risks**: As the use of AI increases, cybersecurity threats become more pressing, requiring specialized solutions and expertise.\n",
            "3. **Data Protection and Privacy**: Ensuring data protection and privacy in an era of widespread AI adoption is crucial to maintaining public trust.\n",
            "4. **International Sanctions and Restrictions**: Ongoing sanctions and restrictions imposed on Iran's technological sector pose significant hurdles for the country's AI development efforts.\n",
            "\n",
            "**Future Prospects**\n",
            "\n",
            "Despite these challenges, the future of AI in Iran looks promising:\n",
            "\n",
            "1. **Increased Investment**: Foreign investment in Iranian startups and research institutions is expected to grow, fueling innovation and growth.\n",
            "2. **Strategic Partnerships**: Collaborations between international companies and Iranian organizations will facilitate knowledge transfer, technology sharing, and joint project development.\n",
            "3. **Talent Development**: Efforts to attract and retain top talent from around the world are underway, aimed at bridging the skills gap in AI research and development.\n",
            "\n",
            "In conclusion, Iran's AI landscape is poised for significant growth and transformation over the next decade. With ongoing government support, strategic partnerships, and increasing investment, the country is expected to become a regional leader in AI innovation by 2035, driven by its unique cultural and socio-economic context.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Try a perian Prompt\n",
        "response_fa = llm.invoke(\" درباره آینده هوش مصنوعی  در ایران بنویس.\")\n",
        "print(\"\\nPersian Response:\\n\", response_fa.content)\n"
      ],
      "metadata": {
        "id": "I1dQQnBSoiod",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4ea4e9-0b83-4cfc-839e-b7ac6d8f80b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Persian Response:\n",
            " ایران در حال پیشرفت سریع در حوزهٔ هوش مصنوعی است و پیش‌بینی‌ها می‌گویند که در سال‌های آتی ایران به یکی از مهم‌ترین مراکز هوش مصنوعی تبدیل خواهد شد.\n",
            "\n",
            "تکنولوژی هوش مصنوعی (AI) آینده را شکل خواهد داد. فناوری‌های متعلق به این شاخه، مانند یادگیری ماشین، پردازش زبان طبیعی، بینایی محاسباتی و هوش مصنوعی عمومی می‌توانند زندگی ما را در آینده بسیار تغییر دهند.\n",
            "\n",
            "ایران با توجه به رکود اقتصادی در جهان از این موضوع استفاده کرده است. در حال حاضر ایران از نظر هزینه در سطح جهانی قرار دارد و از 40 کشور دارای فناوری AI است.\n",
            "در این کشور، دانشگاه‌ها برای تعالی در زمینهٔ هوش مصنوعی تلاش می‌کنند. بسیاری از کارگاه‌های توسعه‌ای، شرکت‌های دولتی و خصوصی در ایران به سمت رشد هوش مصنوعی پیشرفت کرده‌اند.\n",
            "\n",
            "ایران در آینده یکی از مهم‌ترین مراکز هوش مصنوعی خواهد شد.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mcqAuqrrtMc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2. : Using embedding models locally**"
      ],
      "metadata": {
        "id": "u3AX3WGJtOTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**we try first in CPU**"
      ],
      "metadata": {
        "id": "O-zGQTR803Vt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install Libraries**"
      ],
      "metadata": {
        "id": "-Y2GXLTv1N4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-huggingface langchain_community faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JQ7ijlO08W8",
        "outputId": "1d6fdc05-893b-4c4c-fff9-424499f15c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.71)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, faiss-cpu, typing-inspect, pydantic-settings, dataclasses-json, langchain-huggingface, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 faiss-cpu-1.11.0.post1 httpx-sse-0.4.1 langchain-huggingface-0.3.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itkG13mp1Gz_",
        "outputId": "3b68b554-3417-4cde-dcb7-9e7132003af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.3.71)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.33.4 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.1.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.11.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.33.4->langchain-huggingface) (2025.7.14)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-huggingface) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Call Huggingface Emmbeding**"
      ],
      "metadata": {
        "id": "kl6E44gQ1iRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "iUI6Geoh1iRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "hf_embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286,
          "referenced_widgets": [
            "fb1a241c2fd642c99548fc162efb630e",
            "0f1d5d8a924a4994ac6f50df9303f073",
            "260ce6a214bc43b9aac82489ee9b67ba",
            "77d47ac88d6b4686871bfbd58a39785e",
            "569b29e1f3ff4f60aa053f0e3ac8f8b3",
            "3c1f126eca274ca78940fed6a87193af",
            "86e06defb5d64731949e6451b7da308a",
            "b9c3bdbf2bbc470f9a37fa89f20d3d8f",
            "b145f25ce76d4e06bb2a3fd87503f5fb",
            "9078798398d54f9aaa69f30c782d260b",
            "8799819ee4c640baa6d634242730c2bc",
            "a55515a675d44d0d86fedf53a3a96a0b",
            "22e11a3c8ce84ef4aad43ef9fb8af295",
            "230747481d834702932ea9f994c89e93",
            "dd98bc59946743b99efd812545c0991c",
            "b668c1649f5246408389d04781d73bb6",
            "69aaded374174838a9e60616a49bb130",
            "26461d5aa1d048938738b4d4cfac47e9",
            "82f6d89e539647bf963e0c6f5367c933",
            "6e5dc562894643118042ea21ebca3ec1",
            "5bea5412b3bd4c74a1f0066b78cee043",
            "4304b6cf253847cf831a69381ce1b21b",
            "da5399621114460db73e0a1fc88a1451",
            "1ebdd2e201974f14bc5c7faef7b5834d",
            "f56dde12ad374a099dfbe46c06f462bd",
            "e12e4f9a7ddf473ea8a547d538f5d030",
            "331f4788c1f94407a58a022b00090827",
            "4ae28a34ea3446539de0e6c9068ebef9",
            "a7b260ef81904a38aea48d6de401ab03",
            "8ffa5bf8a9114303b9ecc291afd84d46",
            "252a32e43c0441278b9e8bad7002c351",
            "667188570b4b42cda7a90df01ab4278d",
            "acc3395d37e2475c91a9a0d6bce490e1",
            "49bec66036ea454c9ab0e45befe71078",
            "af8162fe8419400b9945f3efbb731f7a",
            "a116bf6710fd4e00b762f04626612fb0",
            "161349300cee46139d9d4f3f2b074c82",
            "77a94efc3b3140e3920c2f5f85f5bb27",
            "a05ede3191a744d49f51b37d248ae0d4",
            "5b3d5cd288604a8b9dfa1512ca792fc9",
            "565bfbd5b1f840d7b4ea11884c418e67",
            "84756faf93aa4756a7d2284bb7b18875",
            "c510aa9815db4a49a2895b35ea2bc689",
            "be726245c303415ebc87b8b2915a7afb"
          ]
        },
        "id": "DxgrdhIt1xQA",
        "outputId": "1d847606-3f99-47b2-b870-8758688f4d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name HooshvareLab/bert-base-parsbert-uncased. Creating a new one with mean pooling.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb1a241c2fd642c99548fc162efb630e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/654M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a55515a675d44d0d86fedf53a3a96a0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/654M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da5399621114460db73e0a1fc88a1451"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49bec66036ea454c9ab0e45befe71078"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"ما در هوشواره معتقدیم با انتقال صحیح دانش و آگاهی، همه افراد\n",
        "می‌توانند از ابزارهای هوشمند استفاده کنند. شعار ما هوش مصنوعی برای همه است.\"\"\"\n",
        "embed = hf_embedding.embed_query(text)\n",
        "print(len(embed))  # باید 768 چاپ کند\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx-GmsQt2BvH",
        "outputId": "5b3c846c-4407-4f6f-fa3e-d1dba37e55fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pE4o-y753fdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2.2 : Building VectorStore**\n"
      ],
      "metadata": {
        "id": "t3zGRyqK3geK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install doc2txt in order to transform doc file to txt file**"
      ],
      "metadata": {
        "id": "6eQv9pI15krG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q docx2txt"
      ],
      "metadata": {
        "id": "ijDg7DBG5JoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "tAWlgEyT34zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_transformers import Html2TextTransformer\n",
        "from langchain.document_loaders import Docx2txtLoader\n",
        "from langchain.docstore.document import Document"
      ],
      "metadata": {
        "id": "gXWsM2g-wP7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We want to see the number of characters and words in order to make a decision for using \"chunk_size\" and \"chunk_overlap\" rightly**"
      ],
      "metadata": {
        "id": "27UHnlibLOVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "from docx import Document\n",
        "\n",
        "file_path = \"/content/Proposal_erfan.docx\"\n",
        "\n",
        "doc = Document(file_path)\n",
        "\n",
        "full_text = \"\"\n",
        "for para in doc.paragraphs:\n",
        "    full_text += para.text + \"\\n\"\n",
        "\n",
        "# count characters and word of this docx file\n",
        "num_chars = len(full_text)\n",
        "num_words = len(full_text.split())\n",
        "\n",
        "print(f\"تعداد کاراکترها: {num_chars}\")\n",
        "print(f\"تعداد کلمات: {num_words}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sU2Qh27J4AqE",
        "outputId": "040f8155-08f9-419f-b18e-5c106f1c834f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "تعداد کاراکترها: 33038\n",
            "تعداد کلمات: 5669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = Docx2txtLoader('/content/Proposal_erfan.docx')\n",
        "docs = loader.load()  # لیست Document برمی‌گرداند\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)\n",
        "text_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "EECmPC-TLAVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sbb5VQzLNeRm",
        "outputId": "516ed85a-d144-4377-abc1-aa14fc59ac59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()\n",
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "EkMSAl8sQUKe",
        "outputId": "9b148866-e3aa-4f1c-9091-2651dbfab2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'مؤسسه آموزش عالی علوم شناختی\\n\\n\\n\\nطرح پایان نامه کارشناسی ارشد\\n\\n\\n\\n1ـ  اطلاعات مربوط به دانشجو:\\n\\n\\n\\nنام:      عرفان                              نام خانوادگی:    اسلامیه                               شماره دانشجويی: 40006102\\n\\nرشته وگرایش تحصیلی:  طراحی و خلاقیت شناختی                                       سال ورود:  1400\\n\\nنشانی و تلفن: \\n\\n\\n\\nرایانامه (ایمیل):  erfan.cognitive.work@gmail.com\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n2ـ استادان راهنما:\\n\\n\\n\\nرديف\\n\\nنام و نام خانوادگی\\n\\nتخصص\\n\\nآخرين مدرک تحصيلی\\n\\nرتبه دانشگاهی\\n\\nسنوات تدریس در  دوره کارشناسی\\u200cارشد\\n\\nمحل خدمت فعلی، \\n\\nتلفن\\n\\nرایانامه\\n\\n (ایمیل)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nسنوات تدریس در  دوره دکتری\\n\\n\\n\\n\\n\\n1\\n\\n\\n\\nپیمان حسنی ابهریان\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\nحمیدرضا مقامی\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n3 ـ استادان مشاور:\\n\\n\\n\\nرديف\\n\\nنام و نام خانوادگی\\n\\nتخصص\\n\\nآخرين مدرک تحصيلی\\n\\nرتبه دانشگاهی\\n\\nسنوات تدریس در  دوره کارشناسی\\u200cارشد\\n\\nمحل خدمت فعلی، \\n\\nتلفن\\n\\nرایانامه\\n\\n (ایمیل)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nسنوات تدریس در  دوره دکتری\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4 ـ پايان نامه:\\n\\n\\n\\n1ـ فارسي:  طراحی رابط دیجیتال بهینه وبسایت ها برای افراد دارای افت عملکرد  توجه و حافظه کاری و اعتبارسنجی اولیه آن\\n\\n\\n\\n\\n\\n2ـ لاتین:   \\n\\nDesigning the optimal digital interface of websites for people with reduced attention and working memory and its initial validation       \\n\\n\\n\\n\\n\\n\\n\\n1-4) عنوان\\u200c پایان\\u200cنامه\\n\\nتوجه: این فرم باید با مساعدت و هدایت استاد راهنما تکمیل شود.\\n\\n\\t\\n\\n2-4) واژگان کلیدی (بین 3 تا 5 واژه، به\\u200cترتیب حروف الفبا)\\n\\n1ـ فارسي: رابط دیجیتال، عملکرد توجه، عملکرد حافظه کاری\\n\\n\\n\\n2ـ لاتین:   Digital interface, attention performance, working memory performance      \\n\\n\\n\\n\\t\\n\\n3-4) نوع پژوهش:          بنیادی              نظری           کاربردی        توسعه\\u200cای\\n\\n4-4) تعداد واحد پایان نامه: \\n\\n5-4) مدت اجرا: \\n\\n\\t\\n\\n\\t6-4) بيان مسئله پژوهش:\\n\\n\\n\\nهدف بیان مسئله این است که نشان دهد چه چیزی مورد مطالعه است و در حقیقت چرایی انتخاب موضوع را نشان میدهد. از مستندات نظری و پژوهشی (نگاه اجمالی به تحقیقات قبلی) برای بیان مسئله و تعریف مفاهیم استفاده میشود. در پاراگراف آخر دغدغه پژوهشگر در جامعه آماری آورده می\\u200cشود و با ذکر سوال اصلی پژوهش بیان مساله پایان می\\u200cپذیرد.\\n\\n\\n\\n\\tرابط کاربری دیجیتال به عنوان جزء مهمی از یک وبسایت و به مثابه\\u200cی اولین مواجهه کاربر با وبسایت، مسئول نمایش صفحه های مختلف و ارتباط کاربر با وبسایت است که دربرگیرنده پارامترهای طراحی تجربه و اجزای مختلفی مانند منوی ناوبری، فرم ها، دکمه ها، نوار ابزار و غیره است. این رابط باید برای کاربران راحت، قابل دسترس و استفاده باشد و کاربران باید بتوانند با استفاده از اجزای رابط کاربری، به راحتی به هدف خود از استفاده رابط کاربری دسترسی پیدا کنند(Smith, 2022). طراحی رابط باید از نظر بصری جذاب و آسان برای کار کردن باشد تا کاربران را به تعامل با وب سایت و ماندن طولانی تر تشویق کند. یکی از تأثیرات اولیه رابط دیجیتال بر شاخص تعامل کاربر، مدت زمانی است که کاربران در وب سایت یا برنامه حضور فعال دارند. یک رابط کاربری با طراحی ضعیف می\\u200cتواند باعث سرخوردگی و سردرگمی شود که درنهایت منجر به کاهش تعامل و زمان صرف شده کاربر در وبسایت خواهد شد (Lee, 2020). یک رابط بصری با حجم مناسب از اطلاعات می\\u200cتواند مردم را به کاوش بیشتر ترغیب کند و از این رو باعث کاهش نرخ خروج افراد و افزایش تعامل \\u200cشود(بابایی، م،  & محمدی, 1399). به دلیل اهمیت تاثیر چیدمان و سازماندهی رابط دیجیتال بر تعامل کاربر، رابط بهم ریخته و نامرتب می تواند در کاربر ایجاد احساس سردرگمی و بی نظمی کند که درنهایت منجر به طاقت فرسا شدن حضور کاربر در وبسایت خواهد شد (Davis & Moreno, 2020) . از سوی دیگر، یک رابط کاربری سازمان یافته با ناوبری واضح و یک جریان منطقی قادر است کاربران را از طریق وب سایت یا برنامه راهنمایی و به تعامل بیشتر تشویق کند. در نتیجه اولویت بندی این عناصر در فرآیند طراحی بسیار مهم است(Smith, 2021). باتوجه به اهمیت چیدمان و سازماندهی رابط دیجیتال می توان گفت که طراحی بهینه رابط دیجیتال در تعدیل موارد فوق موثر خواهد بود. طراحی بهینه رابط دیجیتال پیش از همه باید کاربر محور، کارآمد، زیباشناختی و قابل دسترس برای همه کاربران، از جمله افراد دارای معلولیت باشد. هدف طراحی بهینه و کاربر محور ساخت محصولاتی است که به راحتی قابل درک، یادگیری و استفاده باشند و در عین حال بار شناختی کاربران را به حداقل برسانند. این امر مستلزم در نظر گرفتن عوامل مختلفی مانند قابلیت استفاده، یادگیری پذیری، کارایی، رضایت، دسترسی، زیبایی شناسی، سازگاری، انعطاف پذیری، وضوح، بازخورد، متقاعدسازی، احساسات و حواس پرتی است (Lee, 2021).  در واقع مدل طراحی کاربر محور یک رویکرد طراحی مبتنی بر چرخه تکرار است که بر نیازها، اهداف و رفتارهای کاربران برای ایجاد محصولات یا سیستم های موثر و قابل استفاده تمرکز دارد(Johnson, 2020). طراحی بهینه وب سایت می تواند در دستگاه ها و اندازه های مختلف صفحه نمایش منعطف باشد و دسترسی کاربران را حتی در حال حرکت بهبود بخشد. عناصر طراحی از نظر زیبایی شناختی می تواند تجربه کاربررا بهبود بخشیده و منجر به رضایت بیشتر کاربر و بازدیدهای مکرر شود (رجبی & همکاران, 1399). طراحی ساده و مینیمال می تواند بار شناختی را کاهش و به کاربران این امکان را دهدکه تکالیف را با سرعت و دقت بیشتری انجام دهند. همچنین طراحی سایت بر این مبنا می تواند هویت برند را تقویت کرده و اعتماد کاربران را ارتقا دهد  (Williams, 2019). از طرفی دیگر طراحی سایت های سازگار با موبایل می تواند تجربه کاربر را در هنگام استفاده از دستگاه های تلفن همراه بهبود بخشیده، نرخ پرش را کاهش داده و تعامل کاربر را ارتقا دهد. با این وجود حجم زیاد اطلاعات ارائه شده در یک صفحه می تواند فیلتر کردن اطلاعات مربوطه را برای آنها دشوار کند و منجر به سردرگمی و ناامیدی از ادامه مسیر در وبسایت شود (Doe, 2020)؛ اما وجود عناصر منحرف کننده، مانند تبلیغات، پنجره های بازشو و اعلان ها، می توانند تمرکز وحافظه کاری را مختل کنند. چرا که انجام چند عملکرد ساده به صورت همزمان می تواند برای افراد مبتلا به اختلالات توجه و حافظه کاری چالش برانگیز باشد، زیرا ممکن است برای جابجایی بین تکالیف مختلف یا پیگیری چندین تکلیف به طور همزمان دچار مشکل شوند (Smith, 2019). این درحالی است که با اولویت\\u200cبندی نیازها و ترجیحات کاربران مبتلا به اختلالات توجه و حافظه کاری، طراحی کاربر محور می\\u200cتواند محصولات، خدمات و سیستم\\u200cهای در دسترس، شهودی و پشتیبانی\\u200cکننده\\u200cتری ایجاد کند. همچنین می\\u200cتواند به رفع مشکلات خاص مانند مشکل در توجه مداوم، سازماندهی و برنامه\\u200cریزی، اضافه بار حسی، تکانشگری و حواس\\u200cپرتی کمک کند (Smith, 2018).در واقع افت عملکرد توجه و افت حافظه کاری، دو اختلال شناختی هستند که باعث می\\u200cشوند فرد نتواند اطلاعات موقت را به خوبی نگه\\u200cداری و پردازش کند. این اطلاعات ممکن است شامل صدا، تصویر، متن، عدد، مکان و غیره باشند. این اختلالات می\\u200cتوانند تأثیر منفی بر عملکرد تحصیلی، شغلی، اجتماعی و شخصی فرد داشته باشند. افت عملکرد توجه به معنای عدم توانایی فرد در تمرکز بر روی یک موضوع یا تکلیف، سرکوب کردن حواس\\u200cپرتی، انتخاب کردن اطلاعات مربوط و تغییر توجه بر اساس نیاز است. افت حافظه کاری به معنای عدم توانایی فرد در نگه\\u200cداری و دستکاری اطلاعات موقت برای حل مسائل، استدلال و تصمیم\\u200cگیری است. همچنین بر طبق تحقیقات پیشین، پیش بینی می شود که در آینده ای نه چندان دور، با بکار رفتن فناوری واقعیت افزوده و واقعیت مجازی، طراحی رابط های دیجیتال بهینه برای افراد دارای افت عملکرد توجه و حافظه کاری احتمالاً حیاتی\\u200cتر خواهد شد. این فناوری\\u200cها به درجه بالایی از پردازش شناختی و تمرکز نیاز دارند و در آینده بایستی طراحان برای استفاده از این ابزار ها به منظور بهبود رابط دیجیتال در افراد دارای افت عملکرد توجه و حافظه کاری بار شناختی و میزان پردازش شناختی مصرفی را در هنگام ایجاد رابط برای آنها در نظر بگیرند و آن را به حداقل برسانند. علاوه بر این، با افزایش هوش مصنوعی و یادگیری ماشین ، نیاز به رابط\\u200cهایی وجود خواهد داشت که سازگارتر باشند و بتوانند پردازش های شناختی خود را بر اساس توانایی\\u200cها و ترجیحات  این دسته از کاربران تنظیم کنند. این امر مستلزم آن است که طراحان نه تنها بار شناختی یک کار یا رابط کاربری خاص، بلکه ویژگی پروفایل های شناختی فردی کاربران را نیز در نظر بگیرند. از این رو ما بر آن هستیم که با ایجاد یک رابط دیجیتال بهینه در وبسایت ها برای هر دو گروه یعنی افراد دارای افت عملکرد توجه و حافظه کاری فضایی مناسب برای پوشش افت عملکردی آن ها ایجاد کنیم. هدف از انجام این پژوهش پاسخ به این سوال است که آیا ارتباط معنی\\u200cداری میان طراحی بهینه وبسایت\\u200cها و پردازش اطلاعات و تکلیف در وبسایت\\u200cها برای کاربران دارای افت عملکردهای شناختی حافظه و توجه وجود دارد؟\\n\\n\\t\\n\\n\\t7ـ4) ضرورت و اهمیت انجام پژوهش:\\n\\n\\t\\n\\nاهمیت (کاربردی):\\n\\nافراد با افت عملکرد توجه و حافظه کاری، افرادی هستند که دچار مشکلات شناختی در نگه\\u200cداری و پردازش اطلاعات موقت هستند و این می\\u200cتواند باعث اختلال در عملکرد تحصیلی، شغلی، اجتماعی و شخصی آن\\u200cها شود. این اختلالات ممکن است به عوامل مختلفی مانند اختلالات روانپزشکی، اختلالات عصبی، آسیب\\u200cهای مغزی، مصرف مواد، کم\\u200cخوابی، استرس و اضطراب مربوط باشند .طراحی رابط دیجیتال بهینه وبسایت ها برای افراد دارای افت عملکرد توجه و حافظه کاری، یک رویکرد جدید و نوآورانه است که با استفاده از رابط دیجیتال، سعی می\\u200cکند تجربه کاربری این افراد را بهبود بخشد. این طراحی، با توجه به الگوهای رفتاری و نیازهای فردی این افراد، محتوا، ساختار و ظاهر وبسایت را سفارشی\\u200cسازی می\\u200cکند و می\\u200cتواند مشکلات این افراد را در چند جنبه مانند افزایش تمرکز، حافظه، سرعت و کاهش حواس\\u200cپرتی، فراموشی، اشتباهات و سردرگمی حل کند. این طراحی، هنوز در مرحله اولیه اعتبارسنجی است و نیاز به بررسی\\u200cهای بیشتر دارد. \\n\\nضرورت (بنیادی): \\n\\nدر این پژوهش با توجه به افزایش تعداد افراد دارای افت عملکرد توجه و حافظه کاری در جامعه و نیاز آن\\u200cها به استفاده از وبسایت\\u200cها و اپلیکیشن\\u200cهای مختلف، لازم است که رابط دیجیتال این سرویس\\u200cها به گونه\\u200cای طراحی شود که با الگوهای رفتاری و نیازهای فردی این افراد سازگار باشد و تجربه کاربری آن\\u200cها را بهبود بخشد. این پژوهش می\\u200cتواند به طراحان و توسعه\\u200cدهندگان وبسایت\\u200cها و اپلیکیشن\\u200cها کمک کند تا با استفاده از طراحی وبسایت، رابط دیجیتال خود را به صورت سفارشی\\u200cسازی شده برای افراد دارای افت عملکرد توجه و حافظه کاری طراحی کنند و از ارائه محتوا، ساختار و ظاهر یکسان برای همه کاربران خودداری کنند. این پژوهش می\\u200cتواند به افزایش رضایت، کارایی، لذت و سرگرمی این افراد از استفاده از وبسایت\\u200cها و اپلیکیشن\\u200cها منجر شود و در نتیجه به ارتقای کیفیت زندگی و عملکرد آن\\u200cها کمک کند.\\n\\n\\n\\nسوابق پژوهش فارسی:\\n\\nاحمدی و رضایی(1397) در پژوهشی با عنوان \"طراحی وب سایت دانشگاه با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد نابینا و کم\\u200cبینا با استفاده از روش تحلیل سلسله مراتبی فازی\" به بررسی تجربه کاربر و دسترسی پذیری در طراحی وبسایت برای افراد نابینا یا کم بینا پرداخته است در این پژوهش به منظور بهبود دسترسی پذیری وبسایت برای افراد نابینا یا کم بینا از ایجاد سلسله مراتب منظم و مشخص در عناوین اصلی وبسایت استفاده شده است (احمدی  & رضایی, 1397).\\n\\nبابایی و محمدی (1399) در پژوهشی تحت عنوان \" \\xa0طراحی وب سایت با رویکرد دسترس\\u200cپذیری برای افراد دارای ناتوانی شنوایی \"  به بررسی تاثیر نقش طراحی وب سایت با رویکرد دسترس\\u200cپذیری برای افراد دارای ناتوانی شنوایی مورد بررسی قرار داد؛ در این پژوهش با استفاده از تصاویر بزرگ و واضح و هچنین ایجاد صدای واضح در پس زمینه به ایجاد یک رابط مناسب برای دسترسی پذیری افراد این طیف پرداخته است (بابایی، م،  & محمدی, 1399).\\n\\nجعفرپور و صادقی(1401) در یک تحقیق دیگر با عنوان \"طراحی وب\\u200cسایت با تأکید بر تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای محدودیت حرکتی\"، تأثیر طراحی وب\\u200cسایت با توجه به تجربه کاربری و دسترس\\u200cپذیری برای افراد مبتلا به محدودیت حرکتی مورد بررسی قرار گرفته است. این تحقیق همچنین با تمرکز بر روی افرادی که با محدودیت حرکتی روبه\\u200cرو هستند، به ارائه یک راه\\u200cحل تجربی منطبق با نیازهای این افراد می\\u200cپردازد.(جعفرپور، م،  & صادقی, 1401).\\n\\nحسین زاده ورضاییان(1395) در تحقیقی  مشابه با عنوان\" طراحی وب سایت با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای ناتوانی بینایی \"  به بررسی نقش و اثرگذاری طراحی وب سایت با توجه به رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای ناتوانی بینایی پرداخته است ، همچنین این پژوهش مشابه پژوهش اول با بررسی افراد دارای ناتوانی بینایی اما اینبار با تکیه بر دسترسی پذیری وبسایت مورد مطالعه قرار داده است  (حسین\\u200cزاده، ع & رضائیان, 1395).\\n\\nخسروانژاد (1400) در یک پژوهش به نام \"طراحی وب\\u200cسایت با تمرکز بر تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای مشکلات ذهنی\"، نقش رابط دیجیتال با استفاده از رابط کاربری مناسب در طراحی وب\\u200cسایت برای افراد دارای مشکلات ذهنی مورد بررسی قرار گرفته است. در این تحقیق، با تأکید بر دسترس\\u200cپذیری در صفحه اصلی وب\\u200cسایت برای افراد با مشکلات ذهنی، پیشنهاد شده است که فضایی با فهم و ساده برای آنها ارائه شود.(خسروانژاد, 1400).\\n\\nرضاییان(1394) در تحقیقی با عنوان \" طراحي وب سايت با رويكرد تجربه كاربري و دسترس\\u200cپذيري براي افراد داراي ناتواني شنيدار \" به اثرگذاری  نقش طراحی وبسایت با توجه به  رويكرد تجربه كاربري و دسترس\\u200cپذيري براي افراد داراي ناتواني شنيدار پرداخته است، در این پژوهش نیز با تکیه بر دانش طراحی رابط کاربری منطبق با دسترسی پذیری بالا ، فضایی تصویری مناسب در وبسایت ها با پس زمینه ای از صداهایی با فرکانس مشخص برای این افراد به منظور هدایت در سایت ساخته شده است (رضائيان, 1394).\\n\\n\\n\\n\\n\\n\\n\\nسوابق پژوهش انگلیسی:\\n\\nآبسکل و نیکول(2015) در پژوهشی تحت عنوان \" حرکت به سمت دستورالعمل های طراحی فراگیر برای HCIآگاه از لحاظ اجتماعی و اخلاقی \" به طراحی رابط کاربری بر اساس دستورالعمل HCI  و طراحی از لحاظ اجتماعی و اخلاقی پرداخته است(Abascal & Nicolle, 2015).\\n\\nبولر و استفاندیس(2014) در تحقیقی با عنوان \" طراحی فراگیر محیط های هوش محیطی: تعامل کاربر، زمینه و فناوری \" به بررسی کلی محیط های هوشمند بر اساس هوش محیطی و تاثیر تعامل کاربر  محور و نقش فناوری در بهبود عملکرد آن را بررسی کرده است (Bühler & Stephanidis, 2014).\\n\\nجیمز کلارک و لین کلارک(2016) در پژوهشی با  عنوان \" طراحی برای وب: بررسی روش ها و دستورالعمل های طراحی وب از جمله مسائل دسترسی و قابلیت استفاده \" انواع روش ها و دستورالعمل های موجود برای طراحی وب به منظور بهبود دسترسی پذیری و قابلیت استفاده از وبسایت ها را بررسی کرده است(Clark & Clark, 2016).\\n\\nدیاز بوسینی ومونرو(2014) در تحقیقی تحت عنوان \" دسترسی به رابط های تلفن همراه برای افراد مسن \" به نقش طراحی مناسب و بهینه  رابط های کاربری در تلفن همراه برای ایجاد قابلیت استفاده آسان و همچنین دسترسی پذیری برای افراد مسن را مورد ارزیابی قرار داده است(Davis & Moreno, 2020).\\n\\nهنسون و ریچارد(2013) در تحقیق دیگر با عنوان پژوهشی \" پیشرفت در دسترسی به وب سایت؟  \" به آینده پیشرفت در دسترسی پذیری رابط های دیجیتال طراحی شده متناسب با هر قشری از جامعه و بررسی ویژگی های کلی آن ها در وب سایت ها پرداخته است(Hanson & Richards, 2013).\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t9-4) هدف\\u200cهای پژوهش\\n\\n\\t\\n\\n1-9-4) هدف کلی:\\n\\n\\tطراحی رابط دیجیتال بهینه وبسایت ها برای افراد دارای افت عملکرد  توجه و حافظه کاری و اعتبارسنجی اولیه آن\\n\\n\\t\\n\\n\\t\\n\\n\\t2-9-4) هدف\\u200cهای جزئئ:\\n\\n\\t\\t\\tطراحی رابط دیجیتال بهینه وبسایت ها برای افراد دارای افت عملکرد  توجه و حافظه کاری و اعتبارسنجی اولیه آن\\n\\n2- بررسی تاثیر طراحی رابط دیجیتال بهینه وبسایت بر بهبود کارکرد اجرایی افراد دارای اختلال توجه\\n\\n3- بررسی تاثیر طراحی رابط دیجیتال بهینه وبسایت بر بهبود کارکرد اجرایی افراد دارای اختلال حافظه کاری \\n\\n\\t\\n\\n\\t\\n\\n\\t10-4) فرضیه\\u200cها\\u200c:\\n\\n\\t\\n\\n\\tطراحی رابط دیجیتال بهینه وبسایت ها بر کارکرد اجرایی افراد دارای اختلال توجه و حافظه کاری تاثیر مثبت دارد.\\n\\n\\t\\n\\n\\t طراحی رابط دیجیتال بهینه وبسایت بر بهبود کارکرد اجرایی افراد دارای اختلال توجه تاثیر مثبت دارد.\\n\\n\\t\\n\\n\\t طراحی رابط دیجیتال بهینه وبسایت بر بهبود کارکرد اجرایی افراد دارای اختلال حافظه کاری تاثیر مثبت دارد.\\n\\n\\t\\n\\n\\t11-4 ) متغیرهای پژوهش:\\n\\n\\n\\nرابط دیجیتال\\n\\n\\tدر دنیای پیشرفته و فناورانه امروز، نقش رابط\\u200cهای دیجیتال به عنوان اصولی بنیادین در تعامل انسان و فناوری به شدت افزایش یافته است. این رابط\\u200cها نه تنها وسیله\\u200cهایی برای ارتباط انسان با فناوری انتقال داده\\u200cها هستند، بلکه به طور فراگیر تجربه کاربری را بهبود می\\u200cبخشند، تبادل اطلاعات را ساده\\u200cتر می\\u200cکنند و ارتباطات انسانی را تسهیل می\\u200cکنند. به همین دلیل، ادبیات پژوهش در زمینه رابط\\u200cهای دیجیتال با تمرکز بر تحلیل و بررسی اجزای تشکیل\\u200cدهنده و اصول طراحی این رابط\\u200cها اهمیت ویژه\\u200cای پیدا کرده است(Lazar et al., 2017).\\n\\n\\t\\n\\n\\tتعریف مفهومی رابط دیجیتال\\n\\n\\tرابط دیجیتال در مفهوم کلی به مجموعه از عناصر و ویژگی\\u200cهای گرافیکی، طراحی صفحات و اجزا مرتبط با یک وبسایت یا برنامه وب اشاره دارد که ارتباط بین کاربر و سیستم را برقرار می\\u200cکند. این رابط شامل اقلامی نظیر دکمه\\u200cها، منوها، ترتیب المان\\u200cها، رنگ\\u200cها، نمایش اطلاعات، و همچنین تجربه کاربری به منظور انتقال اطلاعات و انجام وظایف مختلف برای کاربران می\\u200cباشد(Cooper et al., 2017).\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\tتعریف عملیاتی رابط دیجیتال\\n\\n\\t\\tرابط دیجیتال در عمل به عنوان واسطه\\u200cای بین کاربران و محتوا یا عملکرد وبسایت عمل می\\u200cکند. این شامل عناصر مانند دکمه\\u200cها برای انجام عملیات خاص، فرم\\u200cها برای جمع\\u200cآوری اطلاعات از کاربران، منوها برای جابجایی بین صفحات مختلف و همچنین تنظیمات تجربه کاربری می\\u200cشود. رابط دیجیتال هدف دارد تا ارتباط کاربر با وبسایت را به صورت شناخته\\u200cشده، موثر و آسانی برای افرادی که دارای افت عملکرد توجه و حافظه کاری هستند، ایجاد کند. این اقلام طراحی و ویژگی\\u200cها باید با توجه به نیازهای افراد با افت عملکرد مذکور بهبود یابند تا کاربران بتوانند به بهترین شکل ممکن از وبسایت استفاده کنند؛ همچنین در این پژوهش از دستگاه ردیاب چشمی (Eye tracker) به منظور ارزیابی میزان ساکادهای چشمی آزمون دهندگان در هنگام استفاده از رابط دیجیتال وبسایت ،  برای  سنجش و جمع آوری اطلاعات نهایی از نحوه ی عملکرد وبسایت برای استفاده بهتر این افراد استفاده خواهد شد (Lazar et al., 2017).\\n\\n\\t\\t\\n\\n\\t\\tعملکرد توجه \\n\\n\\t\\tدر جهان پیچیده و پویا امروز، توجه به یکی از ویژگی\\u200cهای کلیدی انسان\\u200cهاست که نقش حیاتی در تعامل با محیط اطراف و انجام وظایف روزمره دارد. توانایی تمرکز بر یک وظیفه خاص و نیز تغییر تمرکز در مقابل محرک\\u200cهای متعدد محیطی، از مهارت\\u200cهای اساسی برای انجام فعالیت\\u200cها و بهره\\u200cبرداری از فرصت\\u200cها است. به منظور بهبود کارایی و کیفیت انجام وظایف، پژوهش\\u200cهای متعددی در زمینه عملکرد توجه انجام شده است. این پژوهش\\u200cها به تعریف و تحلیل مفاهیمی چون توانایی تمرکز، نگه\\u200cداشتن توجه، تغییر تمرکز و کنترل منابع توجه می\\u200cپردازند. در این مقدمه، ما قصد داریم با معرفی مفاهیم و تعاریف مفهومی و عملیاتی عملکرد توجه، به عنوان مهمترین ویژگی شناختی انسان، پایه\\u200cریزی کنیم و به تبیین اهمیت این عامل در تجربه انسانی و کارکردهای مختلف زندگی پرداخته و نیز به منابع مرتبط با این حوزه ارجاع دهیم(Posner & Petersen, 2007).\\n\\n\\t\\t\\n\\n\\t\\tتعریف مفهومی عملکرد توجه\\n\\n\\t\\tطبق DSM-5،افرادی که دست کم شش نشانه اصلی یا بیشتر اختلال کاستی توجه را دست کم به مدت شش ماه از خود نشان دهند، منجر به ایجاد ناهماهنگی های موثر بر سطح رشدشان می شود و در صورت اتفاق دچار اختلال بی توجهی یا کم توجهی خواهند شد که می تواند بر عملکرد آن ها در کارهای ضروری اثرات منفی از خود به جای بگذارند(سادوک، et al., 2013).\\n\\n\\t\\n\\n\\t\\tتعریف عملیاتی افت عملکرد توجه\\n\\nاز پرسشنامه کانرز که یک پرسشنامه استاندارد نرم شده بالینی است میتوان برای افت عملکرد توجه استفاده کنیم  و در کنار این ابزار قلم – کاغذی از تست IVA  برای تشخیص و اندازه گیری دقیق علائم ADHD استفاده خواهد شد . این تست ترکیبی از دو آیتم بی توجهی و تکانشگری در دو حالت دیداری و شنیداری است. همچنین از این تست برای اندازه گیری توجه و خود کنترلی در اختلالات عصبی –رشدی و روانپزشکی استفاده میشود(Corbett & Constantine, 2006).   \\n\\nحافظه کاری\\n\\n\\t\\tدر دستور راهنمای تشخیصی و آماری اختلالات روانی، ویرایش پنجم DSM-5 مفهوم حافظه کاری به عنوان یکی از عوامل مهم در فرآیندهای شناختی مورد بررسی قرار گرفته است. حافظه کاری به طور کلی به توانایی مغز برای ذخیره و مدیریت اطلاعات در مدت زمان کوتاه\\u200cتر اشاره دارد. این نوع حافظه نقش اساسی در وظایف روزمره مانند تمرکز، برنامه\\u200cریزی، انجام وظایف چندگانه و کنترل عملکرد\\u200cهای اجرایی ایفا می\\u200cکند (سادوک، et al., 2013).\\n\\n\\t\\t\\n\\nتعریف مفهومی حافظه کاری:\\n\\n\\t\\tحافظه کاری (حافظه فعال) در اختلالات روانی مطابق با DSM-5 به توانایی مغز اطلاق می\\u200cشود که اطلاعات را به مدت زمان محدودی نگهداری و در طول انجام وظایف شناختی، مانند تمرکز، برنامه\\u200cریزی و کنترل عملکرد\\u200cهای اجرایی، به کار گیرد. این نوع حافظه جزء از سامانه\\u200cهای فرآیندهای شناختی مغز است که در حفظ و مدیریت اطلاعات در مدت زمان کوتاه کمک می\\u200cکند(سادوک، et al., 2013).\\n\\n\\t\\t\\n\\nتعریف عملیاتی حافظه کاری:\\n\\n\\t\\tیکی از آزمون\\u200cهای شناختی برای ارزیابی حافظه کاری و توانایی تمرکز شناختی افراد ، آزمون تعداد نگاشت (N-Back Task) است. این آزمون به ویژه در تحقیقات روان\\u200cشناسی و علوم اعصاب برای مطالعه عملکرد حافظه کاری و نحوه پردازش اطلاعات در مغز استفاده می\\u200cشود. در این آزمون افراد باید به ترتیب زمانی نسبت به اطلاعات قبلی ورودی، اطلاعات جدید را تشخیص داده و ارزیابی کنند . آزمون به بررسی توانایی نگهداری و بازخوانی اطلاعات در مدت زمان کوتاه می\\u200cپردازد و میزان توجه و تمرکز را نیز ارزیابی می\\u200cکند(سادوک، et al., 2013) .\\n\\n\\t12-4) روش\\u200c پژوهش\\n\\n\\t1-12-4) طرح پژوهش: \\n\\n\\tطرح کلی این پژوهش از نوع کاربردی است به این صورت که از علوم شناختی و تکنیکهای طراحی جهت ارائه بهینه سازی رابط محیط دیجیتال وبسایتها در پاسخ به نیاز افراد دارای افت عملکرد توجه وحافظه کاری استفاده خواهد شد. در نتیجه این پژوهش دارای محصول نهایی است که قابل استفاده در جوامع مذکور خواهد بود. کاربرد نتایج این پژوهش به صورت عملیاتی خواهد بود که با ارائه راه حل های عملی برای مشکلات جامعه هدف تنظیم شده است. \\n\\n\\t\\n\\n\\t2-12-4) تشریح کامل روش تحقیق:  \\n\\nروش تحقيق آزمايشی\\n\\n\\t\\nدر این روش پژوهش، هدف بررسی اثر یک رابط کاربری دیجیتال وبسایت بر عملکرد توجه و حافظه کاری دانشجویان موسسه آموزش عالی شناختی در سال تحصیلی 1402 در بازه سنی درنظر گرفته شده بین 20 تا 27 سال است که دارای اختلال افت عملکرد توجه و حافظه کاری\\xa0خواهند بود. نوع آزمایش در این پژوهش به صورت\\xa0پیش آزمون  -پس آزمون\\xa0خواهد بود.\\n\\n\\tحجم نمونه در این پژوهش به وسیله نرم افزار G power با توجه به اندازه اثر مورد نظر، قدرت آماری و سطح اطمینان 28 نفر محاسبه شده است (شکل 1-1). روش نمونه گیری از نوع غیرتصادفی و دردسترس\\xa0بوده\\xa0و شرکت\\u200cکنندگان از طریق اعلامیه\\u200cهایی که در موسسه عالی علوم شناختی پخش می شود، جذب\\xa0خواهند شد. شرط لازم برای ورود به پژوهش تشخیص اختلال افت عملکرد توجه و حافظه کاری\\xa0است و با استفاده از دو آزمون IVA (آزمون توجه و تمرکز) و N-back (آزمون حافظه کاری) پیش از انجام مداخله بر روی آزمودنی، مورد ارزیابی قرار\\xa0خواهند گرفت. متغیر مستقل این پژوهش رابط کاربری دیجیتال وبسایت\\xa0است\\xa0که با استفاده از نرم افزار Figma طراحی شده و به صورت آنلاین مورد ارزیابی قرار\\xa0خواهد گرفت. طراحی رابط کاربری مورد هدف این پژوهش صفحه اصلی از یک وبسایت\\xa0است\\xa0که در آن\\u200cها افت عملکرد توجه و حافظه کاری افراد در هنگام کار با این رابط کاربری سنجیده\\xa0خواهد شد.\\n\\n\\t.\\n\\n\\t\\n\\n\\t\\n\\n\\tشکل (1-1) مقدار حجم نمونه بدست آمده با استفاده از نرم افزار G power  را نشان میدهد\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t \\n\\n\\tجدول متغییرها:\\n\\n\\tردیف\\n\\n\\tعنوان متغیر\\n\\n\\tنقش متغیر\\n\\n\\tنوع متغیر\\n\\n\\tتعریف علمی - عملی\\n\\n\\tمقیاس\\n\\n\\tنحوه اندازه گیری\\n\\n\\t  \\n\\n\\t\\n\\n\\tمستقل\\n\\n\\tوابسته\\n\\n\\tکمی\\n\\n\\tکیفی\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\tپیوسته\\n\\n\\tگسسته\\n\\n\\tاسمی\\n\\n\\tرتبه ای\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t1\\n\\n\\tطراحی رابط دیجیتال بهینه\\n\\n\\tمستقل\\n\\n\\tکیفی\\n\\n\\tویژگی\\u200cهای بصری، تعامل کاربری و دسترسی\\t\\n\\n\\tمقیاس\\u200cهای کیفی طراحی (نظرسنجی کاربری)\\t\\n\\n\\tارزیابی توسط داده های UX/UI و نظرسنجی کاربر\\n\\n\\t2\\n\\n\\tعملکرد توجه\\t\\n\\n\\tوابسته\\t\\n\\n\\tکمی\\n\\n\\tمدت زمانی که کاربران می\\u200cتوانند روی یک وظیفه متمرکز بمانند\\t\\n\\n\\tزمان صرف\\u200cشده برای تکمیل وظایف\\t\\n\\n\\t انجام آزمون  توجه و تمرکز  (IVA)\\n\\n\\t3\\n\\n\\tحافظه کاری\\t\\n\\n\\tوابسته\\n\\n\\tکمی\\n\\n\\tتوانایی ذخیره و بازیابی اطلاعات کوتاه\\u200cمدت\\t\\n\\n\\tتعداد آیتم\\u200cهای به\\u200cیادمانده\\t\\n\\n\\tانجام آزمون ارزیابی حافظه کاری (N-Back)\\n\\n\\t4\\n\\n\\tشرایط محیطی (عوامل مداخله\\u200cگر)\\t\\n\\n\\tمداخله\\u200cگر\\n\\n\\tکیفی/ کمی\\n\\n\\tمیزان خستگی، نویز محیطی و شرایط استفاده\\t\\n\\n\\tمقیاس شرایط محیطی\\t\\n\\n\\tانجام تسک شناختی با دستگاه ردیاب چشمی\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t3-12-4) ابزار گردآوری داده\\u200cها:\\n\\n\\t\\n\\n\\tتست IVA (آزمون توجه و تمرکز):\\n\\n\\tتوضیح: تست IVA یک آزمون توجه و تمرکز است که به منظور اندازه\\u200cگیری عملکرد توجه و کارایی توجه افراد به کار می\\u200cرود. این تست به افراد مختلف اطلاعات تصویری و متنی نشان داده و از آنها خواسته می\\u200cشود تا وظایف مشخصی را انجام دهند که نیازمند توجه و پاسخ به اطلاعات ارائه شده است.\\n\\n\\tاستانداردها: برای این تست، استانداردهای روایی و پایایی مورد نیاز برای اعتبارسنجی آزمون بسیار مهم هستند. این استانداردها باید تایید شده باشند تا از صحت و قابلیت اندازه\\u200cگیری صحیح توجه اطمینان حاصل شود.\\n\\n\\tتست: N-back\\n\\n\\tتوضیح : آزمون N-back، یک ابزار مفید برای ارزیابی حافظه کاری افراد است که در آن، افراد به ترتیب سری ای از موارد را یادداشت می\\u200cکنند و سپس باید مواردی را بازخوانی کنند که N مورد قبلی آنها بوده است. این آزمون قادر است توانایی حافظه کاری و مهارت مدیریت اطلاعات در حافظه افراد را نمایان سازد.\\n\\n\\tاستانداردها: برای این تست نیز استانداردهای روایی و پایایی بسیار مهم هستند تا اعتبار این تست تضمین شود. اطمینان از اینکه تست واقعاً حافظه کاری را اندازه\\u200cگیری می\\u200cکند از اهمیت بالایی برخوردار است.\\n\\n\\t\\n\\n\\tدستگاه eyetracker (ردیاب چشمی):\\n\\n\\tدستگاه eyetracker یک فناوری  با استفاده از یک دوربین و یک مانیتور است ، که حرکات چشم و نقطه تمرکز نگاه شخص را ثبت و تحلیل می\\u200cکند. این فناوری می\\u200cتواند اطلاعات ارزشمندی از رفتار، توجه، هوشیاری و حالات ذهنی شخص به دست آورد. دستگاه eyetracker در حوزه\\u200cهای مختلفی مانند تجربه کاربری، بازاریابی، علوم شناختی، آموزش و تعامل انسان و رایانه کاربرد دارد. \\n\\n\\t \\n\\n\\t\\n\\n\\tاعتبارسنجی اولیه:  در ابتدای استفاده از وبسایت، اعتبارسنجی اولیه برای اطمینان از اینکه کاربران مختلف، حتی افرادی با عملکرد ذهنی متفاوت، به راحتی از وبسایت استفاده کنند، ضرور است. این شامل اعتبارسنجی اولیه طراحی وبسایت در مورد دسترسی\\u200cپذیری آن برای افراد با نیازهای ویژه می\\u200cشود.\\n\\n\\t\\tسیستم\\u200cهای اندازه\\u200cگیری کارایی: برای ارزیابی عملکرد و کارایی وبسایت برای افراد با افت عملکرد توجه و حافظه کاری، سیستم\\u200cهای اندازه\\u200cگیری کارایی مورد استفاده قرار می\\u200cگیرند. این سیستم\\u200cها می\\u200cتوانند معیارهایی مانند زمان بارگذاری صفحه، تعداد خطاها، و تعامل کاربر را اندازه\\u200cگیری کنند.\\n\\n\\t\\n\\n\\t\\n\\n\\t4-12-4) روش تحلیل داده ها:\\n\\n\\tدر مرحله ابتدای تحلیل داده برای این پژوهش، ابتدا نقاط تمرکز (AOIs) بر روی وبسایت مشخص شدند. این نقاط شامل بخش\\u200cهایی بودند که احتمالاً بر کارکرد اجرایی افراد مبتلا به اختلال توجه تأثیر مثبت گذاشته\\u200cاند. با بهره\\u200cگیری از دستگاه ردیاب چشم(Eye tracker)، حرکات چشم افراد در حین تعامل با وبسایت ضبط شد. سپس با بهره\\u200cگیری از نرم\\u200cافزارهای تحلیل داده چشمی، الگوهای مختلف چشمی، از جمله نقاط تمرکز، زمان نگاه، و حرکات چشمی، مورد تجزیه و تحلیل قرار گرفتند. نتایج نشان دادند که طراحی بهینه وبسایت تأثیر مثبتی بر الگوهای چشمی و کارکرد اجرایی افراد با اختلال توجه دارد. به عنوان مثال، زمان بیشتری در نقاط تمرکز صرف شده و حرکات چشمی با آرامش و به صورت منظم\\u200cتر مشاهده شده\\u200cاند. این نتایج به طراحان رابط دیجیتال نکاتی ارائه می\\u200cدهند تا تجربه افراد با اختلال توجه را بهبود بخشند و کارایی آنان را افزایش دهند.\\n\\n\\t\\n\\n\\t\\n\\n\\t13-4) جنبه جدید بودن و نوآوری پژوهش در چیست؟ \\n\\n\\tدر این پژوهش به بررسی و تحلیل رابط\\u200cهای کاربری دیجیتال با تمرکز بر افراد دارای افت عملکرد توجه و حافظه کاری می\\u200cپردازد و سعی دارد تا راهکارهایی ارائه دهد که تجربه کاربری این گروه از افراد را بهبود بخشد. علاوه بر این، پژوهش اهمیت ویژگی\\u200cها و عناصری چون دکمه\\u200cها برای انجام عملیات خاص، فرم\\u200cها برای جمع\\u200cآوری اطلاعات، منوها برای جابجایی بین صفحات مختلف و تنظیمات تجربه کاربری را به عنوان اجزای مهمی در رابط\\u200cهای دیجیتال مورد تجزیه و تحلیل قرار می\\u200cدهد. این پژوهش به طراحان و توسعه\\u200cدهندگان وبسایت\\u200cها کمک می\\u200cکند تا راهکارهای بهینه\\u200cتری در طراحی رابط\\u200cهای کاربری ارائه دهند و از این طریق به بهبود تجربه کاربری افراد با افت عملکرد توجه و حافظه کاری بپردازند.\\n\\n\\t\\n\\n\\t\\n\\n\\n\\t\\n\\n\\t14-4) از اين پژوهش چه محصول کاربردی انتظار دارید؟\\n\\n\\t\\n\\n\\tما در این پژوهش به دنبال ساخت یک رابط کاربری بهینه، درون وبسایتی متناسب با افراد دارای افت عملکرد توجه و حافظه کاری هستیم؛ انتظار می رود که با این محصول از میزان حواس پرتی و میزان بارشناختی که افراد این طیف در هنگام ورود به  وبسایت های دیگر درگیر آن میشوند،شناسایی کرده و از میزان  کاسته شود.\\n\\n\\t\\n\\n\\t15-4) استفاده\\u200cکنند\\u200cگان از نتيجه پژوهش (اعم ازموسسات آموزشی\\u200c، پژوهشی\\u200c، دستگاه\\u200cهای اجرايی و غيره)\\n\\n\\tبه طور کلی تمامی موسسات شناختی و پژوهشکده های مغز و شناخت و موسساتی که افراد این طیف یعنی افراد دارای افت عملکرد توجه و حافظه کاری را تحت درمان های شناختی قرار میدهند در زمره استفاده کنندگان از نتیجه این پژوهش قرار دارند.\\n\\n\\t\\n\\n\\t16-4) ملاحظات اخلاقی پژوهش (چنانچه رعایت هر مورد از ملاحظات اخلاقی در انجام پژوهش و فرآیند جمع آوری داده ها ضروری است ذکر شود):\\n\\n\\t\\n\\nنام و نام خانوادگی دانشجو\\n\\nامضا و تاریخ\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t17-4) برنامه زمانبندی مراحل انجام پژوهش ( از زمان تصويب تا دفاع نهايی):\\n\\nروند فعاليت ها\\n\\nماه اول\\n\\nماه دوم\\n\\nماه سوم\\n\\nماه چهارم\\n\\nماه پنجم\\n\\nماه ششم\\n\\nماه هفتم\\n\\nماه هشتم\\n\\nماه نهم\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\tفهرست منابع فارسی و انگلیسی (به شیوه APA):\\n\\n\\t\\n\\n\\n\\nفروغی، ا. (1399). طراحی بصری وب\\u200cسایت: راهنمایی جامع برای طراحان حرفه\\u200cای و مبتدی. تهران: انتشارات نور المعارف.\\n\\nرجبی، م.، نقی\\u200cپور، ش.، و حاجی آبادی، س. (1399). بررسی تأثیر طراحی بهینه رابط کاربری وب سایت بر رضایت و وفاداری کاربران. فصلنامه توسعه فناوری اطلاعات و ارتباطات، 12(3)، 1-18.\\n\\nقدیمی، س.، خزائی، ر.، و حسن\\u200cزاده، س. (1399). بررسی تأثیر مشکلات کاربری مرتبط با اختلال کارکرد اجرایی در کاربران ایرانی صفحات وب. فصلنامه علمی-پژوهشی روانشناسی معاصر، 15(2)، 125-142.\\n\\nمازندرانی، سعید و حیدری، حسین (۱۳۹۹). بررسی تأثیر طراحی واسط کاربری بر تجربه کاربری و رضایت کاربراازوب\\u200cسایت\\u200cها، فصلنامه رسانه و توسعه، ۸(۱۹)، ۶۷-۸۴.\\n\\nاحمدی، م، و رضایی، ع.(1397). طراحی وب سایت دانشگاه با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد نابینا و کم\\u200cبینا با استفاده از روش تحلیل سلسله مراتبی فازی(مطالعه موردی: دانشگاه آزاد اسلامی واحد تبریز). فصلنامه علمی پژوهشی مطالعات مدیریت فناوری اطلاعات، 6(22)، 125-154.[11]\\n\\nبابایی، م، و محمدی، م.(1399). طراحی وب سایت با رویکرد دسترس\\u200cپذیری برای افراد دارای ناتوانی شنوایی. فصلنامه علمی پژوهشی رسانه های نوین، 11(1)، 1-24.[13]\\n\\nجعفرپور، م، و صادقی، ح.(1401). طراحی وب سایت با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای ناتوانی حرکتی. فصلنامه علمی پژوهشی فناوری آموزش، 12(2)، 117-128.[14]\\n\\nحسین\\u200cزاده، ع، و رضائیان، ع.(1395). طراحی وب سایت با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای ناتوانی بینایی. فصلنامه علمی پژوهشی مطالعات کاربردی کامپيوتر در علوم انساني، 3(1)، 1-16.[15]\\n\\nخسروانژاد، م. ر.(1398). طراحی وب سایت با رویکرد تجربه کاربری و دسترس\\u200cپذیری برای افراد دارای ناتوانی ذهنی. فصلنامه علمی پژوهشی رسانه های دیداری، 9(2)، 29-40.[16]\\n\\nرضائيان، ع.(1394). طراحي وب سايت با رويكرد تجربه كاربري و دسترس\\u200cپذيري براي افراد داراي ناتواني شنيدار. فصلنامه علمي پژوهشي مطالعات كاربرد كامپيوتر در علوم انساني، 2(2)، 1-18.[17]\\n\\n\\n\\n\\n\\nSmith, J. (2022). Title of article: What is the digital interface on the website and what is its effect on the user engagement index. Journal of Web Design, 5(2), 25-37. \\n\\nSmith, J. (2022). What is the digital interface on the website and what is its effect on the user engagement index. Journal of Web Design, 7(2), 45-56. Johnson, M. (2021). How users interact with the website. Journal of User Experience, 4(1), 12-23. \\n\\nLee, D. (2020). Digital interface\\'s impact on user engagement index. International Journal of Human-Computer Interaction, 36(5), 510-521.\\n\\nWilliams, S. (2019). Bounce rate of users on websites. Journal of Web Analytics, 6(2), 40-53. \\n\\nDavis, R. (2020). Visual interface and layout in websites. Journal of Web Design and Development, 8(1), 25-34.\\n\\nSmith, E. (2021). Clear website navigation. Journal of User Experience, 4(2), 45-55. \\n\\nJohnson, M. (2022). What is optimal digital interface design? International Journal of Human-Computer Studies, 152, 1-10.\\n\\nSmith, J. (2021). Consistent and Simple Design in Websites. Journal of Web Design and Development, 9(1), 12-21.\\n\\nGarcia, R. (2020). Easy accessibility on websites. Journal of Web Accessibility, 7(1), 18-30.\\n\\nJohnson, M. (2022). Optimal design in cognitive science. Journal of Cognitive Science, 15(2), 45-56.\\n\\nLee, M. (2021). Different factors in optimal design in cognitive science. Journal of Applied Cognitive Psychology, 35(4), 763-772.+-\\n\\nJohnson, S. (2020). Usability on websites. Journal of Usability Studies, 15(3), 123-135.\\n\\nDavis, E. (2022). Satisfaction on websites. Journal of Consumer Satisfaction, Dissatisfaction, and Complaining Behavior, 35(1), 23-32.\\n\\nSmith, J. (2021). Compatibility and distraction in websites. Journal of Human-Computer Interaction, 37(2), 256-267. \\n\\nLee, D. (2020). Different types of cognitive design models for digital interfaces. International Journal of Human-Computer Interaction, 36(4), 456-467.\\n\\nJohnson, S. (2021). User centered design model. International Journal of Design, 15(3), 123-135. \\n\\nSmith, M. (2022). What is a user-centered design model? Journal of Human-Computer Interaction, 38(1), 34-48.\\n\\nBrown, E. (2020). Final design with evaluation and feedback in the user-centered design model. Journal of Usability Studies, 15(2), 87-99.\\n\\nLee, J. (2021). The impact of optimum website digital interface design on user performance. International Journal of Human-Computer Interaction, 37(5), 356-370.\\n\\nJohnson, S. (2020). Website load time: Its effects on user satisfaction and engagement. Journal of Web Engineering, 19(1), 41-54.\\n\\nDavis, M. (2022). Exploring the factors that influence user website dwell time. Journal of Information Science, 48(3), 321-333.\\n\\nWilson, E. (2019). The effect of cognitive load reduction on user experience with simple website design. International Journal of Human-Computer Interaction, 35(8), 685-699. \\n\\nLee, S. (2021). Elements of coherent web design: A review of current research. Journal of Web Design, 12(3), 17-29.\\n\\nDoe, J. (2020). Problems with digital interface design for individuals with attention and working memory disorders. Journal of Cognitive Psychology, 32(4), 345-357. \\n\\nSmith, J. (2019). Distracting elements and website navigation: A user-centered design approach. International Journal of Human-Computer Interaction, 35(2), 126-138. \\n\\nDoe, J. (2020). Problems of people with attention and working memory disorders in working with websites. Journal of Neuropsychology, 14(3), 345-356.\\n\\nSmith, J. (2022). User-centered design for people with attention and working memory disorders in websites. Journal of Usability Studies, 17(1), 24-34.\\n\\nDoe, J. (2021). Accessibility features for people with attention and working memory disorders on websites. Journal of Accessibility, 5(2), 45-56. \\n\\nSmith, J. (2022). Which potential problems does your proposed design model address in people with disabilities? Journal of Inclusive Design, 8(1), 12-28.\\n\\nWang, X., & Dunlap, D. R. (2018). User-centered design to address the needs of individuals with cognitive disabilities: A literature review. Journal of Research in Special Educational Needs, 18(3), 154-165.\\n\\nJones, P., & Smith, L. (2020). User-centered design model and its role in reducing potential problems in websites for people with attention and working memory disorders. Journal of Accessibility and Design for All, 10(1), 45-56.\\n\\nSmith, A., Johnson, B., & Thompson, C. (2018). Visual design elements and user experience: An exploration of the impact on user perception. Journal of Usability Studies, 13(2), 58-73.\\n\\nJones, C., & Brown, D. (2020). Navigating digital interfaces: An examination of user interaction with website menus. Journal of Interactive Technology and Pedagogy, 10.\\n\\nGarcia, E., & Martinez, M. (2019). Responsive web design and user experience. International Journal of Human-Computer Interaction, 35(4-5), 311-319.\\n\\nWilliams, J. (2021). Enhancing user interaction: Designing effective interactive elements for digital interfaces. International Journal of Design Creativity and Innovation, 9(2), 87-102.\\n\\nDavis, F. D., Bagozzi, R. P., & Warshaw, P. R. (2017). User acceptance of computer technology: A comparison of two theoretical models. Management Science, 35(8), 982-1003.\\n\\nThompson, R. L., & White, K. M. (2019). Using and designing accessible technology: A survey of practices, challenges, and future plans of professionals in the field. Journal of Computer-Mediated Communication, 24(2), 68-84.\\n\\nPosner, M. I., & Petersen, S. E. (1990). The attention system of the human brain. Annual Review of Neuroscience, 13(1), 25-42.\\n\\nKarrasch, M., & Paulus, M. (2006). A fast, simple and efficient method for the estimation of low-level visual evoked potentials. Clinical Neurophysiology, 117(9), 1995-2003.\\n\\nBaddeley, A. D., & Hitch, G. (1974). Working memory. Psychology of Learning and Motivation, 8, 47-89.\\n\\nCowan, N. (2005). Working memory capacity. Psychology Press.\\n\\nCooper, A., Reimann, R., & Cronin, D. (2017). About Face 3: The Essentials of Interaction Design. Wiley.\\n\\nLazar, J., Feng, J. H., & Hochheiser, H. (2017). Research Methods in Human-Computer Interaction. Morgan Kaufmann.\\n\\nAbascal, J., & Nicolle, C. (2015). Moving towards inclusive design guidelines for socially and ethically aware HCI. Interacting with Computers, 17(5), 484-505.\\n\\nBühler, C., & Stephanidis, C. (2014). Inclusive design of ambient intelligence environments: The interplay of user, context and technology. In International Conference on Computers for Handicapped Persons (pp. 3-10). Springer, Berlin, Heidelberg. \\n\\nClark, J., & Clark, L. (2016). Designing for the web: A survey of web design methodologies and guidelines including accessibility and usability issues. In Proceedings of the 2016 ACM Conference on Innovation and Technology in Computer Science Education (pp. 348-348). \\n\\nDíaz-Bossini, J., & Moreno, L. (2014). Accessibility to mobile interfaces for older people. Procedia Computer Science, 27, 57-66. \\n\\nHanson, V. L., & Richards, J. T. (2013). Progress on website accessibility?. ACM Transactions on the Web (TWEB), 7(1), 1-30. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t18-4) هزینه\\u200cهای انجام پژوهش\\n\\n\\t\\n\\nرديف\\n\\nنوع هزینه\\n\\nهزينه (ريال)\\n\\n\\n\\nنیروی انسانی\\n\\nندارد\\n\\n\\n\\nآزمايشات و خدمات تخصصي\\n\\nآزمون N back  و IVA\\n\\n\\n\\nمواد مصرفی/ غیرمصرفی\\n\\nندارد\\n\\n\\n\\nمسافرت \\n\\nندارد\\n\\n\\n\\nمتفرقه\\n\\n140 هزار تومان (هر نفر)\\n\\nجمع کل هزینه\\u200cها\\n\\n70000000 ریال\\n\\n\\t\\n\\n\\t\\n\\nنام و نام خانوادگی دانشجو\\n\\nامضا و تاریخ\\n\\n01/07/1403\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n5 ـ نظر اساتید پژوهش\\n\\n\\n\\n\\n\\nنام و نام\\u200cخانوادگی\\n\\nتأیید طرح پایان نامه\\n\\n(امضا و تاریخ)\\n\\n1\\n\\nاستاد راهنما (اول)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\nاستاد راهنما (دوم)\\n\\n\\n\\n\\n\\n\\n\\n3\\n\\n\\n\\nاستاد مشاور (اول)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\n\\nاستاد مشاور (دوم)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n6- نظر شورای آموزشی مؤسسه آموزش عالی علوم شناختی\\n\\n\\n\\nموضوع در جلسه مورخ .................................. مطرح و مورد تأیید  قرار گرفت/   قرار نگرفت.\\n\\n\\t\\n\\n\\t\\n\\n\\n\\nمعاون آموزشی- پژوهشی مؤسسه\\n\\nامضا و تاریخ\\n\\n\\n\\n\\n\\n\\n\\n19'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRFz07JfQXkK",
        "outputId": "1e1dbb9a-49f3-4abe-8f78-290f2779263d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35969"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in text_chunks:\n",
        "    print(len(chunk.page_content))\n",
        "    if len(chunk.page_content) > 600:\n",
        "        print('length is over than 600', chunk.page_content)\n",
        "        #break\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYG-A8wHQvgu",
        "outputId": "addb2223-5a60-4f13-c2f2-cc1dbf0f418c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "563\n",
            "495\n",
            "500\n",
            "309\n",
            "391\n",
            "597\n",
            "597\n",
            "599\n",
            "594\n",
            "598\n",
            "598\n",
            "597\n",
            "599\n",
            "599\n",
            "597\n",
            "404\n",
            "52\n",
            "595\n",
            "325\n",
            "15\n",
            "598\n",
            "228\n",
            "454\n",
            "401\n",
            "420\n",
            "436\n",
            "406\n",
            "490\n",
            "520\n",
            "566\n",
            "558\n",
            "585\n",
            "571\n",
            "430\n",
            "596\n",
            "308\n",
            "11\n",
            "593\n",
            "256\n",
            "385\n",
            "464\n",
            "444\n",
            "413\n",
            "567\n",
            "500\n",
            "366\n",
            "598\n",
            "280\n",
            "546\n",
            "557\n",
            "552\n",
            "533\n",
            "377\n",
            "588\n",
            "595\n",
            "266\n",
            "44\n",
            "594\n",
            "119\n",
            "448\n",
            "589\n",
            "516\n",
            "331\n",
            "597\n",
            "537\n",
            "561\n",
            "529\n",
            "555\n",
            "478\n",
            "560\n",
            "457\n",
            "467\n",
            "509\n",
            "583\n",
            "509\n",
            "526\n",
            "545\n",
            "516\n",
            "566\n",
            "590\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuEncj22SSDl",
        "outputId": "6f93e420-a0cc-48b2-bcf9-11a7104b5296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '/content/Proposal_erfan.docx'}, page_content='مؤسسه آموزش عالی علوم شناختی\\n\\n\\n\\nطرح پایان نامه کارشناسی ارشد\\n\\n\\n\\n1ـ  اطلاعات مربوط به دانشجو:\\n\\n\\n\\nنام:      عرفان                              نام خانوادگی:    اسلامیه                               شماره دانشجويی: 40006102\\n\\nرشته وگرایش تحصیلی:  طراحی و خلاقیت شناختی                                       سال ورود:  1400\\n\\nنشانی و تلفن: \\n\\n\\n\\nرایانامه (ایمیل):  erfan.cognitive.work@gmail.com\\n\\n\\t\\n\\n\\t\\n\\n\\t\\n\\n2ـ استادان راهنما:\\n\\n\\n\\nرديف\\n\\nنام و نام خانوادگی\\n\\nتخصص\\n\\nآخرين مدرک تحصيلی\\n\\nرتبه دانشگاهی\\n\\nسنوات تدریس در  دوره کارشناسی\\u200cارشد\\n\\nمحل خدمت فعلی، \\n\\nتلفن\\n\\nرایانامه\\n\\n (ایمیل)')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"HooshvareLab/bert-base-parsbert-uncased\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "hf_embedding = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWSSLhi7QzKB",
        "outputId": "8cb4c910-5e8b-4212-8735-73bae83cfc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name HooshvareLab/bert-base-parsbert-uncased. Creating a new one with mean pooling.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# ساخت VectorStore از text_chunks\n",
        "vectorstore = FAISS.from_documents(text_chunks, hf_embedding)\n",
        "\n",
        "print(\"VectorStore was created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWnqOuWDR3wx",
        "outputId": "8587acf6-69fe-4620-8ee0-7090b311d012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorStore was created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"کاربرد حافظه کاری در علوم شناختی چیست؟\"\n",
        "results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "for i, res in enumerate(results, 1):\n",
        "    print(f\"نتیجه {i}:\")\n",
        "    print(res.page_content)\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UShaqxbDSIPq",
        "outputId": "578121ab-1433-4328-dd9a-725bcff07c4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نتیجه 1:\n",
            "حافظه کاری\n",
            "\n",
            "\t\tدر دستور راهنمای تشخیصی و آماری اختلالات روانی، ویرایش پنجم DSM-5 مفهوم حافظه کاری به عنوان یکی از عوامل مهم در فرآیندهای شناختی مورد بررسی قرار گرفته است. حافظه کاری به طور کلی به توانایی مغز برای ذخیره و مدیریت اطلاعات در مدت زمان کوتاه‌تر اشاره دارد. این نوع حافظه نقش اساسی در وظایف روزمره مانند تمرکز، برنامه‌ریزی، انجام وظایف چندگانه و کنترل عملکرد‌های اجرایی ایفا می‌کند (سادوک، et al., 2013).\n",
            "\n",
            "\t\t\n",
            "\n",
            "تعریف مفهومی حافظه کاری:\n",
            "--------------------------------------------------\n",
            "نتیجه 2:\n",
            "تعریف عملیاتی حافظه کاری:\n",
            "\n",
            "\t\tیکی از آزمون‌های شناختی برای ارزیابی حافظه کاری و توانایی تمرکز شناختی افراد ، آزمون تعداد نگاشت (N-Back Task) است. این آزمون به ویژه در تحقیقات روان‌شناسی و علوم اعصاب برای مطالعه عملکرد حافظه کاری و نحوه پردازش اطلاعات در مغز استفاده می‌شود. در این آزمون افراد باید به ترتیب زمانی نسبت به اطلاعات قبلی ورودی، اطلاعات جدید را تشخیص داده و ارزیابی کنند . آزمون به بررسی توانایی نگهداری و بازخوانی اطلاعات در مدت زمان کوتاه می‌پردازد و میزان توجه و تمرکز را نیز ارزیابی می‌کند(سادوک، et al., 2013) .\n",
            "\n",
            "\t12-4) روش‌ پژوهش\n",
            "\n",
            "\t1-12-4) طرح پژوهش:\n",
            "--------------------------------------------------\n",
            "نتیجه 3:\n",
            "تعریف مفهومی حافظه کاری:\n",
            "\n",
            "\t\tحافظه کاری (حافظه فعال) در اختلالات روانی مطابق با DSM-5 به توانایی مغز اطلاق می‌شود که اطلاعات را به مدت زمان محدودی نگهداری و در طول انجام وظایف شناختی، مانند تمرکز، برنامه‌ریزی و کنترل عملکرد‌های اجرایی، به کار گیرد. این نوع حافظه جزء از سامانه‌های فرآیندهای شناختی مغز است که در حفظ و مدیریت اطلاعات در مدت زمان کوتاه کمک می‌کند(سادوک، et al., 2013).\n",
            "\n",
            "\t\t\n",
            "\n",
            "تعریف عملیاتی حافظه کاری:\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare with other model like Openai**"
      ],
      "metadata": {
        "id": "mNeAcF04Tw0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install openai for langchain**"
      ],
      "metadata": {
        "id": "oijOWV95UKwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trEwdbCWSh82",
        "outputId": "36011bee-7593-4c03-8f5e-868184dc2dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.28-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.71)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.97.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.86.0->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.86.0->langchain-openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.68->langchain-openai) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.28-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-openai\n",
            "Successfully installed langchain-openai-0.3.28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzjwoseiwZIf",
        "outputId": "d811426b-312c-4bc2-997f-26725830ea70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.71)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.8)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.11.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**call OpenAIEmbeddings**"
      ],
      "metadata": {
        "id": "9RMjva_tUViL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n"
      ],
      "metadata": {
        "id": "woZJWjSSUKDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**import API key**"
      ],
      "metadata": {
        "id": "UKCWJ_GDUu-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Enter here your API Key\""
      ],
      "metadata": {
        "id": "WdA5U2tNUqpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_embedding = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
      ],
      "metadata": {
        "id": "pqdjd0SqU4DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a Vectorstore for Openai**"
      ],
      "metadata": {
        "id": "NCO-0reaVo6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_openai = FAISS.from_documents(text_chunks, openai_embedding)\n",
        "print(\"VectorStore was created with OpenAI.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnbK7FHkVv64",
        "outputId": "f7b95c6e-486e-4725-d6f0-79b6c0d80169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VectorStore was created with OpenAI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"کاربردهای حافظه کاری در علوم شناختی چیست؟\"\n",
        "results_openai = vectorstore_openai.similarity_search(query, k=3)\n",
        "\n",
        "print(\"\\n نتایج مدل OpenAI:\\n\")\n",
        "for i, res in enumerate(results_openai, 1):\n",
        "    print(f\"نتیجه {i}:\")\n",
        "    print(res.page_content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "EpE_PcnJVxe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb0de9f-5a77-4437-919a-e977489bdd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " نتایج مدل OpenAI:\n",
            "\n",
            "نتیجه 1:\n",
            "حافظه کاری\n",
            "\n",
            "\t\tدر دستور راهنمای تشخیصی و آماری اختلالات روانی، ویرایش پنجم DSM-5 مفهوم حافظه کاری به عنوان یکی از عوامل مهم در فرآیندهای شناختی مورد بررسی قرار گرفته است. حافظه کاری به طور کلی به توانایی مغز برای ذخیره و مدیریت اطلاعات در مدت زمان کوتاه‌تر اشاره دارد. این نوع حافظه نقش اساسی در وظایف روزمره مانند تمرکز، برنامه‌ریزی، انجام وظایف چندگانه و کنترل عملکرد‌های اجرایی ایفا می‌کند (سادوک، et al., 2013).\n",
            "\n",
            "\t\t\n",
            "\n",
            "تعریف مفهومی حافظه کاری:\n",
            "--------------------------------------------------\n",
            "نتیجه 2:\n",
            "2-12-4) تشریح کامل روش تحقیق:  \n",
            "\n",
            "روش تحقيق آزمايشی\n",
            "\n",
            "\t\n",
            "در این روش پژوهش، هدف بررسی اثر یک رابط کاربری دیجیتال وبسایت بر عملکرد توجه و حافظه کاری دانشجویان موسسه آموزش عالی شناختی در سال تحصیلی 1402 در بازه سنی درنظر گرفته شده بین 20 تا 27 سال است که دارای اختلال افت عملکرد توجه و حافظه کاری خواهند بود. نوع آزمایش در این پژوهش به صورت پیش آزمون  -پس آزمون خواهد بود.\n",
            "--------------------------------------------------\n",
            "نتیجه 3:\n",
            "در طراحی رابط‌های کاربری ارائه دهند و از این طریق به بهبود تجربه کاربری افراد با افت عملکرد توجه و حافظه کاری بپردازند.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3. : Using metadata filtering in vector search**"
      ],
      "metadata": {
        "id": "3iM1uyD9CsNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**install some package**"
      ],
      "metadata": {
        "id": "XDsw8iPhbgP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viCRQO27J8g_",
        "outputId": "6b4db468-c884-429f-b02e-93a93b612fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unstructured\n",
            "  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.4.2)\n",
            "Collecting filetype (from unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting python-magic (from unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.13.4)\n",
            "Collecting emoji (from unstructured)\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured) (0.6.7)\n",
            "Collecting python-iso639 (from unstructured)\n",
            "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting langdetect (from unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured) (2.0.2)\n",
            "Collecting rapidfuzz (from unstructured)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting backoff (from unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.14.1)\n",
            "Collecting unstructured-client (from unstructured)\n",
            "  Downloading unstructured_client-0.41.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured) (5.9.5)\n",
            "Collecting python-oxmsg (from unstructured)\n",
            "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured) (2024.11.6)\n",
            "Collecting olefile (from python-oxmsg->unstructured)\n",
            "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured) (2025.7.14)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (24.1.0)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (2.11.7)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->unstructured)\n",
            "  Downloading pypdf-5.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
            "Downloading unstructured-0.18.11-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.6/167.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_client-0.41.0-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.8.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=37a7aa9a1aade82c62389fe23103cbcdbde995a6a2c71cd74139f388274313c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, olefile, langdetect, emoji, backoff, python-oxmsg, unstructured-client, unstructured\n",
            "Successfully installed backoff-2.2.1 emoji-2.14.1 filetype-1.2.0 langdetect-1.0.9 olefile-0.47 pypdf-5.8.0 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 unstructured-0.18.11 unstructured-client-0.41.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"unstructured[pdf]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NVUUGQ-1KTcb",
        "outputId": "d2fc0e5f-148d-4235-976c-6367d6e4b2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unstructured[pdf] in /usr/local/lib/python3.11/dist-packages (0.18.11)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.4.2)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.13.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.14.1)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2025.2.18)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.0.2)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (3.13.0)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.14.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.41.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.17.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (0.0.2)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (1.1)\n",
            "Collecting onnx>=1.17.0 (from unstructured[pdf])\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting onnxruntime>=1.19.0 (from unstructured[pdf])\n",
            "  Downloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting pdf2image (from unstructured[pdf])\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pdfminer.six (from unstructured[pdf])\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting pikepdf (from unstructured[pdf])\n",
            "  Downloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pi-heif (from unstructured[pdf])\n",
            "  Downloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (from unstructured[pdf]) (5.8.0)\n",
            "Collecting google-cloud-vision (from unstructured[pdf])\n",
            "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting effdet (from unstructured[pdf])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting unstructured-inference>=1.0.5 (from unstructured[pdf])\n",
            "  Downloading unstructured_inference-1.0.5-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf])\n",
            "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.17.0->unstructured[pdf]) (5.29.5)\n",
            "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[pdf])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19.0->unstructured[pdf]) (1.13.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (0.0.20)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (0.33.4)\n",
            "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (2.6.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.0.18)\n",
            "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.9.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from unstructured-inference>=1.0.5->unstructured[pdf]) (1.16.0)\n",
            "Collecting pypdfium2 (from unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured.pytesseract>=0.3.12->unstructured[pdf]) (11.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->unstructured[pdf]) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->unstructured[pdf]) (0.9.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (0.21.0+cu124)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.0.10)\n",
            "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.11/dist-packages (from effdet->unstructured[pdf]) (2.3.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-vision->unstructured[pdf]) (1.26.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib->unstructured[pdf]) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->unstructured[pdf]) (2024.11.6)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six->unstructured[pdf]) (43.0.3)\n",
            "Collecting Deprecated (from pikepdf->unstructured[pdf])\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.11/dist-packages (from python-oxmsg->unstructured[pdf]) (0.47)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->unstructured[pdf]) (2025.7.14)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (24.1.0)\n",
            "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.9)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.6.0)\n",
            "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (2.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from unstructured-client->unstructured[pdf]) (1.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.70.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.73.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[pdf]) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (4.9.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[pdf]) (0.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[pdf]) (4.9.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from omegaconf>=2.0->effdet->unstructured[pdf]) (6.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[pdf]) (0.4.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->unstructured-inference>=1.0.5->unstructured[pdf]) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->unstructured-inference>=1.0.5->unstructured[pdf])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[pdf]) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.25.1->unstructured-inference>=1.0.5->unstructured[pdf]) (0.21.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->unstructured-inference>=1.0.5->unstructured[pdf]) (1.1.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[pdf]) (1.1.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[pdf])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->unstructured-inference>=1.0.5->unstructured[pdf]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=1.0.5->unstructured[pdf]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->unstructured-inference>=1.0.5->unstructured[pdf]) (2025.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[pdf]) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured[pdf]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->unstructured-inference>=1.0.5->unstructured[pdf]) (3.0.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured_inference-1.0.5-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
            "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pi_heif-1.0.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pikepdf-9.10.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unstructured.pytesseract, pypdfium2, pi-heif, pdf2image, onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, humanfriendly, Deprecated, pikepdf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, pdfminer.six, onnxruntime, nvidia-cusolver-cu12, google-cloud-vision, unstructured-inference, effdet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed Deprecated-1.2.18 coloredlogs-15.0.1 effdet-0.4.1 google-cloud-vision-3.10.2 humanfriendly-10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 onnxruntime-1.22.1 pdf2image-1.17.0 pdfminer.six-20250506 pi-heif-1.0.0 pikepdf-9.10.2 pypdfium2-4.30.0 unstructured-inference-1.0.5 unstructured.pytesseract-0.3.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "nvidia"
                ]
              },
              "id": "7deafa1133244dbab2366d71f65acca1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf\n",
        "\n",
        "import fitz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx0SiNOupb1j",
        "outputId": "d45c45c8-b525-48e0-c51f-3a3de40dff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SWEMOcR8ec1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We want to use 3 PDF file and extract the content of each file and transform them into little chunk (approx. 500 characters) and then transform them into Document separately**"
      ],
      "metadata": {
        "id": "zuipgXOFed6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TJX2zk7df6lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3.1 : Loading Data**"
      ],
      "metadata": {
        "id": "_1ygi6QfUfJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF 1**"
      ],
      "metadata": {
        "id": "AANKTyTgf7nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "loader = UnstructuredFileLoader(\"/content/Cognitive Neuroscience.pdf\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVZazZsH6bIU",
        "outputId": "9bd3b80b-15b7-4891-bf66-184b7a9e064d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-2367858620.py:3: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
            "  loader = UnstructuredFileLoader(\"/content/Cognitive Neuroscience.pdf\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "Z8Ms8F9cJ5YD",
        "outputId": "f1fb8c2e-0f47-4954-d46b-828279d3a157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nThe Cognitive Neuroscience of Insight\\n\\nJohn Kounios1 and Mark Beeman2\\n\\n1Department of Psychology, Drexel University, Philadelphia, Pennsylvania 19102; email: john.kounios@gmail.com 2Department of Psychology, Northwestern University, Evanston, Illinois 60208; email: mjungbee@northwestern.edu\\n\\nAnnu. Rev. Psychol. 2014. 65:71–93\\n\\nKeywords\\n\\nThe Annual Review of Psychology is online at http://psych.annualreviews.org\\n\\nattention, cognitive enhancement, creativity, hemispheric asymmetry, problem solving\\n\\nThis article’s doi: 10.1146/annurev-psych-010213-115154\\n\\nAbstract\\n\\nCopyright c(cid:2) 2014 by Annual Reviews. All rights reserved\\n\\nInsight occurs when a person suddenly reinterprets a stimulus, situation, or event to produce a nonobvious, nondominant interpretation. This can take the form of a solution to a problem (an “aha moment”), comprehension of a joke or metaphor, or recognition of an ambiguous percept. Insight research began a century ago, but neuroimaging and electrophysiological techniques have been applied to its study only during the past decade. Recent work has revealed insight-related coarse semantic coding in the right hemisphere and internally focused attention preceding and during problem solving. In- dividual differences in the tendency to solve problems insightfully rather than in a deliberate, analytic fashion are associated with different patterns of resting-state brain activity. Recent studies have begun to apply direct brain stimulation to facilitate insight. In sum, the cognitive neuroscience of in- sight is an exciting new area of research with connections to fundamental neurocognitive processes.\\n\\n71\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nContents\\n\\nINTRODUCTION............................................................... WHAT IS INSIGHT? ............................................................ SCOPE OF THE REVIEW....................................................... COGNITIVE PSYCHOLOGY OF INSIGHT .................................... NEURAL BASIS OF INSIGHT................................................... Hemispheric Asymmetry........................................................ Neural Correlates of Insight Solving............................................. Preparation for Insight.......................................................... Resting-State Brain Activity and Individual Differences........................... ATTENDING IN, OUT, AND AROUND........................................ FACTORS THAT INFLUENCE THE LIKELIHOOD OF INSIGHT............ Mood .......................................................................... Mood and Attention ............................................................ Models of Attention ............................................................ Cognitive Control .............................................................. STIMULATING INSIGHT ...................................................... FUTURE DIRECTIONS.........................................................\\n\\n72 73 74 75 77 77 78 80 81 82 83 83 84 85 85 86 87\\n\\nINTRODUCTION\\n\\nIn an article in the Annual Review of Astronomy and Astrophysics, William Wilson Morgan (1988) summarized several of the groundbreaking scientiﬁc contributions he made over his long career. One of these was the discovery of the structure of the Milky Way galaxy. What isn’t obvious from his article is how he came to make this discovery (Sheehan 2008).\\n\\nIn1951,MorganhadbeencalculatingthedistancesofOBassociations,whicharegroupsofhot, bright stars. OB associations are considered “star nurseries” because these stars are young. One evening, he ﬁnished his work for the night and started to walk home from the Yerkes Observatory. He glanced up at the sky to observe the stars that he had been studying and had what he called a “ﬂashinspiration...acreativeintuitionalburst”:Thesestarsareorganizedinathree-dimensional, strand-like structure. Galaxies come in a variety of forms, but he knew that in spiral galaxies OB associationsresideinthegalacticarms.Morganunderstoodthatthestrand-likeformwasagalactic arm and that he had directly apprehended the spiral structure of the Milky Way, a realization that he substantiated with data that he presented at a conference a few months later.\\n\\nMorgan’s breakthrough realization was an insight, colloquially known as an “aha moment”—a sudden, conscious change in a person’s representation of a stimulus, situation, event, or problem (Kaplan & Simon 1990). Awareness of this kind of representational change, though abrupt, takes place after a period of unconscious processing (van Steenburgh et al. 2012). Because insights are largely a product of unconscious processing, when they emerge, they seem to be disconnected from the ongoing stream of conscious thought. In contrast, analytic thought is deliberate and conscious and is characterized by incremental awareness of a solution (Smith & Kounios 1996).\\n\\nAlthough Morgan’s insight was literally on a cosmic scale, the phenomenon of insight, in a more modest guise, is a common experience that occurs in perception, language comprehension, problem solving, and other domains of cognition (van Steenburgh et al. 2012). It is therefore of interest to ask what happened in Morgan’s brain and in the brains of many other people when they have had an insight. This article reviews relevant cognitive neuroscience research and an emerging\\n\\n72\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\ntheoretical framework that is progressing toward an answer to this question. Before describing this work, we circumscribe the insight phenomenon to specify the domain of this review.\\n\\nWHAT IS INSIGHT?\\n\\nInsight is often deﬁned as a sudden change in or the formation of a concept or other type of knowledge representation, often leading to the solution of a problem. These changes are thought to have certain attributes. For example, insights are frequently accompanied by a burst of emotion, including a highly positive surprise at either the content or manner of the realization. In contrast, analytic solutions are not typically accompanied by an emotional response except perhaps for a sense of satisfaction resulting from completing the task. However, though not an unusual con- comitant,aconsciousemotionalresponseisnotanecessaryfeatureofinsight.Participantsinmany studies have solved dozens of verbal puzzles with insight (e.g., Jung-Beeman et al. 2004, Smith & Kounios 1996) without reports of multiple bursts of emotion.\\n\\nAnother feature is that insights often break an impasse or mental block produced because a solver initially ﬁxated on an incorrect solution strategy or strong but ultimately unhelpful as- sociations of a problem. The breaking of an impasse is accompanied by the reinterpretation or restructuring of a problem to reveal a new, often simple, solution or solution strategy. Some re- searchers implicitly consider problem restructuring and the breaking of an impasse to be deﬁning features of insight (e.g., Cranford & Moss 2012). However, this view excludes prominent types of insights, such as those that occur (a) when the solution suddenly intrudes on a person’s awareness when he or she is not focusing on any solution strategy, (b) when an insight pointing to a solution occurs while a person is actively engaged in analytic processing but has not yet reached an impasse, and (c) when a person has a spontaneous realization that does not relate to any explicitly posed problem.Wethereforedonot consider thebreaking ofan impassetobe apreconditionfor insight. Thus, there are a number of potential deﬁnitions of insight, depending on which combination of features one selects. Very narrowly deﬁned, insight could be thought of as a sudden solution to a problem preceded by an impasse and problem restructuring and followed by a positive emotional response.Incontrast,thebroadestdeﬁnitionofinsightisthecommonnonscientiﬁconeinwhichan insight is any deep realization, whether sudden or not. Within cognitive psychology and cognitive neuroscience, inconsistency exists concerning what we consider to be a basic criterion for insight, namely, suddenness. For example, a number of purported insight studies do not speciﬁcally isolate and focus on solutions that occurred suddenly (e.g., Luo & Niki 2003, Wagner et al. 2004).\\n\\nAnother broad use of the term insight can be found in clinical psychology, in which insight refersto self-awareness,often ofone’s own symptoms,functional deﬁcits, or other kind ofpredica- ment. The clinical and nonscientiﬁc uses of the term do not require suddenness of realization or any accompanying emotional response. Indeed, in clinical psychology, the lack of an emotional response could itself be considered a symptom signifying a lack of insight.\\n\\nTheissueofdeﬁninginsightisnotanexerciseinpedantry.Wheninsightisdeﬁnedtoobroadly, it includes so many diverse, loosely related phenomena that it becomes virtually impossible for researchers to draw general conclusions. For example, one recent review of cognitive neuroscience researchoncreativityandinsightlumpstogetherwidelydiversestudiescharacterizedbyavarietyof deﬁnitions, assumptions, experimental paradigms, empirical phenomena, analytical methods, and stagesofthesolvingprocess(andinconsistentexperimentalrigor).Unsurprisingly,becauseofsuch indiscriminate agglomeration, that review failed to ﬁnd much consistency across studies, leading the authors to pronounce a negative verdict on the ﬁeld (Dietrich & Kanso 2010). In contrast, going to the other extreme by adopting an overly narrow deﬁnition of insight can lead one to miss important large-scale generalizations that cut across particular experimental paradigms.\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n73\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nThus,progressinstudyinginsightcanbefacilitatedorenhancedby“carvingnatureatitsjoints” and adopting a middle-path deﬁnition of insight to guide the selection of empirical phenomena and the development of experimental paradigms for its study. Speciﬁcally, we deﬁne insight as any sudden comprehension, realization, or problem solution that involves a reorganization of the elements of a person’s mental representation of a stimulus, situation, or event to yield a nonobvious or nondominant interpretation. Insights are not conﬁned to any particular domain of understanding, but we do not include all sudden realizations within this deﬁnition. For example, the reading of an isolated word starts with unconscious processing followed by a sudden conscious realizationoftheword’smeaning.Butthisisnotaninsight,becauseitdoesn’tinvolvereorganizing a mental representation to arrive at a nonobvious or nondominant interpretation. Insights may be especially salient when they follow an impasse, but impasse is not a necessary precondition for insight; otherwise, spontaneous sudden realizations would be excluded because they are not associated with an explicit problem whose solution is blocked by another idea. Insights are often accompanied by surprise and a positive burst of conscious emotion, but we do not consider these to be deﬁning features because individual insights in a sequence of insights, as occur in many experimental studies, don’t all elicit such conscious affective responses. (Of course, this doesn’t exclude the possibility that all insights may be accompanied by unconscious affective responses; cf. Topolinski & Reber 2010.) Phenomena such as impasse and emotion play important roles in problem solving and are worthy of study. However, isolating the core processes of insight is a prerequisite for investigating it. To accomplish this, we adopt a “Goldilocks” approach— neither too much nor too little—and argue that this strategy can, and has, enabled progress in understanding insight’s neurocognitive substrates.\\n\\nIt is also critical to recognize that insight involves several component processes working to- gether and unfolding over time. Experimental paradigms that emphasize one process over another will reveal different parts of this network. Such results may appear complex but actually paint a richer picture of insight, just as studying encoding and retrieval, or implicit and explicit learning, paints a more complete picture of how the brain supports memory.\\n\\nSCOPE OF THE REVIEW\\n\\nThis review discusses the current state of cognitive neuroscience research on insight. Though we also discuss selected behavioral cognitive studies that inform the neuroscientiﬁc framework we describe, we do not provide an overall review of the relevant cognitive literature here. Re- cent reviews of the cognitive literature are available elsewhere (e.g., van Steenburgh et al. 2012). Moreover, our discussion of neuroscientiﬁc studies is not exhaustive. We focus on those that meet several methodological criteria.\\n\\nThe ﬁrst desideratum is that a study must demonstrably isolate the insight phenomenon. Some studies present problems to participants and simply assume that the solutions are the result of insight rather than analytical thought. However, as described below, many types of problems can be solved by either insight or analysis (Bowden et al. 2005). Therefore, with some exceptions, we do not discuss studies that do not demonstrate that participants’ solutions were, in fact, a product of insight. One exception to this criterion is studies that use classic insight problems, such as the Nine-Dot Problem, that have been used by researchers for many decades and for which a consensus has been tacitly reached—though perhaps not yet with sufﬁcient justiﬁcation—that solutions to these problems are usually achieved by insight (e.g., Chi & Snyder 2012).\\n\\nA number of studies examine brain activity when people recognize rather than generate so- lutions (e.g., Ludmer et al. 2011, Luo et al. 2011, Metuki et al. 2012). People may feel a sense of insight upon recognizing solutions, but these postsolution recognition processes differ from\\n\\n74\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nthe processes responsible for generating the solutions. Once people see a solution word, they can perform a directed semantic memory search to connect the solution to the problem rather than an open-ended search for associations that might lead to the solution. Although solution recognition is itself interesting, it differs from pure insight.\\n\\nA second criterion is that a candidate study must use an appropriate control or comparison condition. For example, in studies that use remote associates problems or anagrams, insight so- lutions can be directly compared to analytic solutions for the same type of problem because this comparison controls for all factors except for the cognitive solving strategy—insight versus ana- lytic processing—that is the factor of primary interest. We therefore do not focus on studies that directly compare neural activity for sets of problems that differ in complexity, solving duration, visual content, working-memory load, and so forth (e.g., Aziz-Zadeh et al. 2013, Sheth et al. 2008) because differences in cognitive strategy are confounded with these ancillary factors. We wish to highlight how insight solving differs from analytic solving when other factors are held relatively constant.\\n\\nOther studies are not discussed here due to methodological issues that cannot be addressed on a study-by-study basis in an article of this scope, such as problematic baselining of neural activity (e.g., Sandk¨uhler & Bhattacharya 2008). Another type of methodological issue involves the integration over time of functional magnetic resonance imaging (fMRI) signal. One study attempted to use both subjective (self-report) and objective measures to distinguish insight from analytic solving (Aziz-Zadeh et al. 2009). Unfortunately, the objective measure was speed of solution: It was assumed that fast solutions were achieved with insight and slow solutions were achieved analytically. Not only is this assumption questionable, it also completely confounds the experimental contrast with the duration of solving effort. Because fMRI signal is integrated over time—the longer an area is active, the more the measured signal will increase—it is very sensitive tosuchconfounds.Thus,itisimpossibletoknowwhicheffectswererealandwhichwereconfound related.\\n\\nIt is important to note that the studies that are not discussed here due to methodological issues are not entirely uninformative. However, careful consideration must be given to each of these issues in the context of interpreting the results.\\n\\nCOGNITIVE PSYCHOLOGY OF INSIGHT\\n\\nMuch of the cognitive psychology research on insight done over the past three decades aimed to clarify the relationship between insightful and analytic thought (Sternberg & Davidson 1995). Early gestalt studies distinguished insight and analysis almost solely on the basis of the informal conscious experience of a problem solution emerging suddenly versus gradually. To extend this research, cognitive psychologists attempted to uncover more formal evidence to distinguish these two types of processing. A prominent example is a pioneering series of studies done by Janet Metcalfe during the 1980s. For example, Metcalfe & Wiebe (1987) focused on metacognitive characteristicsofinsightsuchasparticipants’feelingsof“warmth”(i.e.,closenesstosolution)while working on insight and analytic problems. Participants reported a gradual increase in feelings of warmth leading up to analytic solutions, but little or no warmth preceding insights until shortly before they solved the problem. Moreover, insight problems that were accompanied by feelings of warmth usually elicited incorrect solutions.\\n\\nMetcalfe & Wiebe’s (1987) study was groundbreaking in showing a behavioral difference be- tween insight and analytic solving beyond factors that differentially affected solution rates for these two types of problems. However, because Metcalfe’s study sampled participants’ feelings only once every 15 seconds, it did not directly address one of the central characteristics thought\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n75\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nto distinguish insight and analytic solving, namely, the suddenness of solution. Rather, her pro- cedure was designed to examine changes over time in participants’ feelings about their closeness to solution.\\n\\nHowever, it is possible to measure the accrual of solution information with higher temporal resolution using the speed-accuracy decomposition procedure (Kounios et al. 1987, Meyer et al. 1988). This technique revealed no discernable partial response information preceding the solution when people solve insight-like anagrams (Smith & Kounios 1996). Thus, insight solving occurs in a discrete transition from a state of no conscious information about the solution to the ﬁnal complete solution, with no intermediate states. In contrast, for similar speed-accuracy decompo- sition studies of other (noninsight) tasks, such as lexical decision, semantic veriﬁcation, short-term recognition memory, and long-term recognition memory, people show evidence of substantial partial information (Smith & Kounios 1996). This ﬁnding objectively validated the conscious experience of the abruptness of insight.\\n\\nThe conscious experience of insight directly relates to unconscious processing that precedes it. When people solve problems (anagrams), they solve better and experience their solutions as more insight-like when, prior to solution, solution-related words are presented to them subliminally (Bowden 1997). The fact that a subliminal prime can spark a later insight supports the hypothesis that insight solutions are preceded by substantial unconscious processing rather than sponta- neously generated. Similarly, when people respond to solution words before solving a problem, the amount of semantic priming for solution words—an index of related unconscious processing— is directly related to how they experience the recognition of the solution. Speciﬁcally, people show more solution priming when they recognize solution words with a feeling of sudden insight than when they recognize the words without an insight experience (Bowden & Jung-Beeman 2003b). These studies are also notable for a methodological innovation. Much of the insight literature compares performance on so-called insight problems with performance on analytic problems, a distinction based largely on researchers’ intuitions or introspections about sudden versus grad- ual solution. This phenomenological difference had rarely been measured and quantiﬁed in a rigorous way. Furthermore, applying the monikers “insight” and “analytic” to speciﬁc problems assumes that all participants will always solve insight problems insightfully and analytic problems analytically—hardly a safe assumption. To put insight research on a ﬁrmer empirical founda- tion, Bowden (1997) developed a procedure for soliciting participants’ trial-by-trial judgments of whether a solution had been derived by insight or analysis. This technique has been validated by subsequent studies that have shown that the number of insight solutions and analytic solutions to a series of problems varies independently as a function of factors such as mood (Subramaniam et al. 2009) and meaningfully with respect to cognitive strategies (Kounios et al. 2008) and brain activations (Jung-Beeman et al. 2004, Kounios et al. 2006, Subramaniam et al. 2009). The insight judgment procedure has thus provided a foundation for subsequent neuroimaging studies of in- sight because it allows researchers to isolate the insight phenomenon by controlling for ancillary differences between problems that were solved insightfully and analytically (Bowden et al. 2005, Kounios & Beeman 2009).\\n\\nThe development of short problems solvable by insight (Bowden & Jung-Beeman 2003a) has also proved useful in later neuroscience studies. Early studies of insight typically posed a small number of complex problems to participants. Most participants take many minutes to solve such problems, when they are able to solve them. However, neuroimaging and electrophysiological methods require many trials to accurately record brain activity. An alternative approach uses a relatively large number of structurally identical verbal problems, called remote associates prob- lems, modeled after one type of problem developed by Mednick for his remote associates test of creativity (Mednick 1962). Bowden & Jung-Beeman (2003a) developed a set of compound remote\\n\\n76\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nassociates problems that consist of three words (e.g., pine, crab, sauce). The participant’s task is to think of a single solution word (apple) that will form a compound or familiar phrase with each of the three problem words (pineapple, crabapple, applesauce).\\n\\nRemote associates problems are well suited to neuroimaging and electrophysiological studies. Large numbers of these problems have been developed, allowing for neuroimaging and elec- trophysiological studies with a sufﬁcient number of trials per condition. Other types of short problems can serve this function as well. For example, anagrams have also been used with the insight judgment procedure (Bowden 1997, Kounios et al. 2008).\\n\\nNEURAL BASIS OF INSIGHT\\n\\nHemispheric Asymmetry\\n\\nMuch of the research on the neural basis of insight has been framed by hemispheric differences, namely, that the right hemisphere contributes relatively more to insight solving than to analytic solving, whereas the left hemisphere contributes more to analytic solving than to insight solving. This hypothesis particularly inﬂuenced the experimental methods and predictions of early cog- nitive neuroscience studies of insight. For instance, several studies used visually lateralized probe words to detect and compare semantic processing in the hemispheres while participants worked on remote associates problems. On trials for which participants failed to solve problems within a time limit, they still showed semantic priming for the solution words by responding to solution word probes more quickly than to unrelated word probes. Importantly, this solution priming was especially pronounced when the solution word probes were presented to the left visual ﬁeld, thus being directed initially to the right hemisphere (Beeman & Bowden 2000, Bowden & Beeman 1998). Furthermore, enhanced priming in the right hemisphere occurred only when participants reported that they recognized a solution word probe with a feeling of insight (Bowden & Jung- Beeman 2003b).\\n\\nThisrightwardasymmetryofinsightprocessingwaspredicted(Bowden&Beeman1998)onthe basis of prior evidence of right hemisphere involvement in integrating distant semantic relations in language input (e.g., St George et al. 1999) as well as a theoretical framework that describes the right hemisphere as engaging in relatively coarser semantic coding than the left hemisphere (Jung-Beeman2005).Thisframeworkincorporatesneuropsychologicalandneurologicalevidence of subtle comprehension deﬁcits following right hemisphere brain damage with neuroanatomical ﬁndings of asymmetric neuronal wiring.\\n\\nAccordingtothecoarsesemanticcodingframework,whenreadersorlistenersencounteraword or concept, they activate a semantic ﬁeld related to the word: a subset of features, properties, and associations of that word. Evidence suggests that the left hemisphere strongly activates a relatively smaller semantic ﬁeld of features, those most closely related to the dominant interpretation or the current context; in contrast, the right hemisphere weakly activates a relatively broader semantic ﬁeld, including features that are distantly related to the word or context (Chiarello 1988, Chiarello et al. 1990). Despite some obvious limitations, coarser semantic coding in the right hemisphere has one big advantage: The less sharply each word’s meaning is speciﬁed, the more likely it is to connect to other words and concepts. This is a key ingredient for drawing inferences (Virtue et al. 2006, 2008), extracting the gist (St George et al. 1999), comprehending ﬁgurative language (Mashal et al. 2008), and for insight.\\n\\nThe coarse semantic coding notion is more than a metaphor. Rather, it potentially links asym- metric semantic processing to asymmetric brain wiring. Aside from some size asymmetries in par- ticular regions of cortex (such as Broca’s area and Wernicke’s area), lateralized cytoarchitectonic\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n77\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\ndifferencesalsoinﬂuencehowneuronsintegrateinputs(forareview,seeHutsler&Galuske2003). In brief, pyramidal neurons collect inputs through their dendrites. Differences in synaptic distri- butionsalongdendritesinﬂuencethetypeofinputsthatcausethesepyramidalneuronstoﬁre.The range of cortical area over which neurons collect inputs could be termed their input ﬁelds. In as- sociation cortices in or near language-critical areas, such as Wernicke’s area, Broca’s area, and the anteriortemporalcortex,righthemisphereneuronshavelargerinputﬁeldsthandolefthemisphere neurons (e.g., Jacob et al. 1993, Scheibel et al. 1985, Seldon 1981). Speciﬁcally, right hemisphere pyramidal neurons have more synapses overall and especially more synapses far from the cell body. This indicates that they have larger input ﬁelds than corresponding left hemisphere pyramidal neurons. Because cortical connections are spatially organized, the right hemisphere’s larger in- put ﬁelds collect more differentiated inputs, perhaps requiring a variety of inputs to ﬁre. The left hemisphere’ssmallerinputﬁeldscollecthighlysimilarinputs,likelycausingtheneurontorespond best to somewhat redundant inputs. Outputs from neurons appear to show similar asymmetry; for example, axons in superior temporal cortex are longer in the right hemisphere than in the left hemisphere, favoring more integrative processing in the right hemisphere (Tardif & Clarke 2001). These neuroanatomical asymmetries could contribute to the right hemisphere’s bias to en- gage in coarser semantic coding and the left hemisphere’s bias to engage in ﬁner (i.e., less coarse) semantic coding. As previously noted (Jung-Beeman 2005), there is a huge gap between descrip- tions of dendritic branching and modes of language processing or problem solving. However, the asymmetries that exist in neuronal wiring almost certainly inﬂuence information processing, and the asymmetries that indisputably exist in language processing must have some neuroanatomical basis. The coarser semantic coding framework attempts to bridge that gap. In so doing, it also provides an avenue for future research on the relationship between neural microcircuitry and higher cognitive functions.\\n\\nNeural Correlates of Insight Solving\\n\\nFurther speciﬁcation of the neural bases of insight can be achieved through neuroimaging studies. These studies have identiﬁed a number of distinct components of insight and have generally supported the idea that the right hemisphere contributes relatively more to insight than to analytic solving.\\n\\nOne early neuroimaging study of insight isolated neural correlates of the insight experience with both fMRI and high-density EEG in separate experiments matched as closely as possible for procedure (Jung-Beeman et al. 2004). EEG has excellent temporal resolution but limited spatial resolution.Itisthereforegoodatcircumscribinganeuralprocessintime.fMRIhasexcellentspatial resolution but limited temporal resolution and is therefore best suited to localize a neural event in space. Together these techniques were able to isolate insight’s neural correlates in both space and time. This combination of methods was crucial, because fMRI’s power to localize insight-related neural activity would have been less informative without knowing whether these neural correlates occurred before, after, or at the moment of solution. A neural correlate of the insight experience itself would have to occur at, or immediately prior to, the moment of conscious awareness of a solution.\\n\\nAt the moment when people solve problems by insight, relative to solving identical problems by analytic processing, EEG shows a burst of high-frequency (gamma-band) EEG activity over the right temporal lobe, and fMRI shows a corresponding change in blood ﬂow in the medial aspect of the right anterior superior temporal gyrus (Jung-Beeman et al. 2004) (Figure 1). In the initial fMRI experiment, this right temporal area was the only area exceeding strict statistical thresholds, but weak activity was detected in other areas, including bilateral hippocampus and\\n\\n78\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\na\\n\\nb\\n\\nfMRI\\n\\nEEG gamma\\n\\nc\\n\\nr e w o p G E E\\n\\nInsight\\n\\nAnalytic\\n\\nTime to solution\\n\\nFigure 1\\n\\nNeural correlates of insight. (a) Insight-related blood oxygen–level dependent (BOLD) activity in the right anterior superior temporal gyrus recorded by functional magnetic resonance imaging (fMRI). (b) Insight- related gamma-band oscillatory activity recorded by electroencephalogram (EEG) over the anterior right temporal lobe. (c) Time course of insight- and analysis-related gamma-band EEG power recorded at a right anterior electrode. The vertical gray line marks the point in time at which participants made a bimanual button press to indicate that they had solved a problem. EEG power leading up to insight and analytic solutions diverges at approximately 300 ms before the bimanual button press. Taking into consideration that a button press requires about 300 milliseconds to initiate and execute (Smith & Kounios 1996), the insight-related burst of gamma activity occurred at approximately the time at which the solution to the problem became available to participants. Adapted from Jung-Beeman et al. (2004), with permission.\\n\\nparahippocampal gyri and anterior and posterior cingulate cortex. In a later replication with more participants and stronger imaging methods (Subramaniam et al. 2009), the same network of areas all far exceeded critical statistical threshold, with the right anterior temporal region again being the strongest. The close spatial and temporal correspondence of the fMRI and EEG results obtained by Jung-Beeman et al. suggested that they were produced by the same underlying brain activation. This right temporal brain response was identiﬁed as the main neural correlate of the insight experience because (a) it occurred at about the moment when participants realized the solution to each of these problems, (b) the same region is involved in other tasks demanding semantic integration (St George et al. 1999); and (c) gamma-band activity has been proposed to be a mechanismfor binding informationas it emerges intoconsciousness (Tallon-Baudry&Bertrand 1999). Alternative interpretations of this ﬁnding were rejected based on considerations of timing, functional neuroanatomy, etc.\\n\\nThe burst of gamma-band EEG activity in the right temporal lobe was not unexpected, given earlier visual half-ﬁeld studies (Beeman & Bowden 2000, Bowden & Beeman 1998, Bowden & Jung-Beeman 2003b). However, the EEG results revealed another, totally unexpected, ﬁnding. The insight-related gamma-band activity was immediately preceded by a burst of alpha-band activity (10 Hz) measured over right occipital cortex (Jung-Beeman et al. 2004) (see Figure 2).\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n79\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\n1.0 × 10–10\\n\\n2.5 × 10–11\\n\\n1.5 × 10–11\\n\\n6.0 × 10–11\\n\\nAlpha insight effect\\n\\nG a m m a i\\n\\nt c e ff e t h g i s n\\n\\n5.0 × 10–12\\n\\n2.0 × 10–11\\n\\nn s i g h t e ff e c t\\n\\n0\\n\\n0\\n\\ni a h p A\\n\\n–5.0 × 10–12\\n\\n–2.0 × 10–11\\n\\nGamma insight effect\\n\\nl\\n\\n–6.0 × 10–11\\n\\n–1.5 × 10–11\\n\\n–2.5 × 10–11\\n\\n–1.0 × 10–10\\n\\nR\\n\\n–1.5\\n\\n–2.0\\n\\n–1.0 Time (sec)\\n\\n–0.5\\n\\nFigure 2\\n\\nThe time course of the insight effect. Alpha power (9.8 Hz at right parietal-occipital electrode PO8) and gamma power (39 Hz at right temporal electrode T8) for the insight effect (i.e., correct insight solutions minus correct noninsight solutions, in v2). The left y-axis shows the magnitude of the alpha insight effect (purple line); the right y-axis applies to the gamma insight effect (green line). The x-axis represents time (in seconds). The gray arrow and R (at 0.0 sec) signify the time of the button-press response. Note the transient enhancement of alpha on insight trials (relative to noninsight trials) prior to the gamma burst.\\n\\nAlpha-band oscillations reﬂect neural inhibition; occipital alpha reﬂects inhibition of visual inputs, that is, sensory gating (Jensen & Mazaheri 2010). It appears likely that the preinsight alpha burst reﬂects transient sensory gating that reduces noise from distracting inputs to facilitate retrieval of the weakly and unconsciously activated solution represented in the right temporal lobe (Jung- Beemanetal.2004;cf.Wuetal.2009).Thisideaisanalogoustothecommonbehaviorofclosingor averting one’s eyes to avoid distractions that would otherwise interfere with intense mental effort. The discovery of transient sensory gating immediately preceding the insight-related burst of gamma-band activity suggested a promising research strategy. Previous behavioral research had demonstrated the discrete, all-or-nothing nature of insight solutions (Smith & Kounios 1996). On the other hand, visual hemiﬁeld solution-priming studies showed that insight, though consciously abrupt, is preceded by unconscious processing, primarily in the right hemisphere (e.g., Bowden & Beeman 1998). So, an insightful solution is a discrete phenomenon in terms of its availability to awareness, but it is preceded by unconscious neural precursors. It should therefore be possible to trace these neural precursors backward in time from the gamma burst at the moment of insight to reveal the brain mechanisms that unfold to produce an insight.\\n\\nPreparation for Insight\\n\\nBefore people even start to tackle a problem, their state of mind—and their brain activity— predisposes them to solve either by insight or analytic processing. Participants’ neural activity, assessed with both EEG and fMRI during a task-free preparation phase prior to each remote associates problem, shows that such predispositions do occur: Distinct patterns of neural activity precede problems that people eventually solve by insight versus those that they solve by analysis\\n\\n80\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\n(Kounios et al. 2006). EEG showed that preparation for analytic solving involves increased neural activity (i.e., decreased alpha-band activity) measured over visual cortex, hypothesized to reﬂect outward focus of attention directed to the computer monitor on which the next problem in the sequence was to be displayed. Both EEG and fMRI revealed that preparation for insight solving involves activation of the anterior cingulate and bilateral temporal cortices. The temporal lobe activation suggests that cortical regions involved in lexical and semantic processing are prepared to respond. Previous research implicates anterior cingulate cortex in monitoring other brain regions for conﬂicting action tendencies (Botvinick et al. 2004). Kounios et al. (2006) expanded on this notion of conﬂicting action tendencies to propose that the anterior cingulate’s role in problem solving is to detect the activation of conﬂicting solution possibilities or strategies. If the anterior cingulate is sufﬁciently activated at the time a problem is presented, then it can detect the weak activationofnondominantsolutionpossibilities,enablingattentiontoswitchtooneoftheseweakly activated ideas. Switching attention to a nonobvious solution brings the idea to awareness as an insight. However, when the anterior cingulate is relatively deactivated prior to the presentation of a problem, attention is dominated by the more obvious associations and solution possibilities afforded by the problem.\\n\\nThus, the transient state of one’s attentional focus, varying from trial to trial, helps to de- termine the range of potential solutions that a person is prepared to consider when a problem is presented: Outwardly directed attention coupled with low anterior cingulate activity focuses processing on the dominant features or possibilities of a situation; inwardly directed attention and high anterior cingulate activity heightens sensitivity to weakly activated remote associations and long-shot solution ideas.\\n\\nOne important, but unresolved, issue is to what extent such preparatory activity is under vol- untary control and to what extent spontaneous shifts of attention may be involved. Kounios et al. (2006) found no evidence of any trial-to-trial sequential clusters of insight or analytic solutions that would suggest slow spontaneous shifts of attention. However, this kind of analysis would not be capable of showing attention shifts on a timescale shorter than the duration of a single trial (i.e., approximately10sec.).Identifyinganyneuralcorrelatesofpossiblestrategicorspontaneousatten- tion shifts is therefore an important focus for future investigations. Progress in elucidating the role of attention shifts would suggest practical techniques for controlling or enhancing cognitive style.\\n\\nResting-State Brain Activity and Individual Differences\\n\\nGiven that transient shifts in attention inﬂuence whether people solve by analysis or by insight, do any longer-lasting states or traits inﬂuence this preparatory activity and the corresponding predisposition to solve by insight or analysis? One approach is to examine whether individual differences in resting-state brain activity while people have no task to perform or any particular expectation about what will follow may inﬂuence their subsequent problem-solving style. In one study, we recorded participants’ resting-state EEGs before tasking them with solving a series of anagrams (Kounios et al. 2008). Participants were classiﬁed as high insight or low insight based on the proportion of problems they solved with insight. These groups exhibited different patterns of resting-stateEEGs,suggestingthattheinsightfulandanalyticcognitivestyleshavetheiroriginsin, or are at least related to, distinct patterns of resting-state neural activity. The differences between these patterns highlighted two general phenomena.\\n\\nInsightful individuals show greater right hemisphere activity at rest, relative to analytic indi- viduals, consistent with the idea of insight-related right hemisphere bias described above. The fact that an insight-related hemispheric difference can be found in the resting state suggests that the functional hemispheric asymmetry occurring during problem solution (Jung-Beeman et al.\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n81\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\n2004) may have its origin in structural hemispheric differences among people, such as the cytoar- chitectonic differences described above (Jung-Beeman 2005) or in asymmetries of structural or functional connectivity. Insightful individuals also showed greater diffuse activation of visual cortex compared to an- alytical individuals, even when resting-state EEGs were measured while participants’ eyes were closed (Kounios et al. 2008). This ﬁnding mirrors earlier behavioral research showing that highly creative individuals tend to have diffuse attention when at rest or when cognitive resources are not dominated by a task (e.g., Ansburg & Hill 2003, Carson et al. 2003).\\n\\nResting-state EEG can be perturbed by stimuli but otherwise is relatively stable; in fact, be- havioral genetics studies show that individual differences in resting-state EEG have a substantial genetic loading largely attributable to individual differences in gray matter and white matter volume (Smit et al. 2012). It is not yet known whether insight-related individual differences in resting-state EEG are a subset of the genetically loaded individual differences in EEG or brain volume, but if so, this would be a promising avenue of investigation into the stability and origins of cognitive style.\\n\\nStudies of insight-related resting-state brain activity may also provide a link to recent so- cial psychological research on construal level. According to construal level theory, psychological “distance”—thinking about things that are far away in space or time, or about people that are different from oneself—engages abstract thinking (Trope & Liberman 2010). Based on this idea, F¨orsteretal.(2004)predictedthatprimingpeopletothinkaboutthedistantfuturewouldbiasthem to think abstractly, which in turn would induce a person to think more insightfully and creatively; conversely, priming people to think about the near future would bias them to think concretely and therefore analytically. Their studies supported these predictions: Participants primed by asking them to think about the distant future subsequently did better on insight and creativity tasks; those asked to think about the near future did better on analytic tasks. These results are particularly interesting because construal-level priming may inﬂuence task-related cognition by transiently al- tering the resting-state brain activity from which task-related brain activity emerges. A potentially fruitful line of research would be to examine how various types of priming might inﬂuence the ten- dency to solve problems insightfully or analytically by imposing transient changes in resting-state brain activity.\\n\\nThus, distinct patterns of neural activity are associated with insight versus analytic solving at the moment of insight, in the last two seconds leading up to that moment, in the preparation phase prior to presentation of a problem, and even in resting-state brain activity of individuals who tend to solve by insight contrasted with those who tend to solve analytically. These ﬁndings objectively substantiate an abundance of behavioral evidence that indicates insight solving differs from analytic solving and that these solving styles result from different tunings of the network of brain areas involved in problem solving. Moreover, the speciﬁc areas associated at each distinct stage of solving (or preparation) help inform theories of how insight is different from analysis. Compared to analytic solving, insight requires greater input from and integration of relatively coarser semantic processing of right hemisphere temporal areas, greater sensitivity to competing responses in cognitive control mechanisms supported by the anterior cingulate cortex, and a relative emphasis on internal processing and de-emphasis on external stimuli.\\n\\nATTENDING IN, OUT, AND AROUND\\n\\nA prominent theme in insight and creativity research is the role that attention deployment plays in cognitive style. Though a number of behavioral studies have observed that highly creative individuals have diffuse attention, taken as a whole, neuroimaging and electrophysiological studies\\n\\n82\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nof insight suggest that attention plays a more nuanced role. Neural activity during the preparatory phase suggests that attention can also be focused outwardly on external objects or inwardly on internal knowledge in memory (see Chun et al. 2011), which inﬂuences the likelihood that people will solve with insight (Kounios et al. 2006, Wegbreit et al. 2012).\\n\\nOccipital alpha-band EEG activity reﬂects visual cortex inhibition that protects fragile internal processing from potentially interfering or distracting perceptual inputs (Ray & Cole 1985). Levels of occipital alpha during the different phases leading up to the solution of a problem suggest that (a) during the resting state, insightful individuals have more externally oriented attention than do analytical individuals (Kounios et al. 2008); (b) during the preparation phase prior to the presentation of a problem that will be solved by insight, there is greater internal focus of attention (Kounios et al. 2006); and (c) just before the emergence of an insight, there is another brief burst of inward focus (Jung-Beeman et al. 2004).\\n\\nThus, the notion that insight is associated with diffuse attention appears to be an oversimpli- ﬁcation. Insightful individuals may generally have more diffuse and outwardly directed attention, but successful insight solving involves transiently redirecting attention inwardly during the prepa- ration for and solving of a problem. It therefore appears that the tendency to solve problems insightfully is associated with broad perceptual intake as the default mode of resting-state atten- tion deployment, coupled with the tendency to focus inwardly in preparation for, and during, solving. In contrast, analytical people’s resting-state attention is less outwardly focused during the resting state and less inwardly directed during preparation and solving.\\n\\nFACTORS THAT INFLUENCE THE LIKELIHOOD OF INSIGHT\\n\\nUnderstanding the factors that can increase or reduce the likelihood of experiencing insight is important for both theoretical and practical reasons. Besides contributing toward the develop- ment of a theoretical model of insight, understanding these factors also suggests strategies for its enhancement outside the laboratory. Here we focus on the interrelated factors of mood, attention, and cognitive control.\\n\\nMood\\n\\nPositiveaffectenhancesinsightandotherformsofcreativity,bothwhenthemoodoccursnaturally and when it is induced in the laboratory (e.g., Ashby et al. 1999, Isen et al. 1987). Though some aspects of creative production may be impeded by positive mood or aided by other moods such as depression (Verhaeghen et al. 2005), insight and related processes seem to beneﬁt from greater positive mood (or reduced anxiety) either when participants enter the lab in a relatively positive mood or after they watch funny ﬁlm clips (H. Mirous & M. Beeman, manuscript submitted; Subramaniam et al. 2009). Facilitation of insight by positive mood has also been demonstrated in the workplace, as documented by diaries and self-reports (Amabile et al. 2005).\\n\\nMood inﬂuences other cognitive abilities that are related to insight and creativity. For example, positivemoodfacilitatesintuition,theabilitytomakedecisionsorjudgmentsaboutstimuliwithout conscious access to the information or processes inﬂuencing their behavior. When people are working on remote associates problems, they show better-than-chance judgment about whether individual problems are solvable before they are able to state the solution; such intuitive judgments are improved after people recall happy autobiographical events and are impeded after recalling sad autobiographical events (Bolte et al. 2003).\\n\\nFinding or intuiting the presence of a solution to a remote associates problem requires a person to access weak associations of the problem words because the solution is typically not a strong\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n83\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nassociate of all three problem words. Thus, the presentation of the problem will evoke only weak activation of the solution’s representation. Positive mood seems to broaden the scope of semantic processing to make such weak associations more accessible (Isen & Daubman 1984, Isen et al. 1985). For instance, the amplitude of the N400 component of the event-related potential (ERP) is inversely proportional to the relatedness of a word to its semantic context (Kounios 1996), and N400 semantic relatedness effects are modulated by mood. When a positive mood is induced, target words that loosely ﬁt their semantic context elicit a smaller N400 than when the participant is in a neutral mood (Federmeier et al. 2001). This indicates that a positive mood makes a word seem less incongruent with its semantic context. Additionally, when people listen to stories that imply speciﬁc causal events without stating them explicitly, they show sensitivity to semantic information that is related to the implicit inference. Speciﬁcally, they read or respond to probe words that are related to an implicit inference more quickly than they read or respond to unrelated probe words. Such inference-related priming normally occurs earlier and more strongly for words presented to the right hemisphere (left visual ﬁeld) than for words presented to the left hemisphere (right visual ﬁeld) (Beeman et al. 2000). But people show stronger inference-related priming to inference-related words presented in the middle of the visual ﬁeld while listening to stories after watching funny ﬁlm clips than after watching emotionally neutral ﬁlms; they show no inference- related priming after watching scary ﬁlm clips that induce anxiety (H. Mirous & M. Beeman, manuscript submitted).\\n\\nRecently it has also been argued that the relation between broad associations and positive mood is bidirectional, making it possible to induce a positive mood by instructing or otherwise inducing participants to process remote associations (Bar 2009, Bruny´e et al. 2013). According to this idea, a positive mood both facilitates insight and is enhanced by it. This shows deep integration of cognitive and affective processes and relates to many everyday behaviors, such as peoples’ enjoyment of verbal puzzles, especially those with surprising solutions.\\n\\nThe fact that people engage in coarser semantic coding when they are in a positive mood compared to when they are in neutral or anxious mood raises the possibility that positive mood may selectively activate right hemisphere semantic processing. However, as of yet, there is no evidence to support this hypothesis. For example, the neuroimaging study of insight and mood by Subramaniam et al. (2009) found no evidence of lateralized differences in brain activity that were attributable to mood.\\n\\nMood and Attention\\n\\nA more likely hypothesis is that mood inﬂuences the likelihood of insight by modulating attention or cognitive control, which in turn modulates semantic processing. For example, anxiety narrows the scope of attention by eliciting excessive focus on the center of one’s ﬁeld of vision—which usually includes the source of the threat—to the exclusion of peripheral information (Easterbrook 1959). From the evolutionary standpoint, this makes great sense: Early humans spotting a lion on the African savannah would not want to be distracted by less important stimuli. In contrast, positive mood appears to broaden attention. It increases the perception and utilization of global and peripheral perceptual features at the expense of the local details of complex stimuli by spatially broadeningthe“spotlight”ofattention(Gasper&Clore2002).Forexample,inataskthatrequires participants to respond to a centrally located target stimulus and disregard other stimuli that ﬂank it, a positive mood increases both facilitation and interference of target processing attributable to the ﬂanking stimuli (Rowe et al. 2007). Beyond visual processing, positive mood seems to broaden the processing of novel and varied stimuli, stimulating exploratory behavior (Fredrickson & Branigan 2005).\\n\\n84\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nModels of Attention\\n\\nOne possibility is that mood affects the balance between two brain systems: an anterior attention network that maintains top-down control over perceptual processing in the service of goals and a posterior network involved in bottom-up attentional capture by salient stimuli. According to attentional control theory, anxiety shifts the balance away from the top-down system toward the bottom-up system (Derakshan & Eysenck 2009), leading to enhanced distractibility by task- irrelevant stimuli, especially threatening stimuli (Bar-Haim et al. 2007). This view suggests that anxiety shifts attention toward external stimuli and away from internal representations, states, and goals; positive affect may have the opposite effect.\\n\\nInpractice,attentionalcontroltheorymakespredictionsthataresimilartothoseofthespotlight modelof attention.Moreover, itlinks positivemood withinternally focusedattention ina way that is consistent with research implicating both of these factors in preparation for insight (Kounios etal.2006,Subramaniametal.2009).Attentionalcontroltheoryisthereforeapromisingdirection for future insight research.\\n\\nOne important question is why changes in the breadth of perceptual attention due to mood and other factors should be related to changes in the breadth or narrowness of thought to include or exclude remote associations, what Rowe et al. (2007) called conceptual attention. Rowe et al. demonstrated that a positive mood both increases the breadth of visual attention to include stimuli that ﬂank a target and enhances performance on remote associates problems that, as noted, require access to distant associations of the problem words. Rowe et al. (2007) argued that perceptual and conceptual attention are closely linked.\\n\\nCurrent attention theory can be expanded to include both phenomena. The biased competi- tion model of attention characterizes the neural computations subserving vision as a process of competition between the representations of the stimuli in the visual ﬁeld (Desimone 1998). This competition can be biased to favor a particular stimulus by a variety of factors, including externally driven (bottom-up) and internally driven (top-down) processes. Such biasing is manifested as an increase in the neural activity subserving one of the representations.\\n\\nA similar mechanism may underlie the conceptual processing that occurs during problem solv- ing. According to this idea, the presentation of a problem activates a number of associations in a person’s memory. Dominant associations are strongly activated; nondominant ones are weakly activated. All of these associations compete for processing resources, though under normal cir- cumstances, the dominant associations win the competition for further processing. However, top-down mechanisms can bias this competition toward weak associations by actively selecting nondominant representations; by expanding the scope of attention to boost activation of non- dominant representations; or, simply, by not suppressing the less dominant associations when the dominant ones capture the spotlight. Dominant associations are already as activated as they can be, so expanding the scope of attention would beneﬁt weak associations more than it would do for strong ones giving weak associations a greater opportunity to capture attention and spark an insight.Bottom-upprocessescanalsobiasthecompetitionbetweenideas.Stimuliinthesurround- ing environment that are related to the solution can intervene to bias processing toward a weak association by acting as a hint that triggers an insight (Bowden 1997, Seifert et al. 1995). Thus, the biased competition model may be generalized to explain the role of conceptual attention in problem solving by insight.\\n\\nCognitive Control\\n\\nOther research has focused on the role of cognitive control, especially the ability to maintain or switch between different thoughts, actions, and goals. People often focus on a task or goal\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n85\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nto shield it from distraction. In other cases, especially in creative tasks, people need to ﬂexibly switch between different processes, associations, or goals. These two functions, task shielding and task switching, appear to be in direct competition: The need for ﬂexible switching demands relaxation of task shielding and leaves processing open to distraction (Dreisbach 2012). Pos- itive affect enhances task switching but yields increased distractibility (Dreisbach & Goschke 2004).\\n\\nAs noted above, preparation for insight involves increased activity in the anterior cingulate during the preparatory period preceding a problem (Kounios et al. 2006). Anterior cingulate acti- vation is hypothesized to be a sign that problem solvers are sensitized to competing, nondominant associations that they can switch to, resulting in an insight. When a person is in a positive mood, the preparation period shows stronger anterior cingulate activation than occurs for people not in a positive mood. In fact, the anterior cingulate was found to be the only brain area whose activa- tion varies with mood, preparation for insight versus analytic processing, and later insight versus analytic solving (Subramaniam et al. 2009).\\n\\nTheanteriorcingulatehaslongbeenrecognizedasacriticalcomponentofthecognitivecontrol network. One hypothesis, backed up by substantial evidence, is that this brain region monitors other regions for competing action tendencies or stimulus representations (Kerns et al. 2004, Weissman et al. 2005). It has been proposed to be an interface between emotion and cognition (Allman et al. 2001, Bush et al. 2000, Lane et al. 1998) in part because some (ventral) regions of the cingulate are important for emotional processing (Mayberg et al. 1999).\\n\\nAnother brain area implicated in insight-related cognitive control is prefrontal cortex. Consid- erable evidence supports the idea that prefrontal cortex exerts control over other brain regions in response to input from the anterior cingulate signaling the presence of cognitive conﬂict (Miller & Cohen 2001). According to this idea, modulation of insight solving due to changes in an- terior cingulate activity should be mediated, at least in part, by control signals originating in prefrontal cortex that limit the range of possibilities that a person considers when working on a problem. This limiting function is ordinarily helpful because it focuses the solver on a small number of the most viable solution paths to avoid computational overload. However, it can be a hindrance when a person tries to solve a problem whose solution lies on a nonobvious solu- tion path. In support of this idea, patients with damage to lateral prefrontal cortex were better able to solve matchstick insight problems than were healthy control participants (Reverberi et al. 2005).\\n\\nSTIMULATING INSIGHT\\n\\nOne limitation of neuroimaging and electrophysiological studies is that they are inherently correlational—they don’t directly show that the recorded patterns of brain activity cause the measured changes in behavior or experience. But the advent of brain stimulation techniques now affords the opportunity to treat brain activity as an independent variable rather than a dependent one.\\n\\nRecent efforts have applied one brain-stimulation technique, transcranial direct current stimulation (tDCS), to attempt to enhance insight solving. Two recent studies have yielded promising results (Chi & Snyder 2011, 2012). Researchers tested the hemispheric hypothesis of insightbyapplyingfacilitatory(anodal)stimulationtorightfrontal-temporalcortexandinhibitory (cathodal) stimulation to left frontal-temporal cortex. This pattern of stimulation, but not the re- verse hemispheric pattern, signiﬁcantly enhanced solution rates for the nine-dot problem and for aninsightmatchstickproblem.Thisstimulationprotocolyieldedespeciallydramaticenhancement for the classic nine-dot problem, increasing the solution rate from 0% to 40%. Additional studies\\n\\n86\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nhave found that stimulation interfering with left dorsolateral prefrontal cortex facilitates solving compound remote associates problems (Metuki et al. 2012) or ﬂexibly generating unusual uses for objects (Chrysikou et al. 2013). These studies suggest that such tasks beneﬁt from the release of cognitive control that would otherwise maintain focus on ﬁne semantic coding in the left hemisphere; however, neither of these studies speciﬁcally contrasted insight versus analytic performance.\\n\\nSuch stimulation studies are encouraging initial efforts that provide striking support for the claim that insight depends relatively more on right than on left temporal lobe processes. They also raise the alluring possibility that someday brain stimulation techniques will be reﬁned to the point at which individuals grappling with difﬁcult problems may have the option of donning a “thinking cap” that will increase their ability to ﬁnd solutions. However, it should be kept in mind that these early studies—similar to other groundbreaking studies—raise as many questions as they answer. For example, this stimulation protocol simultaneously stimulated right frontal-temporal cortex and inhibited left frontal-temporal cortex (Chi & Snyder 2012). It is not yet known whether it was the right frontal-temporal stimulation, the left frontal-temporal inhibition, or the combination of the two that facilitated solving these insight problems. Moreover, it is not yet known whether this tDCS protocol actually increased the probability that participants solved these problems insightfully or whether it increased the probability that they solved the problems analytically. As we have noted, most problems can be solved by either strategy. The fact that researchers have considered the nine-dot and matchstick-type problems to be insight problems doesn’t preclude the possibility that they can be solved analytically. These studies did not assess the strategies that their participants employed, so all that is known is that the solution rates for these problems increased rather than how or why they increased. Thus, much foundational work must be done before tDCS can be considered a realistic possibility for adaptively modifying people’s cognitive strategies.\\n\\nPharmacological intervention is another route to insight enhancement. To date, we are aware of no studies that use drugs to attempt selective facilitation of insight solving. However, the recent demonstration that alcohol can enhance insight, but not analytic, solving of remote as- sociates problems shows that pharmacological promotion of insight is achievable (Jarosz et al. 2012).\\n\\nFUTURE DIRECTIONS\\n\\nNeuroimaging and electrophysiological techniques have begun to reveal neural substrates of in- sight that were invisible to behavioral research. This has led to progress in understanding how insight emerges from more basic cognitive mechanisms. Technologies for stimulating insightful thought are becoming available, including intervention by direct brain stimulation.\\n\\nNevertheless,fromourcurrentvantagepoint,itisimportanttokeepinmindthatthesurfacehas barely been scratched. Research has shown that multiple component processes and corresponding neural substrates are involved, and some of these are susceptible to subtle shifts in attention, mood, and other factors. Reﬁned methods will expand on the research we have described and contribute new ﬁndings from connectivity and network analyses. And one can only guess what will be uncovered by future studies of insight-related individual differences in neuroanatomy, cytoarchitectonics, and genetics. The psychopharmacology of insight and creativity, currently virtually unexplored, holds out the promise of contributing both to our scientiﬁc understanding of insight and to methods for enhancing it. Further research will reveal the limits and applicability of brain stimulation, neurofeedback, and cognitive training techniques for enhancing insight and, more generally, inﬂuencing and optimizing cognitive styles to suit different circumstances. The\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n87\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nstudy of insight began in the early twentieth century, but a century from now, researchers may look back at the early twenty-ﬁrst century as the beginning of a golden age of insight research.\\n\\nSUMMARY POINTS\\n\\n1. Insight is any sudden comprehension, realization, or problem solution that involves a reorganizationoftheelementsofaperson’smentalrepresentationofastimulus,situation, or event to yield a nonobvious or nondominant interpretation. Insight is sudden, but it is preceded by substantial unconscious processing.\\n\\n2. Some critical components of insight are preferentially associated with the right cerebral hemisphere.Insightculminateswithasharpincreaseinneuralactivityintherightanterior temporal lobe at the moment of insight.\\n\\n3. Insightsareimmediatelyprecededbyatransientreductionofvisualinputsthatapparently\\n\\nreduce distractions and boost the signal-to-noise ratio of the solution.\\n\\n4. Neural activity immediately before the presentation of an expected problem predicts whether that problem will be solved by insight or analytically. Such preparation for insight involves inwardly directed attention; preparation for analysis involves outwardly directed attention.\\n\\n5. Resting-state neural activity biases later processing to favor insight or analytical problem\\n\\nsolving.\\n\\n6. Positive mood facilitates insight by increasing attentional scope to include weakly acti-\\n\\nvated solution possibilities.\\n\\n7. Directstimulationofrightfrontal-temporalcortexcoupledwithinhibitionofleftfrontal-\\n\\ntemporal cortex enhances solving of insight problems.\\n\\n8. Cognitive neuroscience methods have contributed exciting new results and theories of insight;nevertheless,insightresearchisstillinaveryearlystage.Continuingapplications of new methods, paradigms, and models hold much promise for additional substantial progress.\\n\\nFUTURE ISSUES\\n\\n1. How and when can insight be facilitated? People often ask how they can foster more insightful thinking, even though in many situations and for many problems, analytic processing would be more effective. At the least, problems must be deeply analyzed [what GrahamWallas (1926) termed immersion] before an insight solution can be achieved. So the question becomes when insight should be facilitated. We suggest that the right time to facilitate insight is when semantic integration processes activate the representation of a potential solution to a level just below the threshold for consciousness, setting the stage for an aha moment. Thus, honing intuition to sense the presence of a subthreshold solution may be the ﬁrst step toward facilitating insight. At this time, inducing a positive mood and broadening attention through various means, and not directly focusing on the problem, is likely to increase the chance of achieving insight.\\n\\n88\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\n2. What individual differences in attention or cognitive control are most conducive to insight? Recent work shows that while in the resting state and not engaged in any task, individuals who tend to solve by insight deploy attention differently from individuals who are more analytic. What is the best way to characterize this pattern of differences? Do high-insight individuals deploy attention externally at rest and internally during solving? Or do they just deploy their attention in a less-focused manner? Do these differences primarily occur in bottom-up attention or primarily in top-down attention and cognitive control? How tightly coupled are insight-related individual differences in visual cortex and those in the anterior cingulate? Do individual differences in insightfulness have a genetic basis?\\n\\n3. How do individual differences in processes that support insight interact? In general, pos- itive mood facilitates insight by encouraging broader or less selective attention. How- ever, a greater tendency or ability to solve problems with insight may also be associated with individual differences in other factors that cause broad associative thinking, such as schizotypy (Folley & Park 2005). It is possible that for individuals who typically think more broadly, anxiety would facilitate solving by focusing their attention to harness their broader associative processes toward a useful solution.\\n\\n4. How does insight problem solving relate to other creative behavior? Insight is considered a critical facet of creative cognition. However, creativity is a highly complex behavior. Although aspects of creativity may be entirely unrelated, others are likely linked, perhaps sharingsimilarpatternsofattention.Howdoinsight,intuitivedecisionmaking,divergent thinking, and creative achievement relate to each other, and to attention and cognitive control?\\n\\nDISCLOSURE STATEMENT\\n\\nThe authors are not aware of any afﬁliations, memberships, funding, or ﬁnancial holdings that might be perceived as affecting the objectivity of this review.\\n\\nACKNOWLEDGMENTS\\n\\nPreparation of this article and some of the research described therein was supported by National Science Foundation grant 1144976 to J.K. and John Templeton Foundation grant 24467 to M.B.\\n\\nLITERATURE CITED\\n\\nAllman J, Hakeem A, Erwin JM, Nimchinsky E, Hof P. 2001. The anterior cingulate cortex: the evolution of\\n\\nan interface between emotion and cognition. Ann. N. Y. Acad. Sci. 935:107–17\\n\\nAmabile TM, Barsade SG, Mueller JS, Staw BM. 2005. Affect and creativity at work. Adm. Sci. Q. 50:367–403 Ansburg PI, Hill K. 2003. Creative and analytic thinkers differ in their use of attentional resources. Personal.\\n\\nIndivid. Differ. 34:1141–52\\n\\nAshby FG, Isen AM, Turken U. 1999. A neuropsychological theory of positive affect and its inﬂuence on\\n\\ncognition. Psychol. Rev. 106:529–50\\n\\nAziz-Zadeh L, Kaplan JT, Iacoboni M. 2009. “Aha!”: the neural correlates of verbal insight solutions. Hum.\\n\\nBrain Mapp. 30:908–16\\n\\nAziz-Zadeh L, Liew S-L, Dandekar F. 2013. Exploring the neural correlates of visual creativity. Soc. Cogn.\\n\\nAffect. Neurosci. 8: 475–80\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n89\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nBar M. 2009. A cognitive neuroscience hypothesis of mood and depression. Trends Cogn. Sci. 13:456–63 Bar-Haim Y, Lamy D, Pergamin L, Bakermans-Kranenburg MJ, van IJzendoorn MH. 2007. Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study. Psychol. Bull. 133:1–24 Beeman MJ, Bowden EM. 2000. The right hemisphere maintains solution-related activation for yet-to-be-\\n\\nsolved problems. Mem. Cogn. 28:1231–41\\n\\nBeeman MJ, Bowden EM, Gernsbacher MA. 2000. Right and left hemisphere cooperation for drawing pre-\\n\\ndictive and coherence inferences during normal story comprehension. Brain Lang. 71:310–36\\n\\nBolte A, Goschke T, Kuhl J. 2003. Emotion and intuition: effects of positive and negative mood on implicit\\n\\njudgments of semantic coherence. Psychol. Sci. 14:416–21\\n\\nBotvinick MM, Cohen JD, Carter CS. 2004. Conﬂict monitoring and anterior cingulate cortex: an update.\\n\\nTrends Cogn. Sci. 8:539–46\\n\\nBowden EM. 1997. The effect of reportable and unreportable hints on anagram solution and the aha! experi-\\n\\nence. Conscious. Cogn. 6:545–73\\n\\nBowden EM, Beeman MJ. 1998. Getting the right idea: Semantic activation in the right hemisphere may help\\n\\nsolve insight problems. Psychol. Sci. 9(6):435–40\\n\\nBowden EM, Jung-Beeman M. 2003a. Normative data for 144 compound remote associate problems. Behav.\\n\\nRes. Methods 35(4):634–39\\n\\nBowden EM, Jung-Beeman M. 2003b. Aha! Insight experience correlates with solution activation in the right\\n\\nhemisphere. Psychon. Bull. Rev. 10:730–37\\n\\nBowden EM, Jung-Beeman M, Fleck J, Kounios J. 2005. New approaches to demystifying insight. Trends\\n\\nCogn. Sci. 9(7):322–28\\n\\nBruny´e TT, Gagnon SA, Paczynski M, Shenhav A, Mahoney CR, Taylor HA. 2013. Happiness by association:\\n\\nBreadth of free association inﬂuences affective states. Cognition 127:93–98\\n\\nBush G, Luu P, Posner MI. 2000. Cognitive and emotional inﬂuence in anterior cingulate cortex. Trends Cogn.\\n\\nSci. 4:215–22\\n\\nCarson SH, Peterson JB, Higgins DM. 2003. Decreased latent inhibition is associated with increased creative\\n\\nachievement in high-functioning individuals. J. Personal. Soc. Psychol. 85:499–506\\n\\nChi RP, Snyder AW. 2011. Facilitate insight by non-invasive brain stimulation. PLoS ONE 6(2):e16655 Chi RP, Snyder AW. 2012. Brain stimulation enables the solution of an inherently difﬁcult problem. Neurosci.\\n\\nLett. 515:121–24\\n\\nChiarello C. 1988. Lateralization of lexical processes in the normal brain: a review of visual half-ﬁeld research. In Contemporary Reviews in Neuropsychology, ed. HA Whitaker, pp. 36–76. New York: Springer-Verlag Chiarello C, Burgess C, Richards L, Pollock A. 1990. Semantic and associative priming in the cerebral hemi-\\n\\nspheres: some words do, some words don’t...sometimes, some places. Brain Lang. 38:75–104\\n\\nChrysikou EG, Hamilton RH, Coslett HB, Datta A, Bikson M, Thompson-Schill SL. 2013. Noninvasive transcranial direct current stimulation over the left prefrontal cortex facilitates cognitive ﬂexibility in tool use. Cogn. Neurosci. 4:81–89\\n\\nChun MM, Golomb JD, Turk-Browne NB. 2011. A taxonomy of external and internal attention. Annu. Rev.\\n\\nPsychol. 62:73–101\\n\\nCranford EA, Moss J. 2012. Is insight always the same? A protocol analysis of insight in compound remote\\n\\nassociate problems. J. Probl. Solving 4(2):128–53\\n\\nDerakshan N, Eysenck MW. 2009. Anxiety, processing efﬁciency, and cognitive performance: new develop-\\n\\nments from attentional control theory. Eur. Psychol. 14:168–76\\n\\nDesimone R. 1998. Visual attention mediated by biased competition in extrastriate visual cortex. Philos. Trans.\\n\\nR. Soc. Lond. B 353:1245–55\\n\\nDietrich A, Kanso R. 2010. A review of EEG, ERP, and neuroimaging studies of creativity and insight. Psychol.\\n\\nBull. 136:822–48\\n\\nDreisbach G. 2012. Mechanisms of cognitive control: the functional role of task rules. Curr. Dir. Psychol. Sci.\\n\\n21:227–31\\n\\nDreisbach G, Goschke T. 2004. How PA modulates cognitive control: reduced perseveration at the cost of\\n\\nincreased distractibility. J. Exp. Psychol.: Learn. Mem. Cogn. 30(2):343–53\\n\\nEasterbrook JA. 1959. The effect of emotion on cue utilization and the organization of behavior. Psychol. Rev.\\n\\n66:183–201\\n\\n90\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nFedermeier KD, Kirson DA, Moreno EM, Kutas M. 2001. Effects of transient, mild mood states on semantic memory organization and use: an event-related potential investigation in humans. Neurosci. Lett. 305:149– 52\\n\\nFolley BS, Park S. 2005. Verbal creativity and schizotypal personality in relation to prefrontal hemi- spheric laterality: a behavioral and near-infrared optical imaging study. Schizophr. Res. 80:271–82 F¨orster J, Friedman RS, Liberman N. 2004. Temporal construal effects on abstract and concrete thinking:\\n\\nDemonstrates that schizotypes show more creativity than normal control participants in a divergent thinking task.\\n\\nconsequences for insight and creative cognition. J. Personal. Soc. Psychol. 87:177–89\\n\\nFredrickson B, Branigan C. 2005. Positive emotions broaden the scope of attention and thought-action reper-\\n\\ntoires. Cogn. Emot. 19:313–32\\n\\nGasper K, Clore GL. 2002. Attending to the big picture: mood and global versus local processing of visual\\n\\ninformation. Psychol. Sci. 13:34–40\\n\\nHutsler J, Galuske RA. 2003. Hemispheric asymmetries in cerebral cortical networks. Trends Neurosci. 26:429–\\n\\n35\\n\\nIsen AM, Daubman KA. 1984. The inﬂuence of affect on categorization. J. Personal. Soc. Psychol. 47:1206–17 Isen AM, Daubman KA, Nowicki GP. 1987. Positive affect facilitates creative problem solving. J. Personal.\\n\\nSoc. Psychol. 52:1122–31\\n\\nIsen AM, Johnson MM, Mertz E, Robinson GF. 1985. The inﬂuence of positive affect on the unusualness of\\n\\nword associations. J. Personal. Soc. Psychol. 48:1413–26\\n\\nJacob R, Schall M, Scheibel AB. 1993. A quantitative dendritic analysis of Wernicke’s area in humans. II.\\n\\nGender, hemispheric, and environmental factors. J. Comp. Neurol. 327:97–111\\n\\nJarosz AF, Colﬂesh GJ, Wiley J. 2012. Uncorking the muse: Alcohol intoxication facilitates creative problem\\n\\nsolving. Conscious. Cogn. 21:487–93\\n\\nJensen O, Mazaheri A. 2010. Shaping functional architecture by oscillatory alpha activity: gating by inhibition.\\n\\nFront. Hum. Neurosci. 4:186\\n\\nJung-Beeman M. 2005. Bilateral brain processes for comprehending natural language. Trends Cogn. Sci. 9:512–\\n\\n18\\n\\nJung-Beeman M, Bowden EM, Haberman J, Frymiare JL, Arambel-Liu S, et al. 2004. Neural activity when\\n\\npeople solve verbal problems with insight. PLoS Biol. 2(4):e97\\n\\nKaplan CA, Simon HA. 1990. In search of insight. Cogn. Psychol. 22:374–419 Kerns JG, Cohen JD, MacDonald AW III, Cho RY, Stenger VA, Carter CS. 2004. Anterior cingulate, conﬂict\\n\\nmonitoring and adjustments in control. Science 303:1023–26\\n\\nKounios J. 1996. On the continuity of thought and the representation of knowledge: Electrophysiological and behavioral time-course measures reveal levels of structure in semantic memory. Psychon. Bull. Rev. 3:265–86\\n\\nKounios J, Beeman M. 2009. The Aha! moment. The cognitive neuroscience of insight. Curr. Dir. Psychol.\\n\\nSci. 18(4):210–16\\n\\nKounios J, Fleck JI, Green DL, Payne L, Stevenson JL, et al. 2008. The origins of insight in resting-state\\n\\nbrain activity. Neuropsychologia 46(1):281–91\\n\\nKouniosJ,FrymiareJL,BowdenEM,FleckJI,SubramaniamK,etal.2006.Thepreparedmind:Neuralactivity prior to problem presentation predicts subsequent solution by sudden insight. Psychol. Sci. 17(10):882–90 Kounios J, Osman AM, Meyer DE. 1987. Structure and process in semantic memory: new evidence based on\\n\\nspeed–accuracy decomposition. J. Exp. Psychol.: Gen. 116:3–25\\n\\nLaneRD,ReimanEM,AxelrodB,YunL,HolmesA.1998.Neuralcorrelatesofemotionalawareness:evidence ofaninteractionbetweenemotionandattentionintheanteriorcingulatecortex.J.Cogn.Neurosci.10:525– 35\\n\\nLudmerR,DudaiY,RubinN.2011.Uncoveringcamouﬂage:Amygdalaactivationpredictslong-termmemory\\n\\nof induced perceptual insight. Neuron 69:1002–14\\n\\nLuo J, Li W, Fink A, Jia L, Xiao X, et al. 2011. The time course of breaking mental sets and forming novel\\n\\nassociations in insight-like problem solving: an ERP investigation. Exp. Brain Res. 212:583–91 Luo J, Niki K. 2003. Function of hippocampus in “insight” of problem solving. Hippocampus 13:316–23 Mashal N, Faust M, Hendler T, Jung-Beeman M. 2008. Hemispheric differences in processing the literal interpretation of idioms: converging evidence from behavioral and fMRI studies. Cortex 44:848–60\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n91\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nMaybergHS,LiottiM,BrannanSK,McGinnisS,MahurinRK,etal.1999.Reciprocallimbic-corticalfunction andnegativemood:convergingPETﬁndingsindepressionandnormalsadness.Am.J.Psychiatry156:675– 82\\n\\nMednick S. 1962. The associative basis of the creative process. Psychol. Rev. 69(3):220–32 Metcalfe J, Wiebe D. 1987. Intuition in insight and noninsight problem solving. Mem. Cogn. 15(3):238–46 Metuki N, Sela T, Lavidor M. 2012. Enhancing cognitive control components of insight problem solving by\\n\\nanodal tDCS of the left dorsolateral prefrontal cortex. Brain Stimul. 5:110–15\\n\\nMeyer DE, Irwin DE, Osman AM, Kounios J. 1988. The dynamics of cognition and action: mental processes\\n\\ninferred from speed-accuracy decomposition. Psychol. Rev. 95:183–237\\n\\nMiller EK, Cohen JD. 2001. An integrative theory of prefrontal cortex function. Annu. Rev. Neurosci. 24:167–\\n\\n202\\n\\nMorgan WW. 1988. A morphological life. Annu. Rev. Astron. Astrophys. 26:1–10 Ray WJ, Cole HW. 1985. EEG alpha activity reﬂects attentional demands, and beta activity reﬂects emotional\\n\\nand cognitive processes. Science 228:750–52\\n\\nReverberiC,ToraldoA,D’AgostiniS,SkrapM.2005.Betterwithout(lateral)frontalcortex?Insightproblems\\n\\nsolved by frontal patients. Brain 128:2882–90\\n\\nRowe G, Hirsch JB, Anderson AK. 2007. Positive affect increases the breadth of attentional selection. Proc.\\n\\nNatl. Acad. Sci. USA 101:383–88\\n\\nSandk¨uhler S, Bhattacharya J. 2008. Deconstructing insight: EEG correlates of insightful problem solving.\\n\\nPLoS ONE 3(1):e1459\\n\\nScheibel AB, Fried I, Paul L, Forsythe A, Tomiyasu U, et al. 1985. Differentiating characteristics of the human speech cortex: a quantitative Golgi study. In The Dual Brain: Hemispheric Specialization in Humans, ed. DF Benson, E Zaidel, pp. 65–74. New York: Guilford\\n\\nSeifert CM, Meyer DE, Davidson N, Patalano AL, Yaniv I. 1995. Demystiﬁcation of cognitive insight: op- portunistic assimilation and the prepared-mind hypothesis. In The Nature of Insight, ed. R Sternberg, JE Davidson, pp. 65–124. Cambridge, MA: MIT Press\\n\\nSeldonHL.1981.Structureofhumanauditorycortex.II.Cytoarchitectonicsanddendriticdistributions.Brain\\n\\nRes. 229:277–94\\n\\nSheehan W. 2008. W. W. Morgan and the discovery of the spiral arm structure of our galaxy. J. Astron. Hist.\\n\\nHerit. 11:3–21\\n\\nShethBR,SandkuhlerS,BhattacharyaJ.2008.Posteriorbetaandanteriorgammaoscillationspredictcognitive\\n\\ninsight. J. Cogn. Neurosci. 21:1269–79\\n\\nSmit DJ, Boomsma DI, Schnack HG, Hulshoff Pol HE, de Geus EJ. 2012. Individual differences in EEG spectralpowerreﬂectgeneticvarianceingrayandwhitemattervolumes.TwinRes.Hum.Genet.15:384–92 Smith RW, Kounios J. 1996. Sudden insight: all-or-none processing revealed by speed–accuracy decomposi-\\n\\ntion. J. Exp. Psychol.: Learn. Mem. Cogn. 22(6):1443–62\\n\\nSt George M, Kutas M, Martinez A, Sereno MI. 1999. Semantic integration in reading: engagement of the\\n\\nright hemisphere during discourse processing. Brain 122:1317–25\\n\\nSternberg RJ, Davidson JE. 1995. The Nature of Insight. Cambridge, MA: MIT Press Subramaniam K, Kounios J, Parrish TB, Jung-Beeman M. 2009. A brain mechanism for facilitation of insight\\n\\nby positive affect. J. Cogn. Neurosci. 21:415–32\\n\\nTallon-BaudryC,BertrandO.1999.Oscillatorygammaactivityinhumansanditsroleinobjectrepresentation.\\n\\nTrends Cogn. Sci. 3:151–62\\n\\nTardif E, Clarke S. 2001. Intrinsic connectivity of human auditory areas: a tracing study with Dil. Eur. J.\\n\\nNeurosci. 13:1045–50\\n\\nTopolinski S, Reber R. 2010. Gaining insight into the “aha” experience. Curr. Dir. Psychol. Sci. 19:402–5 Trope Y, Liberman N. 2010. Construal-level theory of psychological distance. Psychol. Rev. 117:440–63 van Steenburgh J, Fleck JI, Beeman M, Kounios J. 2012. Insight. In The Oxford Handbook of Thinking and\\n\\nReasoning, ed. K Holyoak, R Morrison, pp. 475–91. New York: Oxford Univ. Press\\n\\nVerhaeghen P, Joormann J, Khan R. 2005. Why we sing the blues: the relation between self-reﬂective rumi-\\n\\nnation, mood, and creativity. Emotion 5:226–32\\n\\nVirtue S, Haberman J, Clancy Z, Parrish T, Jung-Beeman M. 2006. Neural activity of inferences during story\\n\\ncomprehension. Brain Res. 1084:104–14\\n\\n92\\n\\nKounios· Beeman\\n\\nPS65CH04-Kounios\\n\\nARI\\n\\n31 October 2013\\n\\n9:37\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nVirtue S, Parrish T, Beeman M. 2008. Inferences during story comprehension: cortical recruitment affected\\n\\nby predictability of events and working memory capacity. J. Cogn. Neurosci. 20:2274–84\\n\\nWallas G. 1926. The Art of Thought. New York: Harcourt Brace Wagner U, Gais S, Haider H, Verleger R, Born J. 2004. Sleep inspires insight. Nature 427(6972):352–55 Wegbreit E, Suzuki S, Grabowecky M, Kounios J, Beeman M. 2012. Visual attention modulates insight versus\\n\\nDescribes a theory of stages leading to insight or “illumination”; immersion in the facts of a problem is the ﬁrst stage.\\n\\nanalytic solving of verbal problems. J. Probl. Solving 4(2):artic. 5\\n\\nWeissman DH, Gopalakrishnan A, Hazlett CJ, Woldorff MG. 2005. Dorsal anterior cingulate cortex resolves\\n\\nconﬂict from distracting stimuli by boosting attention toward relevant events. Cereb. Cortex 15:229–37\\n\\nWu L, Knoblich G, Wei G, Luo J. 2009. How perceptual processes help to generate new meaning: an EEG\\n\\nstudy of chunk decomposition in Chinese characters. Brain Res. 1296:104–12\\n\\nwww.annualreviews.org • The Cognitive Neuroscience of Insight\\n\\n93\\n\\nPS65-FrontMatter\\n\\nARI\\n\\n13 November 2013\\n\\n20:27\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nContents\\n\\nAnnual Review of Psychology\\n\\nPrefatory\\n\\nVolume 65, 2014\\n\\nI Study What I Stink At: Lessons Learned from a Career in Psychology (cid:2) (cid:2) (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nRobert J. Sternberg (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 1\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nStress and Neuroendocrinology\\n\\nOxytocin Pathways and the Evolution of Human Behavior (cid:2) (cid:2) (cid:2) (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nC. Sue Carter (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)17\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nGenetics of Behavior\\n\\nGene-Environment Interaction\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nStephen B. Manuck and Jeanne M. McCaffery (cid:2)\\n\\n(cid:2)41\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nCognitive Neuroscience\\n\\nThe Cognitive Neuroscience of Insight (cid:2) (cid:2) John Kounios and Mark Beeman (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)71\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nColor Perception\\n\\nColor Psychology: Effects of Perceiving Color on Psychological\\n\\nFunctioning in Humans Andrew J. Elliot and Markus A. Maier (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)95\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nInfancy\\n\\nHuman Infancy...and the Rest of the Lifespan (cid:2) (cid:2) (cid:2) (cid:2) (cid:2)\\n\\n(cid:2) 121\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nMarc H. Bornstein (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nAdolescence and Emerging Adulthood\\n\\nBullying in Schools: The Power of Bullies and the Plight of Victims (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nJaana Juvonen and Sandra Graham (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 159\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nIs Adolescence a Sensitive Period for Sociocultural Processing? (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 187\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nSarah-Jayne Blakemore and Kathryn L. Mills\\n\\nAdulthood and Aging\\n\\nPsychological Research on Retirement (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 209\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nMo Wang and Junqi Shi\\n\\nDevelopment in the Family\\n\\nAdoption: Biological and Social Processes Linked to Adaptation (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 235\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nHarold D. Grotevant and Jennifer M. McDermott\\n\\nvi\\n\\nPS65-FrontMatter\\n\\nARI\\n\\n13 November 2013\\n\\n20:27\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nIndividual Treatment\\n\\nCombination Psychotherapy and Antidepressant Medication Treatment\\n\\nfor Depression: For Whom, When, and How W. Edward Craighead and Boadie W. Dunlop (cid:2)\\n\\n(cid:2) 267\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nAdult Clinical Neuropsychology\\n\\nSport and Nonsport Etiologies of Mild Traumatic Brain Injury:\\n\\nSimilarities and Differences Amanda R. Rabinowitz, Xiaoqi Li, and Harvey S. Levin (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 301\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nSelf and Identity\\n\\nThe Psychology of Change: Self-Afﬁrmation and Social\\n\\nPsychological Intervention Geoffrey L. Cohen and David K. Sherman (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 333\\n\\nGender\\n\\nGender Similarities and Differences (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 373\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nJanet Shibley Hyde\\n\\nAltruism and Aggression\\n\\nDehumanization and Infrahumanization (cid:2) (cid:2) Nick Haslam and Steve Loughnan (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 399\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nThe Sociocultural Appraisals, Values, and Emotions (SAVE) Framework\\n\\nof Prosociality: Core Processes from Gene to Meme Dacher Keltner, Aleksandr Kogan, Paul K. Piff, and Sarina R. Saturn (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 425\\n\\nSmall Groups\\n\\nDeviance and Dissent in Groups\\n\\n(cid:2) 461\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nJolanda Jetten and Matthew J. Hornsey (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nSocial Neuroscience\\n\\nCultural Neuroscience: Biology of the Mind in Cultural Contexts (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 487\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nHeejung S. Kim and Joni Y. Sasaki\\n\\nGenes and Personality\\n\\nA Phenotypic Null Hypothesis for the Genetics of Personality (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nEric Turkheimer, Erik Pettersson, and Erin E. Horn (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 515\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nEnvironmental Psychology\\n\\nEnvironmental Psychology Matters (cid:2) (cid:2) (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nRobert Gifford (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 541\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nContents\\n\\nvii\\n\\nPS65-FrontMatter\\n\\nARI\\n\\n13 November 2013\\n\\n20:27\\n\\nby WIB6242 - Universitaets- und Landesbibliothek Duesseldorf on 01/06/14. For personal use only.\\n\\nAnnu. Rev. Psychol. 2014.65:71-93. Downloaded from www.annualreviews.org\\n\\nCommunity Psychology\\n\\nSocioecological Psychology (cid:2)\\n\\n(cid:2) 581\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nShigehiro Oishi\\n\\nSubcultures Within Countries\\n\\nSocial Class Culture Cycles: How Three Gateway Contexts Shape Selves\\n\\nand Fuel Inequality Nicole M. Stephens Hazel Rose Markus, and L. Taylor Phillips\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 611\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nOrganizational Climate/Culture\\n\\n(Un)Ethical Behavior in Organizations\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 635\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nLinda Klebe Trevi˜no, Niki A. den Nieuwenboer, and Jennifer J. Kish-Gephart\\n\\nJob/Work Design\\n\\nBeyond Motivation: Job and Work Design for Development, Health,\\n\\nAmbidexterity, and More (cid:2) (cid:2) Sharon K. Parker (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 661\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nSelection and Placement\\n\\nA Century of Selection\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nAnn Marie Ryan and Robert E. Ployhart (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 693\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nPersonality and Coping Styles\\n\\nPersonality, Well-Being, and Health\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nHoward S. Friedman and Margaret L. Kern (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 719\\n\\nTimely Topics\\n\\nProperties of the Internal Clock: First- and Second-Order Principles of\\n\\nSubjective Time Melissa J. Allman, Sundeep Teki, Timothy D. Grifﬁths, and Warren H. Meck (cid:2)\\n\\n(cid:2) 743\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nIndexes\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 773\\n\\nCumulative Index of Contributing Authors, Volumes 55–65 (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\nCumulative Index of Article Titles, Volumes 55–65 (cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2)\\n\\n(cid:2) 778\\n\\nErrata An online log of corrections to Annual Review of Psychology articles may be found at http://psych.AnnualReviews.org/errata.shtml\\n\\nviii\\n\\nContents'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRVS-MlCLC8E",
        "outputId": "1e5c91d1-d641-41d4-8e08-b5e831a2d21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/Cognitive Neuroscience.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Cognitive Neuroscience.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "print(f\"📄 Number of pages in the first file: {len(doc)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XClshPFwXXt1",
        "outputId": "ecbbf90d-9edb-461c-88bb-cc82434c91c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Number of pages in the first file: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "W5QtKX_xac19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document"
      ],
      "metadata": {
        "id": "nTWCZnIYdfhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_chunks = []\n",
        "for chunk in chunks:\n",
        "    processed_chunks.append(Document(page_content=chunk.page_content, metadata={\"source\": \"source_01\"}))\n",
        "print(f\"✅ Number of chunks in the first file: {len(processed_chunks)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU8XJk5ackAa",
        "outputId": "b0062f17-f5e2-4333-ab20-83411cee9881"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Number of chunks in the first file: 276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF 2**"
      ],
      "metadata": {
        "id": "KIi3_rkogIIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "loader_2 = UnstructuredFileLoader(\"/content/Cognitive computational neuroscience.pdf\")\n",
        "docs_2 = loader_2.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y51An4VacqGY",
        "outputId": "8fe7a354-4bbb-4aa4-bcc1-2aae4c61b835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_2[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "0sTBqzlFn6Vt",
        "outputId": "5de87509-f316-45c8-ad14-69ea0aef15d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Cognitive computational neuroscience\\n\\nNikolaus Kriegeskorte1* and Pamela K. Douglas2\\n\\nTo learn how cognition is implemented in the brain, we must build computational models that can perform cognitive tasks, and test such models with brain and behavioral experiments. Cognitive science has developed computational models that decom- pose cognition into functional components. Computational neuroscience has modeled how interacting neurons can implement elementary components of cognition. It is time to assemble the pieces of the puzzle of brain computation and to better integrate these separate disciplines. Modern technologies enable us to measure and manipulate brain activity in unprecedentedly rich ways in animals and humans. However, experiments will yield theoretical insight only when employed to test brain-computa- tional models. Here we review recent work in the intersection of cognitive science, computational neuroscience and artificial intelligence. Computational models that mimic brain information processing during perceptual, cognitive and control tasks are beginning to be developed and tested with brain and behavioral data.\\n\\nUnderstanding brain information processing requires that we\\n\\nbuild computational models that are capable of performing cognitive tasks. The argument in favor of task-performing computational models was well articulated by Allen Newell in 1973 in his commentary “You can’t play 20 questions with nature and win”1. Newell was criticizing the state of cognitive psychology. The field was in the habit of testing one hypothesis about cognition at a time, in the hope that forcing nature to answer a series of binary questions would eventually reveal the brain’s algorithms. Newell argued that testing verbally defined hypotheses about cognition might never lead to a computational understanding. Hypothesis testing, in his view, needed to be complemented by the construction of comprehensive task-performing computational models. Only synthesis in a computer simulation can reveal what the interac- tion of the proposed component mechanisms actually entails and whether it can account for the cognitive function in question. If we did have a full understanding of an information-processing mecha- nism, then we should be able to engineer it. “What I cannot create, I do not understand,” in the words of physicist Richard Feynman, who left this sentence on his blackboard when he died in 1988.\\n\\nHere we argue that task-performing computational models that explain how cognition arises from neurobiologically plausible dynamic components will be central to a new cognitive computa- tional neuroscience. We first briefly trace the steps of the cognitive and brain sciences and then review several exciting recent develop- ments that suggest that it might be possible to meet the combined ambitions of cognitive science (to explain how humans learn and think)2 and computational neuroscience (to explain how brains adapt and compute)3 using neurobiologically plausible artificial intelligence (AI) models.\\n\\nIn the spirit of Newell’s critique, the transition from cognitive psychology to cognitive science was defined by the introduction of task-performing computational models. Cognitive scientists knew that understanding cognition required AI and brought engineering to cognitive studies. In the 1980s, cognitive science made impor- tant advances with symbolic cognitive architectures4,5 and neural networks6, using human behavioral data to adjudicate between candidate computational models. However, computer hardware and machine learning were not sufficiently advanced to simulate\\n\\ncognitive processes in their full complexity. Moreover, these early developments relied on behavioral data alone and did not leverage constraints provided by the anatomy and activity of the brain.\\n\\nWith the advent of human functional brain imaging, scientists began to relate cognitive theories to the human brain. This endeavor came to be called cognitive neuroscience7. Cognitive neuroscien- tists began by mapping cognitive psychology’s boxes (information- processing modules) and arrows (interactions between modules) onto the brain. This was a step forward in terms of engaging brain activity, but a step back in terms of computational rigor. Methods for testing the task-performing computational models of cogni- tive science with brain-activity data had not been conceived. As a result, cognitive science and cognitive neuroscience parted ways in the 1990s.\\n\\nCognitive psychology’s tasks and theories of high-level func- tional modules provided a reasonable starting point for mapping the coarse-scale organization of the human brain with functional imaging techniques, including electroencephalography, positron emission tomography and early functional magnetic resonance imaging (fMRI), which had low spatial resolution. Inspired by cog- nitive psychology’s notion of the module8, cognitive neuroscience developed its own game of 20 questions with nature. A given study would ask whether a particular cognitive module could be found in the brain. The field mapped an ever increasing array of cogni- tive functions to brain regions, providing a useful rough draft of the global functional layout of the human brain.\\n\\nA brain map, at whatever scale, does not reveal the computa- tional mechanism (Fig. 1). However, mapping does provide con- straints for theory. After all, information exchange incurs costs that scale with the distance between the communicating regions— costs in terms of physical connections, energy and signal latency. Component placement is likely to reflect these costs. We expect regions that need to interact at high bandwidth and short latency to be placed close together9. More generally, the topology and geometry of a biological neural network constrain its dynam- ics, and thus its functional mechanism. Functional localization results, especially in combination with anatomical connectivity, may therefore ultimately prove useful for modeling brain infor- mation processing.\\n\\n1Department of Psychology, Department of Neuroscience, Department of Electrical Engineering, Zuckerman Mind Brain Behavior Institute, Columbia University, New York, NY, USA. 2Center for Cognitive Neuroscience, University of California, Los Angeles, Los Angeles, CA, USA. *e-mail: n.kriegeskorte@columbia.edu\\n\\n1148\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\na\\n\\nb\\n\\n2 C P\\n\\n4 C P\\n\\nPC3\\n\\nPC1\\n\\nPC1\\n\\nPC2\\n\\nPC1\\n\\nSEF\\n\\nSMFA\\n\\nSMHA\\n\\nM1F\\n\\nS1F\\n\\nRSC\\n\\nFEF\\n\\nM1H\\n\\nS1H\\n\\nIPS\\n\\nV7\\n\\n100 μm\\n\\nsPMv M1M\\n\\nFO\\n\\nS1M\\n\\nOPA\\n\\nV3A\\n\\nV3B\\n\\n50\\n\\nBA\\n\\nS2H\\n\\nLO\\n\\n3 C P\\n\\n0\\n\\nGain decrease\\n\\nS2F\\n\\nAC\\n\\nEBA\\n\\nMT+\\n\\nV4\\n\\nV3\\n\\nV2\\n\\nV1\\n\\n−50\\n\\nOFA\\n\\nFFA\\n\\nGain increase\\n\\nSuperior\\n\\nPPA\\n\\n0 PC2\\n\\n200\\n\\n400\\n\\n0 PC1\\n\\nAnterior\\n\\nLeft cortical hemisphere\\n\\nFig. 1 | Modern imaging techniques provide unprecedentedly detailed information about brain activity, but data-driven analyses support only limited insights. a, Two-photon calcium imaging results121 show single-neuron activity for a large population of cells measured simultaneously in larval zebrafish while the animals interact with a virtual environment. b, Human fMRI results70 reveal a detailed map of semantically selective responses while a subject listened to a story. These studies illustrate, on the one hand, the power of modern brain-activity measurement techniques at different scales (a,b) and, on the other, the challenge of drawing insights about brain computation from such datasets. Both studies measured brain activity during complex, time- continuous, naturalistic experience and used principal component analysis (a, bottom; b, top) to provide an overall view of the activity patterns and their representational significance. PC, principal component.\\n\\nDespite methodological challenges10,11, many of the findings of cognitive neuroscience provide a solid basis on which to build. For example, the findings of face-selective regions in the human ventral stream12 have been thoroughly replicated and generalized. Nonhuman primates probed with fMRI exhibit similar face-selec- tive regions, which had evaded explorations with invasive electrodes because the latter do not provide continuous images over large fields of view. Localized with fMRI and probed with invasive electrode recordings, the primate face patches revealed high densities of face- selective neurons13, with invariances emerging at higher stages of hierarchical processing, including mirror-symmetric tuning and view-tolerant representations of individual faces in the anterior- most patch14. The example of face perception illustrates, on one hand, the solid progress in mapping the anatomical substrate and characterizing neuronal responses15 and, on the other, the lack of definitive computational models. The literature does provide clues to the computational mechanism. A brain-computational model of face recognition16 will have to explain the spatial clusters of face- selective units and the selectivities and invariances observed with fMRI17,18 and invasive recordings14,19.\\n\\ncognition, including the way our minds model the physical and social world2. These developments occurred in interaction with statistics and machine learning, where a unified perspective on probabilistic empirical inference has emerged. This literature pro- vides essential computational theory for understanding the brain. In addition, it provides algorithms for approximate inference on generative models that can grow in complexity with the available data—as might be required for real-world intelligence25,26.\\n\\n2. Computational neuroscience has taken a bottom-up approach, demonstrating how dynamic interactions between biological neu- rons can implement computational component functions. In the past two decades, the field developed mathematical models of elementary computational components and their implementation with biologi- cal neurons27,28. These include components for sensory coding29,30, normalization31, working memory32, evidence accumulation and decision mechanisms33–35, and motor control36. Most of these com- ponent functions are computationally simple, but they provide building blocks for cognition. Computational neuroscience has also begun to test complex computational models that can explain high- level sensory and cognitive brain representations37,38.\\n\\nCognitive neuroscience has mapped the global functional layout of the human and nonhuman primate brain20. However, it has not achieved a full computational account of brain information process- ing. The challenge ahead is to build computational models of brain information processing that are consistent with brain structure and function and perform complex cognitive tasks. The following recent developments in cognitive science, computational neuroscience and artificial intelligence suggest that this may be achievable.\\n\\n1. Cognitive science has proceeded from the top down, decom- posing complex cognitive processes into their computational com- ponents. Unencumbered by the need to make sense of brain data, it has developed task-performing computational models at the cogni- tive level. One success story is that of Bayesian cognitive models, which optimally combine prior knowledge about the world with sensory evidence21–23. Initially applied to basic sensory and motor processes23,24, Bayesian models have begun to engage complex\\n\\n3. Artificial intelligence has shown how component functions can be combined to create intelligent behavior. Early AI failed to live up to its promise because the rich world knowledge required for feats of intelligence could not be either engineered or automat- ically learned. Recent advances in machine learning, boosted by growing computational power and larger datasets from which to learn, have brought progress at perceptual39, cognitive40 and con- trol challenges41. Many advances were driven by cognitive-level symbolic models. Some of the most important recent advances are driven by deep neural network models, composed of units that compute linear combinations of their inputs, followed by static nonlinearities42. These models employ only a small subset of the dynamic capabilities of biological neurons, abstracting from fun- damental features such as action potentials. However, their func- tionality is inspired by brains and could be implemented with biological neurons. 1149\\n\\n3. Artificial intelligence has shown how component functions can be combined to create intelligent behavior. Early AI failed to live up to its promise because the rich world knowledge required for feats of intelligence could not be either engineered or automat- ically learned. Recent advances in machine learning, boosted by growing computational power and larger datasets from which to learn, have brought progress at perceptual39, cognitive40 and con- trol challenges41. Many advances were driven by cognitive-level symbolic models. Some of the most important recent advances are driven by deep neural network models, composed of units that compute linear combinations of their inputs, followed by static nonlinearities42. These models employ only a small subset of the dynamic capabilities of biological neurons, abstracting from fun- damental features such as action potentials. However, their func- tionality is inspired by brains and could be implemented with biological neurons. 1149\\n\\nThe three disciplines contribute complementary elements to biologically plausible computational models that perform cognitive tasks and explain brain information processing and behavior (Fig. 2). Here we review the first steps in the literature toward a cognitive computational neuroscience that meets the combined criteria for success of cognitive science (computational models that perform cognitive tasks and explain behavior) and computational neurosci- ence (neurobiologically plausible mechanistic models that explain brain activity). If computational models are to explain animal and human cognition, they will have to perform feats of intelligence. AI, and in particular machine learning, is therefore a key discipline that provides the theoretical and technological foundation for cognitive computational neuroscience.\\n\\nComputational neuroscience\\n\\nExplain neuronal activity patterns\\n\\nusing biologically plausible...\\n\\nCognitive science\\n\\nExplain behavioral data\\n\\n…computational models…\\n\\nThe overarching challenge is to build solid bridges between theory (instantiated in task-performing computational models) and experiment (providing brain and behavioral data). The first part of this review describes bottom-up developments that begin with experimental data and attempt to build bridges from the data in the direction of theory43. Given brain-activity data, connectiv- ity models aim to reveal the large-scale dynamics of brain acti- vation; decoding and encoding models aim to reveal the content and format of brain representations. The models employed in this literature provide constraints for computational theory, but they do not in general perform the cognitive tasks in question and thus fall short of explaining the computational mechanism underlying task performance.\\n\\n...that perform complex cognitive tasks\\n\\nArtificial intelligence\\n\\nFig. 2 | What does it mean to understand how the brain works? The goal of cognitive computational neuroscience is to explain rich measurements of neuronal activity and behavior in animals and humans by means of biologically plausible computational models that perform real-world cognitive tasks. Historically, each of the disciplines (circles) has tackled a subset of these challenges (white labels). Cognitive computational neuroscience strives to meet all the challenges simultaneously.\\n\\nThe second part of this article describes developments that proceed in the opposite direction, building bridges from theory to experiment37,38,44. We review emerging work that has begun to test task-performing computational models with brain and behavioral data. The models include cognitive models, specified at an abstract computational level, whose implementation in biological brains has yet to be explained, and neural network models, which abstract from many features of neurobiology, but could plausibly be imple- mented with biological neurons. This emerging literature suggests the beginnings of an integrative approach to understanding brain computation, where models are required to perform cognitive tasks, biology provides the admissible component functions, and the com- putational mechanisms are optimized to explain detailed patterns of brain activity and behavior.\\n\\ndynamics, delays, indirect interactions and noise into account49. From local neuronal interactions to large-scale spatiotemporal pat- terns spanning cortex and subcortical regions, generative models of spontaneous dynamics can be evaluated with brain-activity data. Effective connectivity analyses take a more hypothesis-driven approach, characterizing the interactions among a small set of regions on the basis of generative models of the dynamics50. Whereas activation mapping maps the boxes of cognitive psychology onto brain regions, effective connectivity analyses map the arrows onto pairs of brain regions. Most work in this area has focused on char- acterizing interactions at the level of the overall activation of a brain region. Like the classical brain mapping approach, these analyses are based on regional-mean activation, measuring correlated fluc- tuations of overall regional activation rather than the information exchanged between regions.\\n\\nFrom experiment toward theory Models of connectivity and dynamics. One path from measured brain activity toward a computational understanding is to model the brain’s connectivity and dynamics. Connectivity models go beyond the localization of activated regions and characterize the interactions between regions. Neuronal dynamics can be mea- sured and modeled at multiple scales, from local sets of interact- ing neurons to whole-brain activity45. A first approximation of brain dynamics is provided by the correlation matrix among the measured response time series, which characterizes the pairwise ‘functional connectivity’ between locations. The literature on rest- ing-state networks has explored this approach46, and linear decom- positions of the space-time matrix, such as spatial independent component analysis, similarly capture simultaneous correlations between locations across time47.\\n\\nBy thresholding the correlation matrix, the set of regions can be converted into an undirected graph and studied with graph- theoretic methods. Such analyses can reveal ‘communities’ (sets of strongly interconnected regions), ‘hubs’ (regions connected to many others) and ‘rich clubs’ (communities of hubs)48. Connectivity graphs can be derived from either anatomical or functional mea- surements. The anatomical connectivity matrix typically resembles the functional connectivity matrix because regions interact through anatomical pathways. However, the way anatomical connectivity generates functional connectivity is better modeled by taking local\\n\\nAnalyses of effective connectivity and large-scale brain dynam- ics go beyond generic statistical models such as the linear models used in activation and information-based brain mapping in that they are generative models: they can generate data at the level of the measurements and are models of brain dynamics. However, they do not capture the represented information and how it is processed in the brain.\\n\\nDecoding models. Another path toward understanding the brain’s computational mechanisms is to reveal what information is present in each brain region. Decoding can help us go beyond the notion of activation, which indicates the involvement of a region in a task, and reveal the information present in a region’s population activ- ity. When particular content is decodable from activity in a brain region, this indicates the presence of the information. To refer to the brain region as ‘representing’ the content adds a functional inter- pretation51: that the information serves the purpose of informing regions receiving these signals about the content. Ultimately, this interpretation needs to be substantiated by further analyses of how the information affects other regions and behavior52–54.\\n\\nDecoding has its roots in the neuronal-recording literature27 and has become a popular tool for studying the content of representa- tions in neuroimaging55–59. In the simplest case, decoding reveals which of two stimuli gave rise to a measured response pattern.\\n\\n1150\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\nBox 1 | the many meanings of “model”\\n\\nThe word “model” has many meanings in the brain and behavio- ral sciences. Data-analysis models are generic statistical models that help establish relationships between measured variables. Examples include linear correlation, univariate multiple lin- ear regression for brain mapping, and linear decoding analysis. Effective connectivity and causal-interaction models are, simi- larly, data-analysis models. They help us infer causal influences and interactions between brain regions. Data-analysis models can serve the purpose of testing hypotheses about relationships among variables (for example, correlation, information, causal influence). They are not models of brain information process- ing. A box-and-arrow model, by contrast, is an information- processing model in the form of labeled boxes that represent cognitive component functions and arrows that represent in- formation flow. In cognitive psychology, such models provided useful, albeit ill-defined, sketches for theories of brain compu- tation. A word model, similarly, is a sketch for a theory about brain information processing that is defined vaguely by a verbal description. While these are models of information processing, they do not perform the information processing thought to oc- cur in the brain. An oracle model is a model of brain responses (often instantiated in a data-analysis model) that relies on infor- mation not available to the animal whose brain is being mod- eled. For example, a model of ventral temporal visual responses as a function of an abstract shape description, or as a function of category labels or continuous semantic features, constitutes an oracle model if the model is not capable of computing the shape, category or semantic features from images. An oracle model may provide a useful characterization of the informa- tion present in a region and its representational format, without specifying any theory as to how the representation is computed by the brain. A brain-computational model (BCM), by contrast, is a model that mimics the brain information processing un- derlying the performance of some task at some level of abstrac- tion. In visual neuroscience, for example, an image-computable\\n\\nmodel is a BCM of visual processing that takes image bitmaps as inputs and predicts brain activity and/or behavioral responses. Deep neural nets provide image-computable models of visual processing. However, deep neural nets trained by supervision rely on category-labeled images for training. Because labeled examples are not available (in comparable quantities) during biological development and learning, these models are BCMs of visual processing, but they are not BCMs of development and learning. Reinforcement learning models use environmental feedback that is more realistic in quality and can provide BCMs of learning processes. A sensory encoding model is a BCM of the computations that transform sensory input to some stage of internal representation. An internal-transformation model is a BCM of the transformation of representations between two stages of processing. A behavioral decoding model is a BCM of the transformation from some internal representation to a be- havioral output. Note that the label BCM indicates merely that the model is intended to capture brain computations at some level of abstraction. A BCM may abstract from biological detail to an arbitrary degree, but must predict some aspect of brain activity and/or behavior. Psychophysical models that predict be- havioral outputs from sensory input and cognitive models that perform cognitive tasks are BCMs formulated at a high level of description. The label BCM does not imply that the model is either plausible or consistent with empirical data. Progress is made by rejecting candidate BCMs on empirical grounds. Like microscale biophysical models, which capture biological pro- cesses that underlie brain computations, and macroscale brain- dynamical and causal-interaction models, BCMs are models of processes occurring in the brain. However, unlike the other types of process model, BCMs perform the information pro- cessing that is thought to be the function of brain dynamics. Finally, the term “model” is used to refer to models of the world employed by the brain, as in model-based reinforcement learning and model-based cognition.\\n\\nThe content of the representation can be the identity of a sensory stimulus (to be recognized among a set of alternative stimuli), a stimulus property (such as the orientation of a grating), an abstract variable needed for a cognitive operation, or an action60. When the decoder is linear, as is usually the case, the decodable information is in a format that can plausibly be read out by downstream neu- rons in a single step. Such information is said to be ‘explicit’ in the activity patterns61.\\n\\nThree types of representational model analysis have been intro- duced in the literature: encoding models63–65, pattern component models66 and representational similarity analysis57,67,68. These three methods all test hypotheses about the representational space, which are based on multivariate descriptions of the experimental condi- tions—for example, a semantic description of a set of stimuli, or the activity patterns across a layer of a neural network model that processes the stimuli52.\\n\\nDecoding and other types of multivariate pattern analysis have helped reveal the content of regional representations55,56,58,59, pro- viding evidence that brain-computational models must incorpo- rate. However, the ability to decode particular information does not amount to a full account of the neuronal code: it doesn’t specify the representational format (beyond linear decodability) or what other information might additionally be present. Most importantly, decoders do not in general constitute models of brain computation. They reveal aspects of the product, but not the pro- cess of brain computation.\\n\\nRepresentational models. Beyond decoding, we would like to exhaustively characterize a region’s representation, explaining its responses to arbitrary stimuli. A full characterization would also define to what extent any variable can be decoded. Representational models attempt to make comprehensive predictions about the rep- resentational space and therefore provide stronger constraints on the computational mechanism than decoding models52,62.\\n\\nIn encoding models, each voxel’s activity profile across stimuli is predicted as a linear combination of the features of the model. In pattern component models, the distribution of the activity profiles that characterizes the representational space is modeled as a multivariate normal distribution. In representational simi- larity analysis, the representational space is characterized by the representational dissimilarities of the activity patterns elicited by the stimuli.\\n\\nRepresentational models are often defined on the basis of descriptions of the stimuli, such as labels provided by human observers63,69,70. In this scenario, a representational model that explains the brain responses in a given region provides, not a brain- computational account, but at least a descriptive account of the rep- resentation. Such an account can be a useful stepping-stone toward computational theory when the model generalizes to novel stimuli. Importantly, representational models also enable us to adjudicate among brain-computational models, an approach we will return to in the next section.\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n1151\\n\\ny t i l\\n\\ne d i f\\n\\ne v i t i n g o C\\n\\nTop-down\\n\\nCognitive models\\n\\nDeep neural networks\\n\\nRecurrent DNN\\n\\nDCNN\\n\\nBottom-up\\n\\nGabor-wavelet V1 model\\n\\nSpaun\\n\\nMulti-compartment single-neuron model\\n\\nHodgkin & Huxley model\\n\\nBlue Brain Project\\n\\nModel complexity\\n\\nFrom theory to experiment To build a better bridge between experiment and theory, we first need to fully specify a theory. This can be achieved by defining the theory mathematically and implementing it in a computa- tional model (Box 1). Computational models can reside at different levels of description, trading off cognitive fidelity against bio- logical fidelity (Fig. 3). Models designed to capture only neuronal components and dynamics71 tend to be unsuccessful at explaining cognitive function72 (Fig. 3, horizontal axis). Conversely, models designed to capture only cognitive functions are difficult to relate to the brain (Fig. 3, vertical axis). To link mind and brain, models must attempt to capture aspects of both behavior and neuronal dynam- ics. Recent advances suggest that constraints from the brain can help explain cognitive function42,73,74 and vice versa37,38, turning the tradeoff into a synergy.\\n\\nBiological fidelity\\n\\nFig. 3 | the space of process models. Models of the processes taking place in the brain can be defined at different levels of description and can vary in their parametric complexity (dot size) and in their biological (horizontal axis) and cognitive (vertical axis) fidelity. Theoreticians approach modeling with a range of primary goals. The bottom-up approach to modeling (blue arrow) aims first to capture characteristics of biological neural networks, such as action potentials and interactions among multiple compartments of single neurons. This approach disregards cognitive function so as to focus on understanding the emergent dynamics of small parts of the brain, such as cortical columns and areas, and to reproduce biological network phenomena, such as oscillations. The top-down approach (red arrow) aims first to capture cognitive functions at the algorithmic level. This approach disregards the biological implementation so as to focus on decomposing the information processing underlying task performance into its algorithmic components. The two approaches form the extremes of a continuum of paths toward the common goal of explaining how our brains give rise to our minds. Overall, there is tradeoff (negative correlation) between cognitive and biological fidelity. However, the tradeoff can turn into a synergy (positive correlation) when cognitive constraints illuminate biological function and when biology inspires models that explain cognitive feats. Because intelligence requires rich world knowledge, models of human brain information processing will have high parametric complexity (large dot in the upper right corner). Even if models that abstract from biological details can explain task performance, biologically detailed models will still be needed to explain the neurobiological implementation. This diagram is a conceptual cartoon that can help us understand the relationships between models and appreciate their complementary contributions. However, it is not based on quantitative measures of cognitive fidelity, biological fidelity and model complexity. Definitive ways to measure each of the three variables have yet to be developed. Figure inspired by ref. 122.\\n\\nIn this section, we focus on recent successes with task-perform- ing models that explain cognitive functions in terms of representa- tions and algorithms. Task-performing models have been central to psychophysics and cognitive science, where they are traditionally tested with behavioral data. An emerging literature is beginning to test task-performing models with brain-activity data as well. We will consider two broad classes of model in turn, neural network models and cognitive models.\\n\\nNeural network models. Neural network models (Box 2) have a long history, with interwoven strands in multiple disciplines. In com- putational neuroscience, neural network models, at various levels of biological detail, have been essential to understanding dynamics in biological neural networks and elementary computational func- tions27,28. In cognitive science, they defined a new paradigm for under- standing cognitive functions, called parallel distributed processing, in the 1980s6,75, which brought the field closer to neuroscience. In AI, they have recently brought substantial advances in a number of applications42,74, ranging from perceptual tasks (such as vision and speech recognition) to symbolic processing challenges (such as lan- guage translation), and on to motor tasks (including speech synthesis and robotic control). Neural network models provide a common lan- guage for building task-performing models that meet the combined criteria for success of the three disciplines (Fig. 2).\\n\\nLike brains, neural network models can perform feedforward as well as recurrent computations37,76. The models driving the recent advances are deep in the sense that they comprise multiple stages of linear-nonlinear signal transformation. Models typically have mil- lions of parameters (the connection weights), which are set so as to optimize task performance. One successful paradigm is super- vised learning, wherein a desired mapping from inputs to outputs is learned from a training set of inputs (for example, images) and asso- ciated outputs (for example, category labels). However, neural net- work models can also be trained without supervision and can learn complex statistical structure inherent to their experiential data.\\n\\nIn this section, we considered three types of model that can help us glean computational insight from brain-activity data. Connectivity models capture aspects of the dynamic interactions between regions. Decoding models enable us to look into brain regions and reveal what might be their representational content. Representational models enable us to test explicit hypotheses that fully characterize a region’s representational space. All three types of model can be used to address theoretically motivated questions—taking a hypothesis-driven approach. However, in the absence of task-performing computational models, they are subject to Newell’s argument that asking a series of questions might never reveal the computational mechanism underlying the cognitive feat we are trying to explain. These methods fall short of building the bridge all the way to theory because they do not test mechanistic models that specify precisely how the information processing underlying some cognitive function might work.\\n\\nThe large number of parameters creates unease among research- ers who are used to simple models with small numbers of inter- pretable parameters. However, simple models will never enable us to explain complex feats of intelligence. The history of AI has shown that intelligence requires ample world knowledge and suf- ficient parametric complexity to store it. We therefore must engage complex models (Fig. 3) and the challenges they pose. One chal- lenge is that the high parameter count renders the models difficult to understand. Because the models are entirely transparent, they can be probed cheaply with millions of input patterns to under- stand the internal representations, an approach sometimes called ‘synthetic neurophysiology’. To address the concern of overfitting, models are evaluated in terms of their generalization performance. A vision model, for example, will be evaluated in terms of its ability to predict neural activity and behavioral responses for images it has not been trained on.\\n\\n1152\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\nBox 2 | Neural network models\\n\\nThe term “neural network model” has come to be associated with a class of model that is inspired by biological neural networks in that each unit combines many inputs and information is pro- cessed in parallel through a network. In contrast to biologically detailed models, which may capture action potentials and dy- namics in multiple compartments of each neuron, these models abstract from the biological details. However, they can explain certain cognitive functions, such as visual object recognition, and therefore provide an attractive framework for linking cogni- tion to the brain.\\n\\nA typical unit computes a linear combination of its inputs and passes the result through a static nonlinearity. The output is sometimes interpreted as analogous to the firing rate of a neuron. Even shallow networks (those with a single layer of hidden units between inputs and outputs) can approximate arbitrary functions123. However, deep networks (those with multiple hidden layers) can more efficiently capture many of the complex functions needed in real-world tasks. Many applications—for example, in computer vision—use feedforward architectures. However, recurrent neural networks, which reprocess the outputs of their units and generate complex dynamics, have brought additional engineering advances76 and better capture the recurrent signaling in brains35,124–126. Whereas feedforward networks are universal function approximators, recurrent networks are universal approximators of dynamical systems127. Recurrent processing enables a network to recycle its limited computational resources through time so as to perform more complex sequences of computations. Recurrent networks can represent the recent stimulus history in a dynamically compressed format, providing the temporal context information needed for current processing. As a result, recurrent networks can recognize, predict, and generate dynamical patterns.\\n\\nBoth feedforward and recurrent networks are defined by their architecture and the setting of the connection weights. One way to set the weights is through iterative small adjustments that bring the output closer to some desired output (supervised\\n\\nlearning). Each weight is adjusted in proportion to the reduction in the error that a small change to it would yield. This method is called gradient descent because it produces steps in the space of weights along which the error declines most steeply. Gradient descent can be implemented using backpropagation, an efficient algorithm for computing the derivative of the error function with respect to each weight.\\n\\nWhether the brain uses an algorithm like backpropagation for learning is controversial. Several biologically plausible implementations of backpropagation or closely related forms of supervised learning have been suggested128–130. Supervision signals might be generated internally131 on the basis of the context provided by multiple sensory modalities; on the basis of the dynamic refinement of representations over time, as more evidence becomes available from the senses and from memory132; and on the basis of internal and external reinforcement signals arising in interaction with the environment133. Reinforcement learning41 and unsupervised learning of neural network parameters119,134 are areas of rapid current progress.\\n\\nNeural network models have demonstrated that taking inspiration from biology can yield breakthroughs in AI. It seems likely that the quest for models that can match human cognitive abilities will draw us deeper into the biology135. The abstract neural network models currently most successful in engineering could be implemented with biological hardware. However, they only use a small subset of the dynamical components of brains. Neuroscience has described a rich repertoire of dynamical components, including action potentials108, canonical microcircuits136, dendritic dynamics128,130,137 and network pheno- mena27, such as oscillations138, that may have computational functions. Biology also provides constraints on the global architecture, suggesting, for example, complementary subsystems for learning139. Modeling these biological components in the context of neural networks designed to perform meaningful tasks may reveal how they contribute to brain computation and may drive further advances in AI.\\n\\nSeveral recent studies have begun to test neural network mod- els as models of brain information processing37,38. These studies predicted brain representations of novel images in the primate ventral visual stream with deep convolutional neural network models trained to recognize objects in images. Results have shown that the internal representations of deep convolutional neural net- works provide the best current models of representations of visual images in inferior temporal cortex in humans and monkeys77–79. When comparing large numbers of models, those that were opti- mized to perform the task of object classification better explained the cortical representation77,78.\\n\\nEarly layers of deep neural networks trained to recognize objects contain representations resembling those in early visual cortex78,80. As we move along the ventral visual stream, higher layers of the neural networks come to provide a better basis for explaining the representations80–82. Higher layers of deep convolutional neural networks also resemble the inferior temporal cortical representa- tion in that both enable the decoding of object position, size and pose, along with the category of the object83. In addition to testing these models by predicting brain-activity data, the field has begun to test them by predicting behavioral responses reflecting perceived shape84 and object similarity85.\\n\\nCognitive models. Models at the cognitive level enable research- ers to envision the information processing without simultaneously\\n\\nhaving to tackle its implementation with neurobiologically plausible components. This enables progress on domains of higher cognition, where neural network models still fall short. Moreover, a cognitive model may provide a useful abstraction, even when a process can also be captured with a neural network model.\\n\\nNeuroscientific explanations now dominate for functional com- ponents closer to the periphery of the brain, where sensory and motor processes connect the animal to its environment. However, much of higher-level cognition has remained beyond the reach of neuroscientific accounts and neural network models. To illustrate some of the unique contributions of cognitive models, we briefly discuss three classes of cognitive model: production systems, rein- forcement learning models and Bayesian cognitive models.\\n\\nProduction systems provide an early example of a class of cogni- tive models that can explain reasoning and problem solving. These models use rules and logic, and are symbolic in that they operate on symbols rather than sensory data and motor signals. They capture cognition, rather than perception and motor control, which ground cognition in the physical environment. A ‘production’ is a cogni- tive action triggered according to an if-then rule. A set of such rules specifies the conditions (‘if’) under which each of a range of produc- tions (‘then’) is to be executed. The conditions refer to current goals and knowledge in memory. The actions can modify the internal state of goals and knowledge. For example, a production may create a subgoal or store an inference. If conditions are met for multiple\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n1153\\n\\nBox 3 | Bayesian cognitive models\\n\\nBayesian cognitive models are motivated by the assumption that the brain approximates the statistically optimal solution to a task. The statistically optimal way to make inferences and decide what to do is to interpret the current sensory evidence in light of all available prior knowledge using the rules of probability. Consider the case of visual perception. The retinal signals reflect the objects in the world, which we would like to recognize. To infer the objects, we should consider what configurations of objects we deem possible and how well each explains the image. Our prior beliefs are repre- sented by a generative model that captures the probability of each configuration of objects and the probabilities with which a given configuration would produce different retinal images.\\n\\nMore formally, a Bayesian model of vision might use a generative model of the joint distribution p(d, c) of the sensory data d (the image) and the causes in the world c (the configuration of surfaces, objects and light sources to be inferred)140. The joint distribution p(d, c) equals the product of the prior, p(c), over all possible configurations of causes and the likelihood, p(d|c), the probability of a particular image given a particular configuration of causes. A prescribed model for p(d|c) would enable us to evaluate the likelihood, the probability of a specific image d given specific causes c. Alternatively, we might have an implicit model for p(d|c) in the form of a stochastic mapping from causes c to data d (images). Such a model would generate natural images. Whether prescribed or implicit, the model of p(d|c) captures how the causes in the world create the image, or at least how they relate to the image. Visual recognition amounts to computing the posterior p(c|d), the probability distribution over the causes given a particular image. The posterior p(c|d) reveals the causes c as they would have to exist in the world to explain the sensory data d141. A model computing p(c|d) is called a discriminative model because it discriminates among images—here mapping from effects (the image) to the causes. The inversion mathematically requires a prior p(c) over the latent causes. The prior p(c) can constrain the interpretation and help\\n\\nreduce the ambiguity resulting from the multiple configurations of causes that can account for any image.\\n\\nBasing the inference of the causes c on a generative model of p(d, c) that captures all available knowledge and uncertainty is statistically optimal (i.e., it provides the best inferences given limited data), but computationally challenging (i.e., it may require more neurons or time than the animal can use). Ideally, the generative model p(d, c) implicit to the inference p(c|d) should capture our knowledge not just about image formation, but also the things in the world and their interactions, and our uncertainties about these processes. One challenge is to learn a generative model from sensory data. We need to represent the learned knowledge and the remaining uncertainties. If the generative model is mis-specified, then the inference will not be optimal. For real-world tasks, some degree of misspecification of the model is inevitable. For example, the generative model may contain an overly simplified version of the image-generation process. Another challenge is the computation of the posterior p(c|d). For realistically complex generative models, the inference may require computationally intensive iterative algorithms such as Markov chain Monte Carlo, belief propagation or variational inference. The brain’s compromise between statistical and computational efficiency142–144 may learning fast feedforward recognition models that speed up frequent component inferences, crystallizing conclusions that are costly to fluidly derive with iterative algorithms. This is known as amortized inference145,146.\\n\\ninvolve\\n\\nBayesian cognitive models have recently flourished in interaction with machine learning and statistics. Early work used generative models with a fixed structure that were flexible only with respect to a limited set of parameters. Modern generative models can grow in complexity with the data and discover their inherent structure98. They are called nonparametric because they are not limited by a predefined finite set of parameters147. Their parameters can grow in number without any predefined bound.\\n\\nrules, a conflict-resolution mechanism chooses one production. A model specified using this formalism will generate a sequence of productions, which may to some extent resemble our conscious stream of thought while working toward some cognitive goal. The formalism of production systems also provides a universal compu- tational architecture86. Production systems such as ACT-R5 were originally developed under the guidance of behavioral data. More recently such models have also begun to be tested in terms of their ability to predict regional-mean fMRI activation time courses87.\\n\\nReinforcement learning models capture how an agent can learn to maximize its long-term cumulative reward through interaction with its environment88,89. As in production systems, reinforcement learning models often assume that the agent has perception and motor modules that enable the use of discrete symbolic represen- tations of states and actions. The agent chooses actions, observes resulting states of the environment, receives rewards along the way and learns to improve its behavior. The agent may learn a ‘value function’ associating each state with its expected cumulative reward. If the agent can predict which state each action leads to and if it knows the values of those states, then it can choose the most prom- ising action. The agent may also learn a ‘policy’ that associates each state directly with promising actions. The choice of action must bal- ance exploitation (which brings short-term reward) and exploration (which benefits learning and brings long-term reward).\\n\\nThe field of reinforcement learning explores algorithms that define how to act and learn so as to maximize cumulative reward.\\n\\nWith roots in psychology and neuroscience, reinforcement learn- ing theory is now an important field of machine learning and AI. It provides a very general perspective on control that includes the clas- sical techniques dynamic programming, Monte Carlo and exhaus- tive search as limiting cases, and can handle challenging scenarios in which the environment is stochastic and only partially observed, and its causal mechanisms are unknown.\\n\\nAn agent might exhaustively explore an environment and learn the most promising action to take in any state by trial and error (model-free control). This would require sufficient time to learn, enough memory, and an environment that does not kill the agent prematurely. Biological organisms, however, have limited time to learn and limited memory, and must avoid interactions that might kill them. Under these conditions, an agent might do better to build a model of its environment. A model can compress and generalize experience to enable intelligent action in novel situations (model- based control). Model-free methods are computationally efficient (mapping from states to values or directly to actions), but statis- tically inefficient (learning takes long); model-based methods are more statistically efficient, but may require prohibitive amounts of computation (to simulate possible futures)90.\\n\\nUntil experience is sufficient to build a reliable model, an agent might do best to simply store episodes and revert to paths of action that have met with success in the past (episodic con- trol)91,92. Storing episodes preserves sequential dependency information important for model building. Moreover, episodic\\n\\n1154\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\nBox 4 | Why do cognitive science, computational neuroscience and ai need one another?\\n\\nCognitive science needs computational neuroscience, not merely to explain the implementation of cognitive models in the brain, but also to discover the algorithms. For example, the dominant models of sensory processing and object recognition are brain-inspired neural networks, whose computations are not easily captured at a cognitive level. Recent successes with Bayesian nonparametric models do not yet in general scale to real-world cognition. Explaining the computational efficiency of human cognition and predicting detailed cognitive dynamics and behavior could benefit from studying brain-activity dynam- ics. Explaining behavior is essential, but behavioral data alone provide insufficient constraints for complex models. Brain data can provide rich constraints for cognitive algorithms if leveraged appropriately. Cognitive science has always progressed in close interaction with artificial intelligence. The disciplines share the goal of building task-performing models and thus rely on com- mon mathematical theory and programming environments. Computational neuroscience needs cognitive\\n\\nscience to challenge it to engage higher-level cognition. At the experimental level, the tasks of cognitive science enable computational neuroscience to bring cognition into the lab. At the level of theory, cognitive science challenges computational neuroscience to explain how the neurobiological dynamical components it studies contribute to cognition and behavior. Computational neuroscience needs AI, and in particular machine learning, to provide the theoretical and technological\\n\\nbasis for modeling cognitive functions with biologically plausible dynamical components.\\n\\nArtificial intelligence needs cognitive science to guide the engineering of intelligence. Cognitive science’s tasks can serve as benchmarks for AI systems, building up from elementary cognitive abilities to artificial general intelligence. The literatures on human development and learning provide an essential guide to what is possible for a learner to achieve and what kinds of interaction with the world can support the acquisition intelligence. AI needs computational neuroscience for of algorithmic inspiration. Neural network models are an example of a brain-inspired technology that is unrivalled in several domains of AI. Taking further inspiration from the neurobiological dynamical components (for example, spiking neurons, dendritic dynamics, the canonical cortical microcircuit, oscillations, neuromodulatory processes) and the global functional layout of the human brain (for example, subsystems specialized for distinct functions, including sensory modalities, memory, planning and motor control) might lead to further AI breakthroughs. Machine learning draws from separate traditions in statistics and computer science, which have optimized statistical and computational efficiency, respectively. The integration of computational and statistical efficiency is an essential challenge in the age of big data. The brain appears to combine computational and statistical efficiency, and understanding its algorithm might boost machine learning.\\n\\ncontrol enables the agent to exploit such dependencies even before understanding the causal mechanism supporting a suc- cessful path of action.\\n\\nThe brain is capable of each of these three modes of control (model-free, model-based, episodic)89 and appears to combine their advantages using an algorithm that has yet to be discovered. AI and computational neuroscience share the goal of discovering this algorithm41,90,93–95, although they approach this goal from dif- ferent angles. This is an example of how a cognitive challenge can motivate the development of formal models and drive progress in AI and neuroscience.\\n\\nA third, and critically important, class of cognitive model is that of Bayesian models (Box 3)21,96–98. Bayesian inference provides an essential normative perspective on cognition. It tells us what a brain should in fact compute for an animal to behave optimally. Perceptual inference, for example, should consider the current sen- sory data in the context of prior beliefs. Bayesian inference simply refers to combining the data with prior beliefs according to the rules of probability.\\n\\nBayesian models have contributed to our understanding of basic sensory and motor processes22–24. They have also provided insights into higher cognitive processes of judgment and decision mak- ing, explaining classical cognitive biases99 as the product of prior assumptions, which may be incorrect in the experimental task but correct and helpful in the real world.\\n\\nmodels in the probabilistic sense, but may be causal and composi- tional, supporting mental simulations of processes in the world using elements that can be re-composed to generalize to novel and hypo- thetical scenarios2,98,101. This modeling approach has been applied to our reasoning about the physical101–103 and even the social104 world.\\n\\nGenerative models are an essential ingredient of general intel- ligence. An agent attempting to learn a generative model strives to understand all relationships among its experiences. It does not require external supervision or reinforcement to learn, but can mine all its experiences for insights on its environment and itself. In particular, causal models of processes in the world (how objects cause images, how the present causes the future) can give an agent a deeper understanding and thus a better basis for infer- ences and actions.\\n\\nThe representation of probability distributions in neuronal pop- ulations has been explored theoretically and experimentally105,106. However, relating Bayesian inference and learning, especially structure learning in nonparametric models, to its implementation in the brain remains challenging107. As theories of brain compu- tation, approximate inference algorithms such as sampling may explain cortical feedback signals and activity correlations97,108–110. Moreover, the corners cut by the brain for computational effi- ciency, the approximations, may explain human deviations from statistical optimality. In particular, cognitive experiments have revealed signatures of sampling111 and amortized inference112 in human behavior.\\n\\nWith Bayesian nonparametric models, cognitive science has begun to explain more complex cognitive abilities. Consider the human ability to induce a new object category from a single example. Such inductive inference requires prior knowledge of a kind not captured by current feedforward neural network models100. To induce a cat- egory, we rely on an understanding of the object, of the interactions among its parts, of how they give rise to its function. In the Bayesian cognitive perspective, the human mind, from infancy, builds men- tal models of the world2. These models may not only be generative\\n\\nCognitive models, including the three classes highlighted here, decompose cognition into meaningful functional components. By declaring their models independent of the implementation in the brain, cognitive scientists are able to address high-level cognitive processes21,97,98 that are beyond the reach of current neural networks. Cognitive models are essential for cognitive computational neuro- science because they enable us to see the whole as we attempt to understand the roles of the parts.\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n1155\\n\\nBox 5 | shareable tasks, data, models and tests: a new culture of multidisciplinary collaboration\\n\\nNeurobiologically plausible models that explain cognition will have substantial parametric complexity. Building and evaluat- ing such models will require machine learning and big brain and behavioral datasets. Traditionally, each lab has developed its own tasks, datasets, models and tests with a focus on the goals of its own discipline. To scale these efforts up to meet the chal- lenge, we will need to develop tasks, data, models and tests that are relevant across the three disciplines and shared among labs (see figure). A new culture of collaboration will assemble big data and big models by combining components from different labs. To meet the conjoined criteria for success of cognitive science, computational neuroscience and artificial intelligence, the best division of labor might cut across the traditional disciplines.\\n\\nbehavioral data. Every brain is idiosyncratic in its structure and function. Moreover, for a given brain, every act of perception, cognition and action is unique in time and cannot be repeated precisely because it permanently changes the brain in question. These complications make it challenging to compare brains and models. We must define the summary statistics of interest and the correspondence mapping between model and brain in space and time at some level of abstraction. Developing appropriate tests for adjudicating among models and determining how close we are to understanding the brain is not merely a technical chal- lenge of statistical inference. It is a conceptual challenge funda- mental to theoretical neuroscience.\\n\\nTasks. By designing experimental tasks, we carve up cognition into components that can be quantitatively investigated. A task is a controlled environment for behavior. It defines the dynamics of a task ‘world’ that provides sensory input (for example, visual stimuli) and captures motor output (for example, button press, joystick control or higher-dimensional limb or whole-body con- trol). Tasks drive the acquisition of brain and behavioral data and the development of AI models, providing well-defined chal- lenges and quantitative performance benchmarks for comparing models. The ImageNet tasks148, for example, have driven substan- tial progress in computer vision. Tasks should be designed and implemented such that they can readily be used in all three disci- plines to drive data acquisition and model development (related developments include OpenAI’s Gym, https://gym.openai.com/; Universe, https://universe.openai.com/; and DeepMind’s Lab149). The spectrum of useful tasks includes classical psychophysical tasks employing simple stimuli and responses as well as interac- tions in virtual realities. As we engage all aspects of the human mind, our tasks will need to simulate natural environments and will come to resemble computer games. This may bring the added benefit of mass participation and big behavioral data, especially when tasks are performed via the Internet150.\\n\\nThe interaction among labs and disciplines can benefit from adversarial cooperation134. Cognitive researchers who feel that current computational models fall short of explaining an impor- tant aspect of cognition are challenged to design shareable tasks and tests that quantify these shortcomings and to provide human behavioral data to set the bar for AI models. Neuroscientists who feel that current models do not explain brain information processing are challenged to share brain-activity data acquired during task performance and tests comparing activity patterns between brains and models to quantify the shortcomings of the models. Although we will have a plurality of definitions of suc- cess, translating these into quantitative measures of the quality of a model is essential and could drive progress in cognitive compu- tational neuroscience, as well as engineering.\\n\\nTasks\\n\\nCognitive science\\n\\nExperimental worlds\\n\\nComputational neuroscience\\n\\nAI\\n\\nElicit behavior, drive learning, and serve as benchmarks for\\n\\nPerform\\n\\nPerform\\n\\nData. Behavioral data acquired during task performance pro- vides overall performance estimates and detailed signatures of success and failure, of reaction times and movement trajectories. Brain-activity measurements characterize the dynamic computa- tions underlying task performance. Anatomical data can char- acterize the structure and connectivity of the brain at multiple scales. Structural brain data, functional brain data and behavioral data will all be essential for constraining computational models.\\n\\nBiological organisms\\n\\nBrain data\\n\\nBrain & behavioral dynamics\\n\\nData\\n\\nModel data Internal & behavioral dynamics\\n\\nare compared with\\n\\nModels\\n\\nTask-performing computational models\\n\\nModels. Task-performing computational models can take sen- sory inputs and produce motor outputs so as to perform experi- mental tasks. AI-scale neurobiologically plausible models can be shared openly and tested in terms of their task performance and in terms of their ability to explain a variety of brain and behav- ioral datasets, including new datasets acquired after definition of the model. Initially, many models will be specific to small subsets of tasks. Ultimately, models must generalize across tasks.\\n\\nTests. To assess the extent to which a model can explain brain information processing during a particular task, we need tests that compare models and brains on the basis of brain and\\n\\nEliminate\\n\\nTests\\n\\nShareable component\\n\\nStatistical inference evaluating models\\n\\ninteractions among shareable components. Tasks, data, models and tests are components (gray nodes) that lend themselves to sharing among labs and across disciplines, to enable collaborative construction and testing of big models driven by big brain and behavioral datasets assembled across labs.\\n\\nLooking ahead Bottom up and top down. The brain seamlessly merges bottom- up discriminative and top-down generative computations in per-\\n\\nceptual inference, and model-free and model-based control. Brain science likewise needs to integrate its levels of description and to progress both bottom-up and top-down, so as to explain task\\n\\n1156\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\nperformance on the basis of neuronal dynamics and provide a mechanistic account of how the brain gives rise to the mind.\\n\\nBottom-up visions, proceeding from detailed measurements toward an understanding of brain computation, have been promi- nent and have driven the most important recent funding initia- tives. The European Human Brain Project and the US BRAIN Initiative are both motivated by bottom-up visions, in which an understanding of brain computation is achieved by measuring and modeling brain dynamics with a focus on the circuit level. The BRAIN Initiative seeks to advance technologies for measur- ing and manipulating neuronal activity. The Human Brain Project attempts to synthesize neuroscience data in biologically detailed dynamic models. Both initiatives proceed primarily from experi- ment toward theory and from the cellular level of description to larger-scale phenomena.\\n\\nMeasuring large numbers of neurons simultaneously and model- ing their interactions at the circuit level will be essential. The bot- tom-up vision is grounded in the history of science. Microscopes and telescopes, for example, have brought scientific breakthroughs. However, it is always in the context of prior theory (generative mod- els of the observed processes) that better observations advance our understanding. In astronomy, for example, the theory of Copernicus guided Galileo in interpreting his telescopic observations.\\n\\nUnderstanding the brain requires that we develop theory and experiment in tandem and complement the bottom-up, data- driven approach by a top-down, theory-driven approach that starts with behavioral functions to be explained113,114. Unprecedentedly rich measurements and manipulations of brain activity will drive theoretical insight when they are used to adjudicate between brain-computational models that pass the first test of being able to perform a function that contributes to the behavioral fitness of the organism. The top-down approach, therefore, is an essential complement to the bottom-up approach toward understanding the brain (Fig. 3).\\n\\nFor an example of a challenge that transcends the disciplines, consider a child seeing an escalator for the first time. She will rapidly recognize people on steps traveling upward obliquely. She might think of it as a moving staircase and imagine riding on it, being lifted one story without exerting any effort. She might infer its function and form a new concept on the basis of a single experience, before ever learning the word “escalator”.\\n\\nDeep neural network models provide a biologically plausible account of the rapid recognition of the elements of the visual expe- rience (people, steps, oblique upward motion, handrail). They can explain the computationally efficient pattern recognition compo- nent42. However, they cannot explain yet how the child understands the relationships among the elements, the physical interactions of the objects, the people’s goal to go up, and the function of the esca- lator, or how she can imagine the experience and instantly form a new concept.\\n\\nBayesian nonparametric models explain how deep inferences and concept formation from single experiences are even possible. They may explain the brain’s stunning statistical efficiency, its ability to infer so much from so little data by building generative models that provide abstract prior knowledge98. However, current inference algorithms require large amounts of computation and, as a result, do not yet scale to real-world challenges such as forming the new concept “escalator” from a single visual experience.\\n\\nOn a 20-watt power budget, the brain’s algorithms combine statistical and computational efficiency in ways that are beyond current AI of either the Bayesian or the neural network variety. However, recent work in AI and machine learning has begun to explore the intersection between Bayesian inference and neural network models, combining the statistical strengths of the for- mer (uncertainty representation, probabilistic inference, statistical efficiency) with the computational strengths of the latter (repre- sentational learning, universal function approximation, computa- tional efficiency)117–119.\\n\\nIntegrating Marr’s levels. Marr (1982) offered a distinction of three levels of analysis: (i) computational theory, (ii) representation and algorithm, and (iii) neurobiological implementation115. Cognitive science starts from computational theory, decomposing cognition into components and developing representations and algorithms from the top down. Computational neuroscience proceeds from the bottom up, composing neuronal building blocks into representa- tions and algorithms thought to be useful components in the con- text of the brain’s overall function. AI builds representations and algorithms that combine simple components to implement com- plex feats of intelligence. All three disciplines thus converge on the algorithms and representations of the brain and mind, contributing complementary constraints116.\\n\\nMarr’s levels provide a useful guide to the challenge of understanding the brain. However, they should not be taken to suggest that cognitive science need not consider the brain or that computational neuroscience need not consider cognition (Box 4). Marr was inspired by computers, which are designed by human engineers to precisely conform to high-level algorithmic descrip- tions. This enables the engineers to abstract from the circuits when designing the algorithms. Even in computer science, how- ever, certain aspects of the algorithms depend on the hardware, such as its parallel processing capabilities. Brains differ from computers in ways that exacerbate this dependence. Brains are the product of evolution and development, processes that are not constrained to generate systems whose behavior can be perfectly captured at some abstract level of description. It may therefore not be possible to understand cognition without considering its implementation in the brain or, conversely, to make sense of neuronal circuits except in the context of the cognitive functions they support.\\n\\nIntegrating all three of Marr’s levels will require close collabora- tion among researchers with a wide variety of expertise. It is diffi- cult for any single lab to excel at neuroscience, cognitive science and AI-scale computational modeling. We therefore need collaborations between labs with complementary expertise. In addition to conven- tional collaborations, an open science culture, in which components are shared between disciplines, can help us integrate Marr’s levels. Shareable components include cognitive tasks, brain and behavioral data, computational models, and tests that evaluate models by com- paring them to biological systems (Box 5).\\n\\nThe study of the mind and brain is entering a particularly excit- ing phase. Recent advances in computer hardware and software enable AI-scale modeling of the mind and brain. If cognitive sci- ence, computational neuroscience and AI can come together, we might be able to explain human cognition with neurobiologically plausible computational models.\\n\\nReceived: 7 November 2016; Accepted: 11 July 2018; Published online: 20 August 2018\\n\\nreferences 1. Newell, A. You can’t play 20 questions with nature and win: projective\\n\\n2.\\n\\ncomments on the papers of this symposium. Technical Report, School of Computer Science, Carnegie Mellon University (1973). Lake, B. M., Ullman, T. D., Tenenbaum, J. B. & Gershman, S. J. Building machines that learn and think like people. Behav. Brain Sci. 40, e253 (2017).\\n\\n3. Kriegeskorte, N. & Mok, R. M. Building machines that adapt and compute like brains. Behav. Brain Sci. 40, e269 (2017). Simon, H. A. & Newell, A. Human problem solving: the state of the theory in 1970. Am. Psychol. 26, 145–159 (1971).\\n\\n3. Kriegeskorte, N. & Mok, R. M. Building machines that adapt and compute like brains. Behav. Brain Sci. 40, e269 (2017). Simon, H. A. & Newell, A. Human problem solving: the state of the theory in 1970. Am. Psychol. 26, 145–159 (1971).\\n\\n5. Anderson, J. R. The Architecture of Cognition (Harvard Univ. Press, Cambridge, MA, USA, 1983).\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n1157\\n\\n6. McClelland, J. L. & Rumelhart, D. E. Parallel Distributed Processing (MIT\\n\\n39. Krizhevsky, A., Sutskever, I. & Hinton, G. E. ImageNet classification with\\n\\nPress, Cambridge, MA, USA, 1987).\\n\\n7. Gazzaniga, M. S. ed. The Cognitive Neurosciences (MIT Press, Cambridge,\\n\\nMA, USA, 2004). 8. Fodor, J. A. Précis of The Modularity of Mind. Behav. Brain Sci. 8, 1 (1985). 9. Chklovskii, D. B. & Koulakov, A. A. Maps in the brain: what can we learn\\n\\ndeep convolutional neural networks. in Advances in Neural Information Processing Systems 25 1097–1105 (Curran Associates, Red Hook, NY, USA, 2012).\\n\\n40. Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484–489 (2016).\\n\\nfrom them? Annu. Rev. Neurosci. 27, 369–392 (2004).\\n\\n41. Mnih, V. et al. Human-level control through deep reinforcement learning.\\n\\n10. Szucs, D. & Ioannidis, J. P. A. Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS Biol. 15, e2000797 (2017).\\n\\n10. Szucs, D. & Ioannidis, J. P. A. Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. PLoS Biol. 15, e2000797 (2017).\\n\\n42. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436–444 (2015).\\n\\n11. Kriegeskorte, N., Simmons, W. K., Bellgowan, P. S. F. & Baker, C. I.\\n\\n43. Cohen, J. D. et al. Computational approaches to fMRI analysis. Nat.\\n\\nCircular analysis in systems neuroscience: the dangers of double dipping. Nat. Neurosci. 12, 535–540 (2009).\\n\\n12. Kanwisher, N., McDermott, J. & Chun, M. M. The fusiform face area: a module in human extrastriate cortex specialized for face perception. J. Neurosci. 17, 4302–4311 (1997).\\n\\n13. Tsao, D. Y., Freiwald, W. A., Tootell, R. B. & Livingstone, M. S. A cortical region consisting entirely of face-selective cells. Science 311, 670–674 (2006).\\n\\n14. Freiwald, W. A. & Tsao, D. Y. Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science 330, 845–851 (2010).\\n\\nNeurosci. 20, 304–313 (2017).\\n\\n44. Forstmann, B. U., Wagenmakers, E.-J., Eichele, T., Brown, S. & Serences, J. T. Reciprocal relations between cognitive neuroscience and formal cognitive models: opposites attract? Trends Cogn. Sci. 15, 272–279 (2011).\\n\\n45. Deco, G., Tononi, G., Boly, M. & Kringelbach, M. L. Rethinking segregation and integration: contributions of whole-brain modelling. Nat. Rev. Neurosci. 16, 430–439 (2015).\\n\\n46. Biswal, B., Yetkin, F. Z., Haughton, V. M. & Hyde, J. S. Functional connectivity in the motor cortex of resting human brain using echo-planar MRI. Magn. Reson. Med. 34, 537–541 (1995).\\n\\n47. Hyvarinen, A., Karhunen, J. & Oja, E. Independent Component Analysis\\n\\n15. Grill-Spector, K., Weiner, K. S., Kay, K. & Gomez, J. The functional neuroanatomy of human face perception. Annu. Rev. Vis. Sci. 3, 167–196 (2017).\\n\\n15. Grill-Spector, K., Weiner, K. S., Kay, K. & Gomez, J. The functional neuroanatomy of human face perception. Annu. Rev. Vis. Sci. 3, 167–196 (2017).\\n\\n48. Bullmore, E. T. & Bassett, D. S. Brain graphs: graphical models of the human brain connectome. Annu. Rev. Clin. Psychol. 7, 113–140 (2011).\\n\\n16. Yildirim, I. et al. Efficient and robust analysis-by-synthesis in vision: a\\n\\n49. Deco, G., Jirsa, V. K. & McIntosh, A. R. Emerging concepts for the\\n\\ncomputational framework, behavioral tests, and modeling neuronal representations. in Annual Conference of the Cognitive Science Society (eds. Noelle, D. C. et al.) (Cognitive Science Society, Austin, TX, USA, 2015).\\n\\n17. Kriegeskorte, N., Formisano, E., Sorger, B. & Goebel, R. Individual faces elicit distinct response patterns in human anterior temporal cortex. Proc. Natl Acad. Sci. USA 104, 20600–20605 (2007).\\n\\n18. Anzellotti, S., Fairhall, S. L. & Caramazza, A. Decoding representations of face identity that are tolerant to rotation. Cereb. Cortex 24, 1988–1995 (2014).\\n\\n19. Chang, L. & Tsao, D. Y. The code for facial identity in the primate brain. Cell 169, 1013–1028.e14 (2017).\\n\\ndynamical organization of resting-state activity in the brain. Nat. Rev. Neurosci. 12, 43–56 (2011).\\n\\n50. Friston, K. Dynamic causal modeling and Granger causality. Comments on: the identification of interacting networks in the brain using fMRI: model selection, causality and deconvolution. Neuroimage 58, 303–305 (2011). author reply 310–311.\\n\\n51. Dennett, D. C. The Intentional Stance (MIT Press, Cambridge, MA, USA, 1987).\\n\\n52. Diedrichsen, J. & Kriegeskorte, N. Representational models: a common framework for understanding encoding, pattern-component, and representational-similarity analysis. PLoS Comput. Biol. 13, e1005508 (2017).\\n\\n20. Van Essen, D. C. et al. The Brain Analysis Library of Spatial maps and Atlases (BALSA) database. Neuroimage 144(Pt. B), 270–274 (2017).\\n\\n21. Griffiths, T. L., Chater, N., Kemp, C., Perfors, A. & Tenenbaum, J. B. Probabilistic models of cognition: exploring representations and inductive biases. Trends Cogn. Sci. 14, 357–364 (2010).\\n\\n22. Ernst, M. O. & Banks, M. S. Humans integrate visual and haptic information in a statistically optimal fashion. Nature 415, 429–433 (2002).\\n\\n53. Afraz, S.-R., Kiani, R. & Esteky, H. Microstimulation of inferotemporal cortex influences face categorization. Nature 442, 692–695 (2006). 54. Parvizi, J. et al. Electrical stimulation of human fusiform face-selective regions distorts face perception. J. Neurosci. 32, 14915–14920 (2012).\\n\\n55. Norman, K. A., Polyn, S. M., Detre, G. J. & Haxby, J. V. Beyond mind- reading: multi-voxel pattern analysis of fMRI data. Trends Cogn. Sci. 10, 424–430 (2006).\\n\\n23. Weiss, Y., Simoncelli, E. P. & Adelson, E. H. Motion illusions as optimal\\n\\n56. Tong, F. & Pratte, M. S. Decoding patterns of human brain activity. Annu. Rev. Psychol. 63, 483–509 (2012).\\n\\n56. Tong, F. & Pratte, M. S. Decoding patterns of human brain activity. Annu. Rev. Psychol. 63, 483–509 (2012).\\n\\n24. Körding, K. P. & Wolpert, D. M. Bayesian integration in sensorimotor\\n\\n57. Kriegeskorte, N. & Kievit, R. A. Representational geometry: integrating\\n\\nlearning. Nature 427, 244–247 (2004).\\n\\n25. MacKay, D. J. C. Information Theory, Inference, and Learning Algorithms. (Cambridge Univ. Press, Cambridge, 2003)\\n\\n26. Murphy, K. P. Machine Learning: A Probabilistic Perspective (MIT Press, Cambridge, MA, USA, 2012).\\n\\ncognition, computation, and the brain. Trends Cogn. Sci. 17, 401–412 (2013).\\n\\n58. Haxby, J. V., Connolly, A. C. & Guntupalli, J. S. Decoding neural representational spaces using multivariate pattern analysis. Annu. Rev. Neurosci. 37, 435–456 (2014).\\n\\n27. Dayan, P. & Abbott, L. F. Theoretical Neuroscience: Computational and\\n\\n59. Haynes, J.-D. A primer on pattern-based approaches to fMRI: principles,\\n\\nMathematical Modeling of Neural Systems (MIT Press, Cambridge, MA, USA, 2001).\\n\\n28. Abbott, L. F. Theoretical neuroscience rising. Neuron 60, 489–495 (2008). 29. Olshausen, B. A. & Field, D. J. Sparse coding of sensory inputs. Curr. Opin.\\n\\n60.\\n\\npitfalls, and perspectives. Neuron 87, 257–270 (2015). Jin, X. & Costa, R. M. Shaping action sequences in basal ganglia circuits. Curr. Opin. Neurobiol. 33, 188–196 (2015).\\n\\n61. DiCarlo, J. J. & Cox, D. D. Untangling invariant object recognition. Trends Cogn. Sci. 11, 333–341 (2007).\\n\\n61. DiCarlo, J. J. & Cox, D. D. Untangling invariant object recognition. Trends Cogn. Sci. 11, 333–341 (2007).\\n\\n30. Simoncelli, E. P. & Olshausen, B. A. Natural image statistics and neural\\n\\n62. Naselaris, T. & Kay, K. N. Resolving ambiguities of MVPA using explicit models of representation. Trends Cogn. Sci. 19, 551–554 (2015).\\n\\n62. Naselaris, T. & Kay, K. N. Resolving ambiguities of MVPA using explicit models of representation. Trends Cogn. Sci. 19, 551–554 (2015).\\n\\n31. Carandini, M. & Heeger, D. J. Normalization as a canonical neural\\n\\n63. Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008).\\n\\n63. Mitchell, T. M. et al. Predicting human brain activity associated with the meanings of nouns. Science 320, 1191–1195 (2008).\\n\\n32. Chaudhuri, R. & Fiete, I. Computational principles of memory.\\n\\n64. Kay, K. N., Naselaris, T., Prenger, R. J. & Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352–355 (2008).\\n\\n64. Kay, K. N., Naselaris, T., Prenger, R. J. & Gallant, J. L. Identifying natural images from human brain activity. Nature 452, 352–355 (2008).\\n\\n33. Shadlen, M. N. & Kiani, R. Decision making as a window on cognition.\\n\\n65. Dumoulin, S. O. & Wandell, B. A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647–660 (2008).\\n\\n65. Dumoulin, S. O. & Wandell, B. A. Population receptive field estimates in human visual cortex. Neuroimage 39, 647–660 (2008).\\n\\n34. Newsome, W. T., Britten, K. H. & Movshon, J. A. Neuronal correlates of a\\n\\n66. Diedrichsen, J., Ridgway, G. R., Friston, K. J. & Wiestler, T. Comparing the\\n\\nperceptual decision. Nature 341, 52–54 (1989).\\n\\n35. Wang, X.-J. Decision making in recurrent neuronal circuits. Neuron 60, 215–234 (2008).\\n\\n36. Diedrichsen, J., Shadmehr, R. & Ivry, R. B. The coordination of movement: optimal feedback control and beyond. Trends Cogn. Sci. 14, 31–39 (2010).\\n\\nsimilarity and spatial structure of neural representations: a pattern- component model. Neuroimage 55, 1665–1678 (2011).\\n\\n67. Kriegeskorte, N., Mur, M. & Bandettini, P. Representational similarity analysis - connecting the branches of systems neuroscience. Front. Syst. Neurosci. 2, 4 (2008).\\n\\n68. Nili, H. et al. A toolbox for representational similarity analysis. PLoS\\n\\n37. Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446 (2015).\\n\\n37. Kriegeskorte, N. Deep neural networks: a new framework for modeling biological vision and brain information processing. Annu. Rev. Vis. Sci. 1, 417–446 (2015).\\n\\n38. Yamins, D. L. K. & DiCarlo, J. J. Using goal-driven deep learning models to understand sensory cortex. Nat. Neurosci. 19, 356–365 (2016).\\n\\n69. Devereux, B. J., Clarke, A., Marouchos, A. & Tyler, L. K. Representational similarity analysis reveals commonalities and differences in the semantic processing of words and objects. J. Neurosci. 33, 18906–18916 (2013).\\n\\n1158\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n70. Huth, A. G., de Heer, W. A., Griffiths, T. L., Theunissen, F. E. & Gallant, J. L. Natural speech reveals the semantic maps that tile human cerebral cortex. Nature 532, 453–458 (2016).\\n\\n71. Markram, H. The Blue Brain Project. Nat. Rev. Neurosci. 7, 153–160 (2006). 72. Eliasmith, C. & Trujillo, O. The use and abuse of large-scale brain models. Curr. Opin. Neurobiol. 25, 1–6 (2014).\\n\\n73. Eliasmith, C. et al. A large-scale model of the functioning brain. Science 338, 1202–1205 (2012).\\n\\n& Vlek, C.) 141–162, https://doi.org/10.1007/978-94-010-1834-0_8 (Springer Netherlands, Dordrecht, the Netherlands, 1975).\\n\\n100. Lake, B. M., Salakhutdinov, R. & Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science 350, 1332–1338 (2015).\\n\\n101. Ullman, T. D., Spelke, E., Battaglia, P. & Tenenbaum, J. B. Mind games: game engines as an architecture for intuitive physics. Trends Cogn. Sci. 21, 649–665 (2017).\\n\\n74. Hassabis, D., Kumaran, D., Summerfield, C. & Botvinick, M. Neuroscience- inspired artificial intelligence. Neuron 95, 245–258 (2017).\\n\\n75. Rumelhart, D. E., Hinton, G. E. & Williams, R. J. Learning representations by back-propagating errors. Nature 323, 533–536 (1986).\\n\\n102. Battaglia, P. W., Hamrick, J. B. & Tenenbaum, J. B. Simulation as an engine of physical scene understanding. Proc. Natl Acad. Sci. USA 110, 18327– 18332 (2013).\\n\\n103. Kubricht, J. R., Holyoak, K. J. & Lu, H. Intuitive physics: current research\\n\\n76. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (MIT Press, Cambridge, MA, USA, 2016).\\n\\n76. Goodfellow, I., Bengio, Y. & Courville, A. Deep Learning (MIT Press, Cambridge, MA, USA, 2016).\\n\\n104. Pantelis, P. C. et al. Inferring the intentional states of autonomous virtual\\n\\n77. Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 8619–8624 (2014).\\n\\n77. Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 8619–8624 (2014).\\n\\n78. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep supervised, but not\\n\\n105. Pouget, A., Beck, J. M., Ma, W. J. & Latham, P. E. Probabilistic brains:\\n\\nknowns and unknowns. Nat. Neurosci. 16, 1170–1178 (2013). 106. Orhan, A. E. & Ma, W. J. Efficient probabilistic inference in generic\\n\\nunsupervised, models may explain IT cortical representation. PLoS Comput. Biol. 10, e1003915 (2014).\\n\\nneural networks trained with non-probabilistic feedback. Nat. Commun. 8, 138 (2017).\\n\\n79. Cadieu, C. F. et al. Deep neural networks rival the representation of primate\\n\\n107. Tervo, D. G. R., Tenenbaum, J. B. & Gershman, S. J. Toward the neural\\n\\nIT cortex for core visual object recognition. PLoS Comput. Biol. 10, e1003963 (2014).\\n\\n80. Güçlü, U. & van Gerven, M. A. J. Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream. J. Neurosci. 35, 10005–10014 (2015).\\n\\nimplementation of structure learning. Curr. Opin. Neurobiol. 37, 99–105 (2016). 108. Buesing, L., Bill, J., Nessler, B. & Maass, W. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. PLoS Comput. Biol. 7, e1002211 (2011).\\n\\n109. Haefner, R. M., Berkes, P. & Fiser, J. Perceptual decision-making as\\n\\n81. Eickenberg, M., Gramfort, A., Varoquaux, G. & Thirion, B. Seeing it all: convolutional network layers map the function of the human visual system. Neuroimage 152, 184–194 (2017).\\n\\n81. Eickenberg, M., Gramfort, A., Varoquaux, G. & Thirion, B. Seeing it all: convolutional network layers map the function of the human visual system. Neuroimage 152, 184–194 (2017).\\n\\n82. Cichy, R. M., Khosla, A., Pantazis, D., Torralba, A. & Oliva, A. Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. Sci. Rep. 6, 27755 (2016).\\n\\n83. Hong, H., Yamins, D. L. K., Majaj, N. J. & DiCarlo, J. J. Explicit information for category-orthogonal object properties increases along the ventral stream. Nat. Neurosci. 19, 613–622 (2016).\\n\\n84. Kubilius, J., Bracci, S. & Op de Beeck, H. P. Deep neural networks as a\\n\\ncomputational model for human shape sensitivity. PLoS Comput. Biol. 12, e1004896 (2016). Jozwik, K. M., Kriegeskorte, N., Storrs, K. R. & Mur, M. Deep convolutional neural networks outperform feature-based but not categorical models in explaining object similarity judgments. Front. Psychol. 8, 1726 (2017). 86. Moore, C. & Mertens, S. The Nature of Computation. (Oxford Univ. Press,\\n\\n85.\\n\\nOxford, 2011).\\n\\n110. Aitchison, L. & Lengyel, M. The Hamiltonian brain: efficient probabilistic inference with excitatory-inhibitory neural circuit dynamics. PLoS Comput. Biol. 12, e1005186 (2016).\\n\\n111. Sanborn, A. N. & Chater, N. Bayesian brains without probabilities. Trends Cogn. Sci. 20, 883–893 (2016).\\n\\n112. Dasgupta, I., Schulz, E., Goodman, N. & Gershman, S. Amortized hypothesis generation. Preprint at bioRxiv https://doi.org/10.1101/137190 (2017).\\n\\n113. Krakauer, J. W., Ghazanfar, A. A., Gomez-Marin, A., MacIver, M. A. & Poeppel, D. Neuroscience needs behavior: correcting a reductionist bias. Neuron 93, 480–490 (2017).\\n\\n114. Gomez-Marin, A., Paton, J. J., Kampff, A. R., Costa, R. M. & Mainen, Z. F. Big behavioral data: psychology, ethology and the foundations of neuroscience. Nat. Neurosci. 17, 1455–1462 (2014).\\n\\n115. Marr, D. Vision: A Computational Investigation into the Human Representation and Processing of Visual Information (MIT Press, Cambridge, MA, USA, 2010).\\n\\n87. Borst, J., Taatgen & Anderson, J. Using the ACT-R cognitive architecture in\\n\\n116. Love, B. C. The algorithmic level is the bridge between computation and\\n\\ncombination with fMRI data. in An Introduction to Model-Based Cognitive Neuroscience (eds. Forstmann, B. U. & Wagenmakers, E.-J.) (Springer, New York, 2014).\\n\\n88. Sutton, R. & Barto, A. Reinforcement Learning: An Introduction Vol. 1 (MIT Press, Cambridge, MA, USA, 1998).\\n\\n89. O’Doherty, J. P., Cockburn, J. & Pauli, W. M. Learning, reward, and decision making. Annu. Rev. Psychol. 68, 73–100 (2017).\\n\\nbrain. Top. Cogn. Sci. 7, 230–242 (2015).\\n\\n117. Gal, Y. & Ghahramani, Z. Dropout as a Bayesian approximation: representing model uncertainty in deep learning. Preprint at https://arxiv. org/abs/1506.02142 (2016).\\n\\n118. Rezende, D., Mohamed, S., Danihelka, I., Gregor, K. & Wierstra, D. One-shot generalization in deep generative models. Proc. Int. Conf. Mach. Learn. Appl. 48, 1521–1529 (2016).\\n\\n90. Daw, N. D. & Dayan, P. The algorithmic anatomy of model-based evaluation. Phil. Trans. R. Soc. Lond. B 369, 20130478 (2014).\\n\\n119. Kingma, D. & Welling, M. Auto-encoding variational Bayes. Preprint at https://arxiv.org/abs/1312.6114 (2013).\\n\\n91. Lengyel, M. & Dayan, P. Hippocampal contributions to control: the third\\n\\n120. Naselaris, T. et al. Cognitive Computational Neuroscience: a new conference for an emerging discipline. Trends Cogn. Sci. 22, 365–367 (2018).\\n\\n120. Naselaris, T. et al. Cognitive Computational Neuroscience: a new conference for an emerging discipline. Trends Cogn. Sci. 22, 365–367 (2018).\\n\\n121. Ahrens, M. B. et al. Brain-wide neuronal dynamics during motor adaptation\\n\\n92. Gershman, S. J. & Daw, N. D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annu. Rev. Psychol. 68, 101–128 (2017).\\n\\n92. Gershman, S. J. & Daw, N. D. Reinforcement learning and episodic memory in humans and animals: an integrative framework. Annu. Rev. Psychol. 68, 101–128 (2017).\\n\\n93. Schultz, W., Dayan, P. & Montague, P. R. A neural substrate of prediction\\n\\n122. Kietzmann, T., McClure, P. & Kriegeskorte, N. Deep neural networks in computational neuroscience. Preprint at bioRxiv https://doi. org/10.1101/133504 (2017).\\n\\nand reward. Science 275, 1593–1599 (1997).\\n\\n123. Hornik, K. Approximation capabilities of multilayer feedforward networks.\\n\\n94. Sutton, R. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. in Proceedings of the Seventh International Conference on Machine Learning 216–224 (Morgan Kaufmann, San Francisco, 1990).\\n\\nNeural Netw. 4, 251–257 (1991).\\n\\n124. Wyatte, D., Curran, T. & O’Reilly, R. The limits of feedforward vision: recurrent processing promotes robust object recognition when objects are degraded. J. Cogn. Neurosci. 24, 2248–2261 (2012).\\n\\n95. Daw, N. D., Niv, Y. & Dayan, P. Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control. Nat. Neurosci. 8, 1704–1711 (2005).\\n\\n125. Spoerer, C. J., McClure, P. & Kriegeskorte, N. Recurrent convolutional neural networks: a better model of biological object recognition. Front. Psychol. 8, 1551 (2017).\\n\\n96. Ma, W. J. Organizing probabilistic models of perception. Trends Cogn. Sci.\\n\\n126. Hunt, L. T. & Hayden, B. Y. A distributed, hierarchical and recurrent framework for reward-based choice. Nat. Rev. Neurosci. 18, 172–182 (2017).\\n\\n126. Hunt, L. T. & Hayden, B. Y. A distributed, hierarchical and recurrent framework for reward-based choice. Nat. Rev. Neurosci. 18, 172–182 (2017).\\n\\n97. Fiser, J., Berkes, P., Orbán, G. & Lengyel, M. Statistically optimal perception and learning: from behavior to neural representations. Trends Cogn. Sci. 14, 119–130 (2010).\\n\\n127. Schäfer, A. M. & Zimmermann, H. G. Recurrent neural networks are universal approximators. Int. J. Neural Syst. 17, 253–263 (2007).\\n\\n128. O’Reilly, R. C., Hazy, T. E., Mollick, J., Mackie, P. & Herd, S. Goal-driven\\n\\n98. Tenenbaum, J. B., Kemp, C., Griffiths, T. L. & Goodman, N. D. How to grow a mind: statistics, structure, and abstraction. Science 331, 1279–1285 (2011).\\n\\n99. Tversky, A. & Kahneman, D. Judgment under uncertainty: heuristics and biases. in Utility, Probability, and Human Decision Making (eds. Wendt, D.\\n\\ncognition in the brain: a computational framework. Preprint at http://arxiv. org/abs/1404.7591 (2014).\\n\\n129. Whittington, J. C. R. & Bogacz, R. An approximation of the error backpropagation algorithm in a predictive coding network with local Hebbian synaptic plasticity. Neural Comput. 29, 1229–1262 (2017).\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience\\n\\n1159\\n\\n130. Schiess, M., Urbanczik, R. & Senn, W. Somato-dendritic synaptic plasticity and error-backpropagation in active dendrites. PLoS Comput. Biol. 12, e1004638 (2016).\\n\\nMethods in Natural Language Processing and Computational Natural Language Learning 1114–1124 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2012).\\n\\n131. Marblestone, A. H., Wayne, G. & Kording, K. P. Towards an integration of\\n\\n146. Bengio, Y., Scellier, B., Bilaniuk, O., Sacramento, J. & Senn, W. Feedforward\\n\\ndeep learning and neuroscience. Front. Comput. Neurosci. 10, 94 (2016).\\n\\n132. Shadlen, M. N. & Shohamy, D. Decision making and sequential sampling from memory. Neuron 90, 927–939 (2016).\\n\\n132. Shadlen, M. N. & Shohamy, D. Decision making and sequential sampling from memory. Neuron 90, 927–939 (2016).\\n\\n133. Roelfsema, P. R. & van Ooyen, A. Attention-gated reinforcement learning of internal representations for classification. Neural Comput. 17, 2176–2214 (2005).\\n\\n134. Goodfellow, I. et al. Generative adversarial nets. Preprint at https://arxiv. org/abs/1406.2661 (2014).\\n\\n147. Ghahramani, Z. Bayesian non-parametrics and the probabilistic approach to modelling. Philos. Trans. A Math. Phys. Eng. Sci. 371, 20110553 (2012). 148. Deng, J. et al. ImageNet: a large-scale hierarchical image database. in 2009 IEEE\\n\\nConference on Computer Vision and Pattern Recognition 248–255, https://doi.org/10.1109/CVPR.2009.5206848 (IEEE, Piscataway, NJ, USA, 2009). 149. Beattie, C. et al. DeepMind Lab. Preprint at https://arxiv.org/abs/1612.03801\\n\\n135. Kandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum, S. A. & Hudspeth, A. J. Principles of Neural Science (McGraw-Hill Professional, New York, 2013). 136. Bastos, A. M. et al. Canonical microcircuits for predictive coding. Neuron 76, 695–711 (2012).\\n\\n(2016).\\n\\n150. Griffiths, T. L. Manifesto for a new (computational) cognitive revolution. Cognition 135, 21–23 (2015).\\n\\n137. Larkum, M. A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex. Trends Neurosci. 36, 141–151 (2013).\\n\\n138. Fries, P. A mechanism for cognitive dynamics: neuronal communication through neuronal coherence. Trends Cogn. Sci. 9, 474–480 (2005). 139. Kumaran, D., Hassabis, D. & McClelland, J. L. What learning systems do intelligent agents need? complementary learning systems theory updated. Trends Cogn. Sci. 20, 512–534 (2016).\\n\\n140. Yuille, A. & Kersten, D. Vision as Bayesian inference: analysis by synthesis? Trends Cogn. Sci. 10, 301–308 (2006).\\n\\nacknowledgements This paper benefited from discussions in the context of the new conference Cognitive Computational Neuroscience, which had its inaugural meeting in New York City in September 2017120. We are grateful in particular to T. Naselaris, K. Kay, K. Kording, D. Shohamy, R. Poldrack, J. Diedrichsen, M. Bethge, R. Mok, T. Kietzmann, K. Storrs, M. Mur, T. Golan, M. Lengyel, M. Shadlen, D. Wolpert, A. Oliva, D. Yamins, J. Cohen, J. DiCarlo, T. Konkle, J. McDermott, N. Kanwisher, S. Gershman and J. Tenenbaum for inspiring discussions.\\n\\n141. Helmholtz, H. Handbuch der physiologischen Optik (Dover, New York, 1860). 142. Gershman, S. J., Horvitz, E. J. & Tenenbaum, J. B. Computational rationality: a converging paradigm for intelligence in brains, minds, and machines. Science 349, 273–278 (2015).\\n\\nCompeting interests The authors declare no competing interests.\\n\\n143. Simon, H. A. Bounded rationality. in Utility and Probability (eds. Eatwell, J., Milgate, M. & Newman, P.) 15–18, https://doi.org/10.1007/978-1-349- 20568-4_5 (Palgrave Macmillan, London, 1990).\\n\\n144. Griffiths, T. L., Lieder, F. & Goodman, N. D. Rational use of cognitive resources: levels of analysis between the computational and the algorithmic. Top. Cogn. Sci. 7, 217–229 (2015).\\n\\nadditional information Reprints and permissions information is available at www.nature.com/reprints.\\n\\nCorrespondence should be addressed to N.K.\\n\\n145. Srikumar, V., Kundu, G. & Roth, D. On amortizing inference cost for structured prediction Proceedings of the 2012 Joint Conference on Empirical\\n\\nPublisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n1160\\n\\nNature NeurosCieNCe | VOL 21 | SEPTEMBER 2018 | 1148–1160 | www.nature.com/natureneuroscience'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_2[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRJ-i0v8n-vk",
        "outputId": "9a2c9627-3979-4703-80bc-fcc79a046438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/Cognitive computational neuroscience.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/Cognitive computational neuroscience.pdf\"\n",
        "docs_2 = fitz.open(pdf_path)\n",
        "\n",
        "print(f\"📄 Number of pages in the second file: {len(docs_2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPF1fTnho3Fu",
        "outputId": "efc86d8b-94b8-403b-dbdf-2d707dbb735e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Number of pages in the second file: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "0G2vcmrI7icm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks_2 = splitter.split_documents(docs_2)"
      ],
      "metadata": {
        "id": "ve3zCn5YoFqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_chunks_2 = []\n",
        "for chunk in chunks_2:\n",
        "    processed_chunks_2.append(Document(page_content=chunk.page_content, metadata={\"source\": \"source_02\"}))\n",
        "print(f\"✅ Number of chunks in the second file: {len(processed_chunks_2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLOSMiGJp61h",
        "outputId": "6226c37f-b823-4c97-9ae1-cff12c900d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Number of chunks in the second file: 218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PDF 3**"
      ],
      "metadata": {
        "id": "b6vC8zbBrbKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredFileLoader\n",
        "\n",
        "loader_3= UnstructuredFileLoader(\"/content/attention.pdf\")\n",
        "docs_3 = loader_3.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ablunC51rFug",
        "outputId": "f995004f-f18b-4e6b-83eb-e4f67a9c296f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No languages specified, defaulting to English.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_3[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "_rqGxR39rqcp",
        "outputId": "0c09b145-602e-4ada-d4fe-4cf2ff4cf337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Attention Is All You Need\\n\\nAshish Vaswani∗ Google Brain avaswani@google.com\\n\\nNoam Shazeer∗ Google Brain noam@google.com\\n\\nNiki Parmar∗ Google Research nikip@google.com\\n\\nJakob Uszkoreit∗ Google Research usz@google.com\\n\\nLlion Jones∗ Google Research llion@google.com\\n\\nAidan N. Gomez∗ † University of Toronto aidan@cs.toronto.edu\\n\\nŁukasz Kaiser∗ Google Brain lukaszkaiser@google.com\\n\\nIllia Polosukhin∗ ‡ illia.polosukhin@gmail.com\\n\\nAbstract\\n\\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\\n\\n1\\n\\nIntroduction\\n\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\\n\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n†Work performed while at Google Brain. ‡Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n\\n2 Background\\n\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difﬁcult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.\\n\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\n\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28].\\n\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [14, 15] and [8].\\n\\n3 Model Architecture\\n\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]. Here, the encoder maps an input sequence of symbol representations (x1,...,xn) to a sequence of continuous representations z = (z1,...,zn). Given z, the decoder then generates an output sequence (y1,...,ym) of symbols one element at a time. At each step the model is auto-regressive [9], consuming the previously generated symbols as additional input when generating the next.\\n\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\\n\\n3.1 Encoder and Decoder Stacks\\n\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n\\n2\\n\\nFigure 1: The Transformer - model architecture.\\n\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\\n\\n3.2 Attention\\n\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\\n\\n3.2.1 Scaled Dot-Product Attention\\n\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n\\n3\\n\\nScaled Dot-Product Attention\\n\\nMulti-Head Attention\\n\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.\\n\\nquery with all keys, divide each by values.\\n\\n√\\n\\ndk, and apply a softmax function to obtain the weights on the\\n\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as:\\n\\nAttention(Q,K,V ) = softmax(\\n\\nQKT √ dk\\n\\n)V\\n\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code.\\n\\n1√\\n\\ndk\\n\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1√ dk\\n\\n.\\n\\n3.2.2 Multi-Head Attention\\n\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once again projected, resulting in the ﬁnal values, as depicted in Figure 2.\\n\\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\\n\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random i=1 qiki, has mean 0 and variance dk.\\n\\nvariables with mean 0 and variance 1. Then their dot product, q · k = (cid:80)dk\\n\\n4\\n\\n(1)\\n\\nMultiHead(Q,K,V ) = Concat(head1,...,headh)W O\\n\\nwhere headi = Attention(QW Q\\n\\ni ,KW K i\\n\\n,V W V\\n\\ni )\\n\\nWhere the projections are parameter matrices W Q and W O ∈ Rhdv×dmodel.\\n\\ni ∈ Rdmodel×dk, W K\\n\\ni ∈ Rdmodel×dk, W V\\n\\ni ∈ Rdmodel×dv\\n\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\\n\\n3.2.3 Applications of Attention in our Model\\n\\nThe Transformer uses multi-head attention in three different ways:\\n\\nIn \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8].\\n\\nThe encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\\n\\nSimilarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information ﬂow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\\n\\n3.3 Position-wise Feed-Forward Networks\\n\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\\n\\nFFN(x) = max(0,xW1 + b1)W2 + b2\\n\\n(2)\\n\\nWhile the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\\n\\n3.4 Embeddings and Softmax\\n\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax dmodel. linear transformation, similar to [24]. In the embedding layers, we multiply those weights by\\n\\n√\\n\\n3.5 Positional Encoding\\n\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n\\n5\\n\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self-attention.\\n\\nLayer Type\\n\\nSelf-Attention Recurrent Convolutional Self-Attention (restricted)\\n\\nComplexity per Layer\\n\\nO(n2 · d) O(n · d2) O(k · n · d2) O(r · n · d)\\n\\nSequential Maximum Path Length Operations O(1) O(n) O(1) O(1)\\n\\nO(1) O(n) O(logk(n)) O(n/r)\\n\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and ﬁxed [8].\\n\\nIn this work, we use sine and cosine functions of different frequencies:\\n\\nPE(pos,2i) = sin(pos/100002i/dmodel) PE(pos,2i+1) = cos(pos/100002i/dmodel)\\n\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of PEpos.\\n\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\\n\\n4 Why Self-Attention\\n\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1,...,xn) to another sequence of equal length (z1,...,zn), with xi,zi ∈ Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata.\\n\\nOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required.\\n\\nThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\\n\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\n\\n6\\n\\nthe input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work.\\n\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [15], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\\n\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\\n\\n5 Training\\n\\nThis section describes the training regime for our models.\\n\\n5.1 Training Data and Batching\\n\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\\n\\n5.2 Hardware and Schedule\\n\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days).\\n\\n5.3 Optimizer\\n\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and (cid:15) = 10−9. We varied the learning rate over the course of training, according to the formula:\\n\\nlrate = d−0.5\\n\\nmodel · min(step_num−0.5,step_num · warmup_steps−1.5)\\n\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\\n\\n5.4 Regularization\\n\\nWe employ three types of regularization during training:\\n\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\\n\\n7\\n\\n(3)\\n\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\n\\nModel\\n\\nByteNet [15] Deep-Att + PosUnk [32] GNMT + RL [31] ConvS2S [8] MoE [26] Deep-Att + PosUnk Ensemble [32] GNMT + RL Ensemble [31] ConvS2S Ensemble [8] Transformer (base model) Transformer (big)\\n\\nBLEU\\n\\nEN-DE EN-FR 23.75\\n\\n24.6 25.16 26.03\\n\\n26.30 26.36 27.3 28.4\\n\\n39.2 39.92 40.46 40.56 40.4 41.16 41.29 38.1 41.0\\n\\nTraining Cost (FLOPs)\\n\\nEN-DE\\n\\nEN-FR\\n\\n2.3 · 1019 9.6 · 1018 2.0 · 1019\\n\\n1.8 · 1020 7.7 · 1019\\n\\n1.0 · 1020 1.4 · 1020 1.5 · 1020 1.2 · 1020 8.0 · 1020 1.1 · 1021 1.2 · 1021\\n\\n3.3 · 1018 2.3 · 1019\\n\\nLabel Smoothing During training, we employed label smoothing of value (cid:15)ls = 0.1 [30]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n\\n6 Results\\n\\n6.1 Machine Translation\\n\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models.\\n\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.\\n\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty α = 0.6 [31]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [31].\\n\\nTable 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of ﬂoating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision ﬂoating-point capacity of each GPU 5.\\n\\n6.2 Model Variations\\n\\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\\n\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n\\n8\\n\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\\n\\nbase\\n\\n(A)\\n\\n(B)\\n\\n(C)\\n\\n(D)\\n\\nN dmodel\\n\\n6\\n\\n512\\n\\n2 4 8\\n\\n256 1024\\n\\ndff\\n\\n2048\\n\\n1024 4096\\n\\nh\\n\\n8 1 4 16 32\\n\\ndk\\n\\n64 512 128 32 16 16 32\\n\\n32 128\\n\\ndv\\n\\n64 512 128 32 16\\n\\n32 128\\n\\nPdrop\\n\\n0.1\\n\\n0.0 0.2\\n\\n(cid:15)ls\\n\\n0.1\\n\\n0.0 0.2\\n\\nPPL train steps (dev) 100K 4.92 5.29 5.00 4.91 5.01 5.16 5.01 6.11 5.19 4.88 5.75 4.66 5.12 4.75 5.77 4.95 4.67 5.47 4.92 300K 4.33\\n\\nBLEU params ×106 (dev) 25.8 65 24.9 25.5 25.8 25.4 25.1 25.4 23.7 25.3 25.5 24.5 26.0 25.4 26.2 24.6 25.5 25.3 25.7 25.7 26.4\\n\\n58 60 36 50 80 28 168 53 90\\n\\n(E) big\\n\\n6\\n\\npositional embedding instead of sinusoids\\n\\n1024\\n\\n4096\\n\\n16\\n\\n0.3\\n\\n213\\n\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.\\n\\n7 Conclusion\\n\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\\n\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\\n\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\\n\\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\\n\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.\\n\\n9\\n\\nReferences\\n\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\n\\narXiv:1607.06450, 2016.\\n\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\n\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\n\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\n\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014.\\n\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\n\\npreprint arXiv:1610.02357, 2016.\\n\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\n\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n\\n[9] Alex Graves. Generating sequences with recurrent neural networks.\\n\\narXiv preprint\\n\\narXiv:1308.0850, 2013.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im- age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770–778, 2016.\\n\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\n\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n\\n9(8):1735–1780, 1997.\\n\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\n\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\n\\non Learning Representations (ICLR), 2016.\\n\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko- ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2, 2017.\\n\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\n\\nIn International Conference on Learning Representations, 2017.\\n\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\n\\narXiv:1703.10722, 2017.\\n\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017.\\n\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\n\\nInformation Processing Systems, (NIPS), 2016.\\n\\n10\\n\\n[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\n\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\n\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\n\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\n\\npreprint arXiv:1608.05859, 2016.\\n\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\n\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017.\\n\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi- nov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine Learning Research, 15(1):1929–1958, 2014.\\n\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.\\n\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\n\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.\\n\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n\\n11'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_3[0].metadata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRW9GbEtruhy",
        "outputId": "90924115-d28f-4838-b0a4-263a980775cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'source': '/content/attention.pdf'}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"/content/attention.pdf\"\n",
        "docs_3 = fitz.open(pdf_path)\n",
        "\n",
        "print(f\"📄 Number of pages in the second file: {len(docs_3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8hNc0N6ryKm",
        "outputId": "1dddc42b-8992-49e8-bb90-a2c424d9d1ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📄 Number of pages in the second file: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks_3 = splitter.split_documents(docs_3)"
      ],
      "metadata": {
        "id": "x7dMJy6J8_of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_chunks_3 = []\n",
        "for chunk in chunks_3:\n",
        "    processed_chunks_3.append(Document(page_content=chunk.page_content, metadata={\"source\": \"source_03\"}))\n",
        "\n",
        "print(f\"✅ Number of chunks in processed third file: {len(processed_chunks_3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9VMtQtbsc-Z",
        "outputId": "d8e55997-946f-43a6-f7f2-ece4e3835f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Number of chunks in processed third file: 76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Combine all documents from three files**"
      ],
      "metadata": {
        "id": "R0LnbW78vG3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_documents = processed_chunks + processed_chunks_2 + processed_chunks_3\n",
        "print(f\"📚 Total documents after merging: {len(all_documents)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPI_tVZQuyk5",
        "outputId": "1e10fca4-ce75-4a68-b57e-9d74029ab4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Total documents after merging: 570\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3.2 :  Creating a vectorstore and searching based on metadata**"
      ],
      "metadata": {
        "id": "0DlaYvNRU5zr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create VectorStore using local embedding model**"
      ],
      "metadata": {
        "id": "js5O03C0v50f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = FAISS.from_documents(all_documents, hf_embedding)\n",
        "print(\"FAISS VectorStore was created\")"
      ],
      "metadata": {
        "id": "1TSSKn0EvNW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "641a20c8-cc52-4f51-daf3-59466ba8009d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS VectorStore was created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is the application of artificial intelligence in cognitive science?\""
      ],
      "metadata": {
        "id": "tZtceZNUwHxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(query, k=3, filter={\"source\": \"source_01\"})\n",
        "\n",
        "print(\"\\n Search results only from source_01:\")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs5RbaD5_FLv",
        "outputId": "1d7487e4-d62c-4e51-a3f4-428759610d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Search results only from source_01:\n",
            "* Neural Correlates of Insight Solving\n",
            "\n",
            "Further speciﬁcation of the neural bases of insight can be achieved through neuroimaging studies. These studies have identiﬁed a number of distinct components of insight and have generally supported the idea that the right hemisphere contributes relatively more to insight than to analytic solving. [{'source': 'source_01'}]\n",
            "* STIMULATING INSIGHT\n",
            "\n",
            "One limitation of neuroimaging and electrophysiological studies is that they are inherently correlational—they don’t directly show that the recorded patterns of brain activity cause the measured changes in behavior or experience. But the advent of brain stimulation techniques now affords the opportunity to treat brain activity as an independent variable rather than a dependent one. [{'source': 'source_01'}]\n",
            "* differences in EEG or brain volume, but if so, this would be a promising avenue of investigation into the stability and origins of cognitive style. [{'source': 'source_01'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(query, k=3, filter={\"source\": \"source_02\"})\n",
        "\n",
        "print(\"\\n Search results only from source_02:\")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\\n{'-'*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7oLhIoz_tD-",
        "outputId": "58e21d23-8e26-45de-d7b2-b19014084286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Search results only from source_02:\n",
            "* dynamical components.\n",
            "Artificial intelligence needs cognitive science to guide the \n",
            "engineering of intelligence. Cognitive science’s tasks can serve \n",
            "as benchmarks for AI systems, building up from elementary \n",
            "cognitive abilities to artificial general intelligence. The literatures \n",
            "on human development and learning provide an essential \n",
            "guide to what is possible for a learner to achieve and what \n",
            "kinds of interaction with the world can support the acquisition [{'source': 'source_02'}]\n",
            "--------------------------------------------------------------------------------\n",
            "* tions in neuroimaging 55–59. In the simplest case, decoding reveals \n",
            "which of two stimuli gave rise to a measured response pattern.  \n",
            "Artificial intelligence\n",
            "Computational neuroscience\n",
            "Cognitive science\n",
            "Explain neuronal\n",
            "activity patterns\n",
            "Explain\n",
            "behavioral data\n",
            "…computational\n",
            "models… \n",
            "...that perform complex\n",
            "cognitive tasks\n",
            "using biologically\n",
            "plausible...\n",
            "Fig. 2 | What does it mean to understand how the brain works? The goal [{'source': 'source_02'}]\n",
            "--------------------------------------------------------------------------------\n",
            "* and machine learning were not sufficiently advanced to simulate \n",
            "cognitive processes in their full complexity. Moreover, these early \n",
            "developments relied on behavioral data alone and did not leverage \n",
            "constraints provided by the anatomy and activity of the brain.\n",
            "With the advent of human functional brain imaging, scientists \n",
            "began to relate cognitive theories to the human brain. This endeavor \n",
            "came to be called cognitive neuroscience\n",
            "7. Cognitive neuroscien - [{'source': 'source_02'}]\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = vector_store.similarity_search(query, k=3, filter={\"source\": \"source_03\"})\n",
        "\n",
        "print(\"\\n Search results only from source_03:\")\n",
        "for res in results:\n",
        "    print(f\"* {res.page_content} [{res.metadata}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCAfU-QUADwx",
        "outputId": "d66525bc-dd73-4394-ffed-805fa84d0a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Search results only from source_03:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The content of the third file is very different from our query also with this query we cannot find result in third file**"
      ],
      "metadata": {
        "id": "21aSOdZeBpiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XmDsS__GTiy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search the entire database without filters**"
      ],
      "metadata": {
        "id": "Gf-jklep-z-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_all = vector_store.similarity_search(query, k=5)\n",
        "print(\"\\n Search results from all sources:\")\n",
        "for i, res in enumerate(results_all, start=1):\n",
        "    source_name = res.metadata.get(\"source\", \"Unknown\")\n",
        "    print(f\"{i}. [Source: {source_name}]\")\n",
        "    print(res.page_content)\n",
        "    print(\"-\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GN_xUhLBZee",
        "outputId": "875c9396-8908-4a02-d36a-dc922c534da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Search results from all sources:\n",
            "1. [Source: source_02]\n",
            "dynamical components.\n",
            "Artificial intelligence needs cognitive science to guide the \n",
            "engineering of intelligence. Cognitive science’s tasks can serve \n",
            "as benchmarks for AI systems, building up from elementary \n",
            "cognitive abilities to artificial general intelligence. The literatures \n",
            "on human development and learning provide an essential \n",
            "guide to what is possible for a learner to achieve and what \n",
            "kinds of interaction with the world can support the acquisition\n",
            "--------------------------------------------------------------------------------\n",
            "2. [Source: source_02]\n",
            "tions in neuroimaging 55–59. In the simplest case, decoding reveals \n",
            "which of two stimuli gave rise to a measured response pattern.  \n",
            "Artificial intelligence\n",
            "Computational neuroscience\n",
            "Cognitive science\n",
            "Explain neuronal\n",
            "activity patterns\n",
            "Explain\n",
            "behavioral data\n",
            "…computational\n",
            "models… \n",
            "...that perform complex\n",
            "cognitive tasks\n",
            "using biologically\n",
            "plausible...\n",
            "Fig. 2 | What does it mean to understand how the brain works? The goal\n",
            "--------------------------------------------------------------------------------\n",
            "3. [Source: source_02]\n",
            "and machine learning were not sufficiently advanced to simulate \n",
            "cognitive processes in their full complexity. Moreover, these early \n",
            "developments relied on behavioral data alone and did not leverage \n",
            "constraints provided by the anatomy and activity of the brain.\n",
            "With the advent of human functional brain imaging, scientists \n",
            "began to relate cognitive theories to the human brain. This endeavor \n",
            "came to be called cognitive neuroscience\n",
            "7. Cognitive neuroscien -\n",
            "--------------------------------------------------------------------------------\n",
            "4. [Source: source_01]\n",
            "Neural Correlates of Insight Solving\n",
            "\n",
            "Further speciﬁcation of the neural bases of insight can be achieved through neuroimaging studies. These studies have identiﬁed a number of distinct components of insight and have generally supported the idea that the right hemisphere contributes relatively more to insight than to analytic solving.\n",
            "--------------------------------------------------------------------------------\n",
            "5. [Source: source_02]\n",
            "developments in cognitive science, computational neuroscience and \n",
            "artificial intelligence suggest that this may be achievable.\n",
            "1. Cognitive science has proceeded from the top down, decom\n",
            "-\n",
            "posing complex cognitive processes into their computational com -\n",
            "ponents. Unencumbered by the need to make sense of brain data, it \n",
            "has developed task-performing computational models at the cogni\n",
            "-\n",
            "tive level. One success story is that of Bayesian cognitive models,\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}